# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## é¡¹ç›®æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªåŸºäºPythonçš„ç”µåŠ¨æ±½è½¦å……ç”µç«™(EVCS)åšå¼ˆè®ºä»¿çœŸé¡¹ç›®ã€‚**æ ¸å¿ƒç›®æ ‡æ˜¯æ±‚è§£ç”µåŠ¨æ±½è½¦å……ç”µç«™ä»·æ ¼åšå¼ˆä¸­çš„å‡è¡¡è§£**ã€‚

é¡¹ç›®åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼š
1. **åšå¼ˆç¯å¢ƒå®ç°**ï¼šå»ºæ¨¡ç”¨æˆ·å‡è¡¡åŠ¨æ€äº¤é€šåˆ†é…(UE-DTA)ä¸å……ç”µç«™å®šä»·ç«äº‰çš„ä»¿çœŸç¯å¢ƒ
2. **MADRLç®—æ³•å®ç°**ï¼šåŸºäºMADDPGæ”¹é€ çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºæ±‚è§£ä»·æ ¼å‡è¡¡

## ğŸ¯ åšå¼ˆè®ºå®šä¹‰ä¸åˆ†ç±»

### åšå¼ˆç±»å‹

æœ¬é¡¹ç›®å®ç°çš„æ˜¯**å•æ­¥é™æ€ä¸å®Œå…¨ä¿¡æ¯ç©ºé—´å·®å¼‚åŒ–å®šä»·åšå¼ˆ**ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹å¾ï¼š

#### åšå¼ˆç»“æ„
- **æ™ºèƒ½ä½“é›†åˆ**ï¼šN = {å……ç”µç«™5, å……ç”µç«™12, å……ç”µç«™14, å……ç”µç«™18}
- **ç­–ç•¥ç©ºé—´**ï¼šS_i = [0.1, 1.0]^8 ï¼ˆæ¯ä¸ªå……ç”µç«™åœ¨8ä¸ªæ—¶æ®µçš„ä»·æ ¼é€‰æ‹©ï¼‰
- **æ”¶ç›Šå‡½æ•°**ï¼šR_i = Î£(p_i,j Ã— q_i,j)ï¼Œå…¶ä¸­p_i,jä¸ºå……ç”µç«™iåœ¨æ—¶æ®µjçš„ä»·æ ¼ï¼Œq_i,jä¸ºå¯¹åº”çš„å……ç”µæµé‡

#### å…³é”®ç‰¹å¾
1. **ç©ºé—´å·®å¼‚åŒ–**ï¼šæ¯ä¸ªå……ç”µç«™å…·æœ‰ä¸åŒçš„åœ°ç†ä½ç½®ï¼Œå½¢æˆäº§å“å·®å¼‚åŒ–
2. **é—´æ¥éœ€æ±‚å“åº”**ï¼šç”¨æˆ·åŸºäºæ€»æˆæœ¬é€‰æ‹©å……ç”µè·¯å¾„
   ```
   æ€»æˆæœ¬ = å‡ºè¡Œæ—¶é—´æˆæœ¬ + å……ç”µä»·æ ¼
   ```
3. **ä¸å®Œå…¨ä¿¡æ¯**ï¼šæ™ºèƒ½ä½“åªè§‚æµ‹åˆ°`last_round_all_prices`å’Œ`own_charging_flow`
4. **é™æ€åŒæ—¶åšå¼ˆ**ï¼šæ‰€æœ‰å……ç”µç«™åŒæ—¶è®¾å®šä»·æ ¼ç­–ç•¥

#### åšå¼ˆåˆ†ç±»å¯¹æ¯”

**vs. ä¼ ç»ŸBertrandç«äº‰**ï¼š
- âŒ åŒè´¨äº§å“ä»·æ ¼ç«äº‰
- âœ… ç©ºé—´å·®å¼‚åŒ–äº§å“å®šä»·ç«äº‰
- âŒ æ¶ˆè´¹è€…é€‰æ‹©æœ€ä½ä»·æ ¼
- âœ… æ¶ˆè´¹è€…è€ƒè™‘æ€»æˆæœ¬ï¼ˆä»·æ ¼+å‡ºè¡Œæˆæœ¬ï¼‰

**vs. Hotellingç©ºé—´ç«äº‰**ï¼š
- âœ… ç©ºé—´ä½ç½®å·®å¼‚åŒ–
- âœ… æ¯ä¸ªå‚å•†å…·æœ‰å±€éƒ¨å„æ–­åŠ›
- âœ… æ¶ˆè´¹è€…è€ƒè™‘è¿è¾“æˆæœ¬

#### å¤šè½®è®­ç»ƒä¸å‡è¡¡æ±‚è§£

**é‡è¦æ¾„æ¸…**ï¼šMADRLä¸­çš„"é‡å¤åšå¼ˆ"æ˜¯**ç®—æ³•æ±‚è§£å·¥å…·**ï¼Œä¸æ˜¯åšå¼ˆæœ¬èº«çš„æ€§è´¨

- **åšå¼ˆå®šä¹‰**ï¼šå•æ¬¡ä»·æ ¼è®¾å®šåšå¼ˆ
- **å¤šè½®è®­ç»ƒç›®çš„**ï¼šé€šè¿‡æ¢¯åº¦å­¦ä¹ é€¼è¿‘çº³ä»€å‡è¡¡
- **æ¯è½®reset()ä½œç”¨**ï¼šé‡æ–°åˆå§‹åŒ–åŒä¸€åšå¼ˆï¼Œå°è¯•æ‰¾åˆ°æ›´å¥½çš„ç­–ç•¥
- **æ”¶æ•›ç›®æ ‡**ï¼šå•æ­¥åšå¼ˆçš„çº³ä»€å‡è¡¡è§£

```python
# ä¼ªä»£ç è¯´æ˜
åšå¼ˆå®šä¹‰ = å•æ­¥ä»·æ ¼è®¾å®šåšå¼ˆ
for episode in range(max_episodes):
    # ä¸æ˜¯"ä¸‹ä¸€ä¸ªåšå¼ˆ"ï¼Œè€Œæ˜¯å¯¹åŒä¸€åšå¼ˆçš„æ±‚è§£å°è¯•
    observations = env.reset()  # é‡æ–°åˆå§‹åŒ–åŒä¸€åšå¼ˆ
    actions = agents.act(observations)  # å°è¯•ç­–ç•¥
    rewards = env.step(actions)  # è®¡ç®—æ”¶ç›Š
    agents.learn()  # æ›´æ–°ç­–ç•¥é€¼è¿‘å‡è¡¡
```

### MADRLç®—æ³•è®¾è®¡ç›®çš„

#### ğŸ¯ æ ¸å¿ƒç›®æ ‡

**æ±‚è§£å¤æ‚åšå¼ˆçš„çº³ä»€å‡è¡¡**ï¼šæ‰¾åˆ°4ä¸ªå……ç”µç«™çš„æœ€ä¼˜å®šä»·ç­–ç•¥ç»„åˆï¼Œä½¿å¾—æ²¡æœ‰ä»»ä½•å……ç”µç«™èƒ½é€šè¿‡å•æ–¹é¢æ”¹å˜ä»·æ ¼æ¥è·å¾—æ›´é«˜æ”¶ç›Šã€‚

#### ğŸš« ä¼ ç»Ÿåšå¼ˆè®ºæ–¹æ³•å¤±æ•ˆåŸå› 

1. **éœ€æ±‚å‡½æ•°ä¸å¯è§£æ**ï¼šå……ç”µæµé‡q_i,jä¾èµ–å¤æ‚çš„UE-DTAä»¿çœŸï¼Œæ— æ³•å†™å‡ºp_i,j â†’ q_i,jçš„è§£æè¡¨è¾¾å¼

2. **ç­–ç•¥ç©ºé—´å·¨å¤§**ï¼š4ä¸ªæ™ºèƒ½ä½“ Ã— 8ä¸ªæ—¶æ®µ Ã— è¿ç»­ä»·æ ¼ç©ºé—´ = 32ç»´è¿ç»­ç­–ç•¥ç©ºé—´

3. **æ”¶ç›Šå‡½æ•°éå‡¸**ï¼šåŸºäºäº¤é€šä»¿çœŸçš„æ”¶ç›Šå‡½æ•°ä¸å¯å¾®ã€å¤šå³°

4. **è€¦åˆå¤æ‚æ€§**ï¼šæ¯ä¸ªå……ç”µç«™çš„æ”¶ç›Šä¸ä»…ä¾èµ–è‡ªå·±çš„ä»·æ ¼ï¼Œè¿˜å—å…¶ä»–3ä¸ªå……ç”µç«™ä»·æ ¼å½±å“

#### ğŸ“Š ç®—æ³•é¢„æœŸè¾“å‡º

```python
# æ¯ä¸ªå……ç”µç«™çš„æœ€ä¼˜ç­–ç•¥
optimal_strategy = {
    "å……ç”µç«™5": [p_5_1, p_5_2, ..., p_5_8],   # 8ä¸ªæ—¶æ®µçš„ä»·æ ¼
    "å……ç”µç«™12": [p_12_1, p_12_2, ..., p_12_8],
    "å……ç”µç«™14": [p_14_1, p_14_2, ..., p_14_8], 
    "å……ç”µç«™18": [p_18_1, p_18_2, ..., p_18_8]
}

# å¯¹åº”çš„å‡è¡¡ç»“æœ
equilibrium_result = {
    "charging_flows": ...,    # å„ç«™å……ç”µæµé‡
    "total_revenues": ...,    # å„ç«™æ”¶ç›Š
    "user_costs": ...,       # ç”¨æˆ·æ€»æˆæœ¬
    "system_efficiency": ... # ç³»ç»Ÿæ•ˆç‡æŒ‡æ ‡
}
```

#### ğŸ¯ åº”ç”¨ä»·å€¼

1. **æ”¿ç­–åˆ¶å®š**ï¼šä¸ºæ”¿åºœåˆ¶å®šå……ç”µæ¡©å»ºè®¾å’Œå®šä»·ç›‘ç®¡æ”¿ç­–æä¾›ä¾æ®
2. **å•†ä¸šå†³ç­–**ï¼šä¸ºå……ç”µç«™è¿è¥å•†æä¾›æœ€ä¼˜å®šä»·ç­–ç•¥
3. **åŸå¸‚è§„åˆ’**ï¼šä¼˜åŒ–å……ç”µåŸºç¡€è®¾æ–½çš„ç©ºé—´å¸ƒå±€
4. **ç†è®ºè´¡çŒ®**ï¼šä¸ºäº¤é€š-èƒ½æºè€¦åˆç³»ç»Ÿçš„åšå¼ˆåˆ†ææä¾›æ–¹æ³•è®º

**æ ¸å¿ƒæ„ä¹‰**ï¼šé€šè¿‡AIæ±‚è§£ä¼ ç»Ÿæ•°å­¦æ–¹æ³•æ— æ³•å¤„ç†çš„å¤æ‚åšå¼ˆå‡è¡¡é—®é¢˜ã€‚

## ğŸ“‹ MADDPGç®—æ³•è®¾è®¡Todoåˆ—è¡¨

### ğŸ§  ç½‘ç»œæ¶æ„è®¾è®¡
1. **Actorç½‘ç»œç»“æ„è®¾è®¡**ï¼šMLP vs LSTMï¼ˆå¤„ç†æ—¶åºç›¸å…³æ€§ï¼‰
   - âœ… **ç¡®å®šæ–¹æ¡ˆï¼šé€‰ç”¨MLP**
   - **ç†ç”±**ï¼šé™æ€åŒæ—¶åšå¼ˆæœ¬è´¨ - ä¸€æ¬¡æ€§è¾“å‡º8ä¸ªæ—¶æ®µä»·æ ¼ï¼Œç­‰ä»·äº8ç»´å‘é‡è¾“å‡ºï¼Œæ— æ—¶åºä¾èµ–
   - **æ¶æ„è®¾è®¡**ï¼š
     ```python
     class MLPActor(nn.Module):
         def __init__(self, obs_dim, action_dim=8, hidden_sizes=[256, 128]):
             # Input: flatten([last_round_all_prices, own_charging_flow]) 
             # Output: 8ä¸ªæ—¶æ®µä»·æ ¼å‘é‡ [0,1]^8
             # æ¿€æ´»å‡½æ•°: Sigmoidç¡®ä¿è¾“å‡ºèŒƒå›´[0,1]
     ```

2. **Criticç½‘ç»œæ•´åˆæ–¹å¼**ï¼šç®€å•æ‹¼æ¥ vs æ³¨æ„åŠ›æœºåˆ¶
   - âœ… **ç¡®å®šæ–¹æ¡ˆï¼šç®€å•æ‹¼æ¥ï¼ˆé¦–é€‰ï¼‰+ æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¯¹æ¯”ï¼‰**
   - **æ ¸å¿ƒæ´å¯Ÿ**ï¼šç©ºé—´å·®å¼‚åŒ–åšå¼ˆä¸­ï¼Œæ™ºèƒ½ä½“é—´å½±å“æ˜¯éå¯¹ç§°çš„ï¼ˆå¦‚åœ°ç†ä½ç½®ç›¸è¿‘çš„å……ç”µç«™ç«äº‰æ›´æ¿€çƒˆï¼‰
   - **è®¾è®¡æ€è€ƒ**ï¼šæ˜¾å¼å»ºæ¨¡ï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰vs éšå¼å­¦ä¹ ï¼ˆMLPæ‹¼æ¥ï¼‰å“ªç§æ–¹å¼æ›´å¥½ï¼Ÿ
   - **ç†ç”±**ï¼šMLPçš„æƒé‡çŸ©é˜µèƒ½éšå«åœ°å®ç°æ™ºèƒ½ä½“é‡è¦æ€§å·®å¼‚ï¼Œå­¦ä¹ èƒ½åŠ›ç­‰ä»·ä½†æ›´ç¨³å®š
   - **åˆ›æ–°å¯¹æ¯”ç‚¹**ï¼šå®éªŒå¯¹æ¯”ä¸¤ç§æ–¹å¼åœ¨å¤šæ™ºèƒ½ä½“ä»·æ ¼åšå¼ˆä¸­çš„æ•ˆæœå·®å¼‚
   - **è¾“å…¥ç»„ç»‡æ–¹å¼ï¼ˆå»é‡ä¼˜åŒ–è®¾è®¡ï¼‰**ï¼š
     ```python
     # æ¯ä¸ªæ™ºèƒ½ä½“éƒ½æœ‰ç‹¬ç«‹çš„Criticç½‘ç»œï¼Œæ— éœ€æ˜¾å¼èº«ä»½æ ‡è¯†
     # å…³é”®è®¾è®¡ï¼šæ™ºèƒ½ä½“è§‚æµ‹å­˜åœ¨é‡å¤ä¿¡æ¯ï¼Œéœ€è¦å»é‡ä¼˜åŒ–
     
     def organize_global_state_optimized(observations, actions):
         """
         ä¼˜åŒ–çš„å…¨å±€çŠ¶æ€ç»„ç»‡æ–¹å¼
         
         é—®é¢˜ï¼šæ¯ä¸ªagentè§‚æµ‹éƒ½åŒ…å«last_round_all_pricesï¼Œç›´æ¥æ‹¼æ¥ä¼šé‡å¤4æ¬¡
         è§£å†³ï¼šå»é‡å¤„ç†ï¼Œå‡å°‘ç½‘ç»œå‚æ•°é‡50%
         """
         sorted_agents = sorted(observations.keys())
         
         # 1. å…¨å±€ä»·æ ¼å†å²ï¼ˆå»é‡ï¼šæ‰€æœ‰agentè§‚æµ‹ä¸­çš„ä»·æ ¼ä¿¡æ¯ç›¸åŒï¼Œåªå–ä¸€ä»½ï¼‰
         global_prices = observations[sorted_agents[0]]["last_round_all_prices"].flatten()  # (4Ã—8=32)
         
         # 2. æ‰€æœ‰æ™ºèƒ½ä½“å……ç”µæµé‡ï¼ˆæ— é‡å¤ï¼šæ¯ä¸ªagentçš„æµé‡ä¸åŒï¼Œéœ€è¦å…¨éƒ¨ä¿ç•™ï¼‰
         all_flows = []
         for agent_id in sorted_agents:
             flow = observations[agent_id]["own_charging_flow"].flatten()  # (8,)
             all_flows.append(flow)
         all_charging_flows = np.concatenate(all_flows)  # (4Ã—8=32)
         
         # 3. æ‰€æœ‰æ™ºèƒ½ä½“å½“å‰åŠ¨ä½œï¼ˆæ— é‡å¤ï¼šæ¯ä¸ªagentåŠ¨ä½œä¸åŒï¼‰
         all_actions = []
         for agent_id in sorted_agents:
             all_actions.append(actions[agent_id].flatten())  # (8,)
         all_current_actions = np.concatenate(all_actions)  # (4Ã—8=32)
         
         return np.concatenate([global_prices, all_charging_flows, all_current_actions])
     
     # æœ€ç»ˆCriticè¾“å…¥ç»´åº¦
     critic_input = torch.cat([
         last_round_all_prices.flatten(),    # (32,) - å…¨å±€ä»·æ ¼å†å²ï¼ˆå»é‡åï¼‰
         all_charging_flows.flatten(),       # (32,) - æ‰€æœ‰æ™ºèƒ½ä½“æµé‡  
         all_current_actions.flatten()       # (32,) - æ‰€æœ‰æ™ºèƒ½ä½“åŠ¨ä½œ
     ], dim=0)  # æ€»è®¡96ç»´
     ```
     
   - **å»é‡ä¼˜åŒ–æ•ˆæœ**ï¼š
     ```python
     # æœªä¼˜åŒ–å®ç°ï¼ˆå½“å‰MADDPGï¼‰
     æœªä¼˜åŒ–ç»´åº¦ = 4ä¸ªagent Ã— (32ä»·æ ¼ + 8æµé‡) + 4ä¸ªagent Ã— 8åŠ¨ä½œ = 192ç»´
     
     # ä¼˜åŒ–åå®ç°ï¼ˆæ–‡æ¡£è®¾è®¡ï¼‰
     ä¼˜åŒ–ç»´åº¦ = 32ä»·æ ¼(å»é‡) + 32æµé‡ + 32åŠ¨ä½œ = 96ç»´
     
     # å‚æ•°é‡å‡å°‘ï¼š50%ï¼Œæ˜¾è‘—æå‡è®­ç»ƒæ•ˆç‡
     ```
   
   - **æ–¹æ¡ˆAï¼šç®€å•æ‹¼æ¥æ¶æ„**ï¼š
     ```python
     class SimpleCritic(nn.Module):
         def __init__(self, input_dim=96, hidden_sizes=[512, 256]):
             self.mlp = nn.Sequential(
                 nn.Linear(96, 512), nn.ReLU(),
                 nn.Linear(512, 256), nn.ReLU(), 
                 nn.Linear(256, 1)  # è¾“å‡ºå¯¹åº”æ™ºèƒ½ä½“çš„Qå€¼
             )
     ```
   
   - **æ–¹æ¡ˆBï¼šæ³¨æ„åŠ›æœºåˆ¶æ¶æ„**ï¼š
     ```python
     class AttentionCritic(nn.Module):
         def __init__(self):
             self.agent_encoder = nn.Linear(24, 64)  # ç¼–ç æ¯ä¸ªæ™ºèƒ½ä½“
             self.attention = nn.MultiheadAttention(64, num_heads=4)
             self.global_encoder = nn.Linear(32, 64)
             self.q_head = nn.Linear(128, 1)
         
         def forward(self, global_info, agent_infos):
             # ç¼–ç æ‰€æœ‰æ™ºèƒ½ä½“ç‰¹å¾ (4, 64)
             agent_features = torch.stack([
                 self.agent_encoder(info) for info in agent_infos
             ])
             # æ³¨æ„åŠ›æœºåˆ¶è‡ªåŠ¨å­¦ä¹ æ™ºèƒ½ä½“é—´éå¯¹ç§°å½±å“æƒé‡
             attended, weights = self.attention(agent_features, agent_features, agent_features)
             attended_global = attended.mean(dim=0)
             # æ•´åˆå…¨å±€å’Œæ³¨æ„åŠ›ç‰¹å¾
             global_feature = self.global_encoder(global_info)
             combined = torch.cat([attended_global, global_feature])
             return self.q_head(combined)
     ```

3. **ç½‘ç»œå±‚æ•°å’Œéšè—å•å…ƒæ•°**ï¼šå¹³è¡¡è¡¨è¾¾èƒ½åŠ›å’Œè®­ç»ƒæ•ˆç‡
   - âœ… **ç¡®å®šæ–¹æ¡ˆï¼šå°ç½‘ç»œèµ·æ­¥ï¼Œæ¸è¿›ä¼˜åŒ–ç­–ç•¥**
   - **è®¾è®¡åŸåˆ™**ï¼šä»ç®€å•ç½‘ç»œå¼€å§‹éªŒè¯å¯è¡Œæ€§ï¼Œé¿å…è¿‡æ‹Ÿåˆå’Œè¿‡åº¦å¤æ‚åŒ–
   - **åŸºäºå‚è€ƒå®ç°åˆ†æ**ï¼šå‚è€ƒMADDPG (3æ™ºèƒ½ä½“ï¼Œè¾“å…¥8-10ç»´ï¼Œéšè—64ç»´) vs æˆ‘ä»¬çš„åœºæ™¯ (4æ™ºèƒ½ä½“ï¼Œè¾“å…¥40/96ç»´)
   - **èµ·å§‹æ¶æ„é…ç½®**ï¼š
     ```python
     class Actor(nn.Module):
         def __init__(self, obs_dim=40, action_dim=8):
             self.net = nn.Sequential(
                 nn.Linear(40, 64),    # 1.6å€æ‰©å±•ï¼Œåˆç†è¡¨è¾¾ç©ºé—´
                 nn.ReLU(),
                 nn.Linear(64, 64),    # ç­‰å®½ä¿æŒï¼Œå‚è€ƒå®ç°é£æ ¼
                 nn.ReLU(),
                 nn.Linear(64, 8),     # è¾“å‡º8ä¸ªæ—¶æ®µä»·æ ¼
                 nn.Sigmoid()          # ç¡®ä¿[0,1]èŒƒå›´
             )
             
     class Critic(nn.Module):
         def __init__(self, input_dim=96):
             self.net = nn.Sequential(
                 nn.Linear(96, 128),   # 1.33å€æ‰©å±•ï¼Œå¤„ç†å¤æ‚å¤šæ™ºèƒ½ä½“è¾“å…¥
                 nn.ReLU(),
                 nn.Linear(128, 64),   # æ”¶æ•›åˆ°å‚è€ƒå®ç°è§„æ¨¡
                 nn.ReLU(),
                 nn.Linear(64, 1)      # Qå€¼è¾“å‡º
             )
     ```
   - **å‚æ•°é‡è¯„ä¼°**ï¼š
     - Actor: â‰ˆ6.4Kå‚æ•° (40Ã—64 + 64Ã—64 + 64Ã—8)
     - Critic: â‰ˆ20.8Kå‚æ•° (96Ã—128 + 128Ã—64 + 64Ã—1)
     - å•æ™ºèƒ½ä½“: â‰ˆ27Kï¼Œ4æ™ºèƒ½ä½“æ€»è®¡â‰ˆ216Kï¼ˆå«targetç½‘ç»œï¼‰
   - **æ¸è¿›ä¼˜åŒ–è·¯å¾„**ï¼šè¡¨è¾¾èƒ½åŠ›ä¸è¶³æ—¶å¯å‡çº§åˆ°æ›´æ·±/æ›´å®½ç½‘ç»œ

### ğŸ¯ è®­ç»ƒç­–ç•¥è®¾è®¡
4. **å™ªéŸ³æ¢ç´¢ç­–ç•¥**ï¼šGaussianå™ªéŸ³ vs OUå™ªéŸ³ vs å‚æ•°ç©ºé—´å™ªéŸ³
   - âœ… **ç¡®å®šæ–¹æ¡ˆï¼šGaussianå™ªéŸ³ + æŒ‡æ•°è¡°å‡**
   - **ç†ç”±**ï¼šé™æ€åšå¼ˆç‰¹å¾ - 8ä¸ªæ—¶æ®µä»·æ ¼ç‹¬ç«‹å†³ç­–ï¼Œæ— éœ€æ—¶åºç›¸å…³å™ªéŸ³
   - **å‚æ•°è®¾ç½®**ï¼š
     ```python
     class GaussianNoise:
         def __init__(self, action_dim=8, sigma=0.2, sigma_decay=0.9995, min_sigma=0.01):
             self.sigma = sigma          # åˆå§‹æ¢ç´¢å¼ºåº¦20%
             self.sigma_decay = sigma_decay  # æŒ‡æ•°è¡°å‡ç‡
             self.min_sigma = min_sigma      # ä¿æŒæœ€å°æ¢ç´¢1%
         
         def __call__(self, action):
             noise = np.random.normal(0, self.sigma, self.action_dim)
             self.sigma = max(self.min_sigma, self.sigma * self.sigma_decay)
             return np.clip(action + noise, 0.0, 1.0)
     ```
   - **è®¾è®¡ä¼˜åŠ¿**ï¼šç®€å•é«˜æ•ˆã€å„æ—¶æ®µç‹¬ç«‹æ¢ç´¢ã€é€æ­¥æ”¶æ•›åˆ°ç¨³å®šç­–ç•¥
   - **å‚è€ƒå¯å‘**ï¼šBoyuaiå®ç°ä½¿ç”¨exploreå‚æ•°æ¨¡å¼åˆ‡æ¢ï¼Œå¯ä½œä¸ºæœªæ¥ä¼˜åŒ–æ–¹å‘

5. **å¥–åŠ±å·¥ç¨‹**ï¼šåŸå§‹æ”¶ç›Š vs å½’ä¸€åŒ– vs åŸºå‡†æ¯”è¾ƒ
   - âœ… **ç¡®å®šæ–¹æ¡ˆï¼šæ­£ä»¿å°„å˜æ¢ï¼ˆå½“è½®æœ€å¤§å€¼å½’ä¸€åŒ–ï¼‰**
   - **ç†è®ºä¾æ®**ï¼šæ­£ä»¿å°„å˜æ¢ä¿æŒçº³ä»€å‡è¡¡ä¸å˜ï¼Œç¡®ä¿åšå¼ˆç­‰ä»·æ€§
   - **å®ç°æ–¹å¼**ï¼š
     ```python
     def normalize_rewards(self, raw_rewards):
         """ä½¿ç”¨å½“è½®æœ€å¤§å€¼è¿›è¡Œæ­£ä»¿å°„å˜æ¢"""
         current_max = max(raw_rewards.values())
         if current_max > 0:
             return {agent: reward / current_max for agent, reward in raw_rewards.items()}
         else:
             return {agent: 0.0 for agent in raw_rewards.keys()}
     
     # å®é™…æ•°æ®ç¤ºä¾‹ï¼š
     # raw_rewards = {'5': 18067.83, '12': 13634.79, '14': 21977.31, '18': 52348.59}
     # normalized = {'5': 0.345, '12': 0.260, '14': 0.420, '18': 1.000}
     ```
   - **è®¾è®¡ä¼˜åŠ¿**ï¼šä¿æŒçœŸå®åšå¼ˆç»“æ„ã€æ•°å€¼å‹å¥½[0,1]èŒƒå›´ã€å®ç°ç®€å•
   - **é€‚ç”¨æ€§**ï¼šå®Œç¾åŒ¹é…å•æ­¥é™æ€åšå¼ˆçš„çº³ä»€å‡è¡¡æ±‚è§£éœ€æ±‚

6. **å­¦ä¹ ç‡è°ƒåº¦**ï¼šå›ºå®š vs è¡°å‡ vs è‡ªé€‚åº”
   - âœ… **ç¡®å®šæ–¹æ¡ˆï¼šå›ºå®šå­¦ä¹ ç‡**
   - **å‚è€ƒä¾æ®**ï¼šBoyuai MADDPGå®ç°ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡ç­–ç•¥éªŒè¯å¯è¡Œæ€§
   - **å‚æ•°è®¾ç½®**ï¼š
     ```python
     # åŸºäºå‚è€ƒå®ç°çš„ç»éªŒï¼Œé€‚é…æˆ‘ä»¬çš„å¥–åŠ±èŒƒå›´[0,1]
     actor_lr = 0.001     # Actorç½‘ç»œå­¦ä¹ ç‡
     critic_lr = 0.001    # Criticç½‘ç»œå­¦ä¹ ç‡
     
     self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)
     self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)
     ```
   - **è®¾è®¡ç†ç”±**ï¼šå•æ­¥åšå¼ˆæ”¶æ•›åç­–ç•¥ç›¸å¯¹ç¨³å®šï¼Œå›ºå®šå­¦ä¹ ç‡ç®€å•æœ‰æ•ˆ
   - **ä¼˜åŒ–è€ƒè™‘**ï¼šç›¸æ¯”å‚è€ƒå®ç°çš„0.01ï¼Œé€‰æ‹©0.001ç¡®ä¿ç¨³å®šæ”¶æ•›åˆ°çº³ä»€å‡è¡¡

### ğŸ”§ ç¨³å®šæ€§å¢å¼ºè®¾è®¡
7. **å¤šæ¬¡ä»¿çœŸå¹³å‡**ï¼šæ˜¯å¦å¯¹æ¯ä¸ªåŠ¨ä½œè¿›è¡Œå¤šæ¬¡UE-DTAä»¿çœŸæ±‚å¹³å‡
   - âœ… **ç¡®å®šæ–¹æ¡ˆï¼šå•æ¬¡ä»¿çœŸ**
   - **å®ç°æ–¹å¼**ï¼š
     ```python
     def step(self, actions):
         # è®¾ç½®ä»·æ ¼
         self._set_agent_prices(actions)
         # å•æ¬¡UE-DTAä»¿çœŸ
         rewards = self._run_single_simulation()
         return observations, rewards, done, info
     ```
   - **è®¾è®¡ç†ç”±**ï¼šè®¡ç®—æ•ˆç‡ä¼˜å…ˆï¼Œé¿å…è®­ç»ƒæ—¶é—´æˆå€å¢é•¿
   - **é£é™©æ§åˆ¶**ï¼šé€šè¿‡å›ºå®šéšæœºç§å­ç¡®ä¿ä»¿çœŸå¯å¤ç°æ€§
   - **ä¼˜åŒ–è·¯å¾„**ï¼šå¦‚å‘ç°è®­ç»ƒä¸ç¨³å®šï¼Œå¯åç»­å®ç°å¤šæ¬¡å¹³å‡ä½œä¸ºå¯é€‰åŠŸèƒ½

8. **ç»éªŒå›æ”¾æ”¹è¿›**ï¼šæ ‡å‡†å›æ”¾ vs ä¼˜å…ˆçº§å›æ”¾ vs å¤šæ ·æ€§é‡‡æ ·
   - âœ… **ç¡®å®šæ–¹æ¡ˆï¼šæ ‡å‡†ç»éªŒå›æ”¾ï¼ˆå•Episodeç‰ˆæœ¬ï¼‰**
   - **è®¾è®¡ç­–ç•¥**ï¼šä¸“æ³¨äºå•episodeå†…çš„å‡è¡¡æ”¶æ•›ï¼Œè€Œéè·¨episodeæ³›åŒ–
   - **å®ç°æ–¹å¼**ï¼š
     ```python
     class ReplayBuffer:
         def __init__(self, capacity=10000):  # å­˜å‚¨å•episodeçš„ç»éªŒ
             self.buffer = deque(maxlen=capacity)
         
         def reset_episode(self):
             # å¯é€‰ï¼šæ¯ä¸ªepisodeå¼€å§‹æ—¶æ¸…ç©ºæˆ–ä¿ç•™å°‘é‡ç»éªŒ
             pass
             
         def sample(self, batch_size):
             return random.sample(self.buffer, batch_size)
     
     # å‚æ•°è®¾ç½®
     buffer_size = 10000       # å•episodeç»éªŒå®¹é‡
     batch_size = 64           # å›ºå®šæ‰¹æ¬¡å¤§å°ï¼Œä¸æœ€å°ç¼“å†²åŒºåŒ¹é…
     min_buffer_size = 64      # å¼€å§‹å­¦ä¹ çš„æœ€å°ç»éªŒæ•°ï¼ˆå°½æ—©å¼€å§‹å­¦ä¹ ï¼‰
     ```
   - **æ ¸å¿ƒç†å¿µ**ï¼šä¸€æ—¦åœ¨å•episodeä¸­æ”¶æ•›åˆ°çº³ä»€å‡è¡¡å³è¾¾æˆé¡¹ç›®ç›®æ ‡
   - **æ‰©å±•è·¯å¾„**ï¼šå¦‚éœ€éªŒè¯å‡è¡¡ç¨³å®šæ€§ï¼Œå¯åç»­å®ç°è·¨episodeç‰ˆæœ¬

9. **ç›®æ ‡ç½‘ç»œæ›´æ–°**ï¼šè½¯æ›´æ–°é¢‘ç‡å’Œtauå‚æ•°é€‰æ‹©
   - âœ… **ç¡®å®šæ–¹æ¡ˆï¼šè½¯æ›´æ–° tau=0.01**
   - **å®ç°æ–¹å¼**ï¼š
     ```python
     def soft_update(target_net, main_net, tau=0.01):
         """æ¯æ­¥è¿›è¡Œè½¯æ›´æ–°"""
         for target_param, main_param in zip(target_net.parameters(), main_net.parameters()):
             target_param.data.copy_(
                 tau * main_param.data + (1.0 - tau) * target_param.data
             )
     
     # æ¯æ¬¡å­¦ä¹ æ—¶è°ƒç”¨
     def learn(self):
         # ... ç½‘ç»œè®­ç»ƒä»£ç  ...
         
         # æ›´æ–°ç›®æ ‡ç½‘ç»œ
         soft_update(self.actor_target, self.actor_main, tau=0.01)
         soft_update(self.critic_target, self.critic_main, tau=0.01)
     ```
   - **è®¾è®¡ä¼˜åŠ¿**ï¼šå¹³æ»‘æ›´æ–°ä¿è¯è®­ç»ƒç¨³å®šæ€§ï¼Œé€‚åˆçº³ä»€å‡è¡¡çš„ç²¾ç¡®æ”¶æ•›
   - **å‚æ•°é€‰æ‹©**ï¼štau=0.01ä¸ºMADDPGç»å…¸å‚æ•°ï¼Œå¹³è¡¡æ›´æ–°é€Ÿåº¦ä¸ç¨³å®šæ€§

### ğŸ“Š æ”¶æ•›åˆ¤æ–­è®¾è®¡
10. **å‡è¡¡æ£€æµ‹æŒ‡æ ‡**ï¼šä»·æ ¼å˜åŒ–é˜ˆå€¼ vs æ”¶ç›Šç¨³å®šæ€§ vs ç­–ç•¥æ¢¯åº¦èŒƒæ•°
    - âœ… **ç¡®å®šæ–¹æ¡ˆï¼šç›¸å¯¹å˜åŒ–ç‡ä»·æ ¼é˜ˆå€¼**
    - **æ ¸å¿ƒç†å¿µ**ï¼šå•æ­¥é™æ€åšå¼ˆä¸­ï¼Œçº³ä»€å‡è¡¡è¡¨ç°ä¸ºä»·æ ¼ç­–ç•¥ç¨³å®š
    - **è®¾è®¡ç†ç”±**ï¼šç›¸å¯¹å˜åŒ–ç‡èƒ½è‡ªé€‚åº”ä¸åŒä»·æ ¼æ°´å¹³ï¼Œæ¯”L2èŒƒæ•°æ›´ç§‘å­¦
    - **å®ç°æ–¹å¼**ï¼š
      ```python
      def __check_convergence(self) -> bool:
          """æ£€æŸ¥ä»·æ ¼æ˜¯å¦æ”¶æ•›ï¼ˆåŸºäºç›¸å¯¹å˜åŒ–ç‡ï¼‰"""
          if len(self.price_history) < 2:
              return False
          
          current_prices = self.price_history[-1]
          previous_prices = self.price_history[-2]
          
          # ä½¿ç”¨ç›¸å¯¹å˜åŒ–ç‡ï¼Œé¿å…é™¤é›¶
          relative_changes = np.abs(current_prices - previous_prices) / (previous_prices + 1e-8)
          avg_relative_change = np.mean(relative_changes)
          
          converged = avg_relative_change < self.convergence_threshold
          return converged
      ```
    - **ä¼˜åŠ¿å¯¹æ¯”**ï¼š
      - âŒ L2èŒƒæ•°ï¼šå¯¹ä»·æ ¼çŸ©é˜µç»´åº¦æ•æ„Ÿï¼Œä¸åŒä»·æ ¼æ°´å¹³ä¸‹é˜ˆå€¼éš¾è®¾å®š
      - âŒ æ”¶ç›Šç¨³å®šæ€§ï¼šå—UE-DTAä»¿çœŸéšæœºæ€§å½±å“ï¼Œå¯èƒ½è¯¯åˆ¤
      - âŒ ç­–ç•¥æ¢¯åº¦èŒƒæ•°ï¼šå®ç°å¤æ‚ï¼Œéœ€è¦è®¿é—®ç½‘ç»œå†…éƒ¨ä¿¡æ¯
      - âœ… ç›¸å¯¹å˜åŒ–ç‡ï¼šç»´åº¦æ— å…³ã€è‡ªé€‚åº”ã€å®ç°ç®€å•ã€ç†è®ºæ¸…æ™°

11. **è®­ç»ƒç»ˆæ­¢æ¡ä»¶**ï¼šå›ºå®šepisodes vs åŠ¨æ€æ”¶æ•›æ£€æµ‹
    - âœ… **ç¡®å®šæ–¹æ¡ˆï¼šä»…éœ€ç¯å¢ƒå±‚æ”¶æ•›æ£€æµ‹**
    - **æ ¸å¿ƒæ´å¯Ÿ**ï¼šå•æ­¥é™æ€åšå¼ˆæ±‚è§£ä¸éœ€è¦MADRLç®—æ³•å±‚æ”¶æ•›æ£€æµ‹
    - **ç†è®ºä¾æ®**ï¼š
      ```python
      # æ¯ä¸ªepisodeéƒ½æ˜¯åŒä¸€åšå¼ˆçš„æ±‚è§£å°è¯•
      åšå¼ˆå®šä¹‰ = å•æ­¥ä»·æ ¼è®¾å®šåšå¼ˆ  # å›ºå®šä¸å˜
      for episode in range(max_episodes):
          observations = env.reset()    # é‡æ–°åˆå§‹åŒ–åŒä¸€åšå¼ˆ
          actions = agents.act(obs)     # å°è¯•ç­–ç•¥
          rewards = env.step(actions)   # è®¡ç®—æ”¶ç›Š
          if env.converged:             # ç¯å¢ƒå†…æ”¶æ•› = æ‰¾åˆ°çº³ä»€å‡è¡¡
              break                     # ä»»åŠ¡å®Œæˆ
          agents.learn()                # å­¦ä¹ å¦‚ä½•æ›´å¿«æ‰¾åˆ°å‡è¡¡
      ```
    - **ç»ˆæ­¢æ¡ä»¶è®¾è®¡**ï¼š
      - **ç¯å¢ƒå†…æ”¶æ•›**ï¼š`avg_relative_change < convergence_threshold` â†’ æ‰¾åˆ°çº³ä»€å‡è¡¡
      - **Episodeæˆªæ–­**ï¼š`current_step >= max_steps` â†’ ç»§ç»­ä¸‹ä¸ªepisode
      - **MADRLè¯„ä¼°**ï¼šä¸æ˜¯"æ˜¯å¦æ”¶æ•›"ï¼Œè€Œæ˜¯"å¹³å‡å¤šå°‘episodesæ‰¾åˆ°å‡è¡¡"
    - **å®ç°ä½ç½®**ï¼š
      ```python
      # ç¯å¢ƒå±‚ï¼ˆEVCSChargingGameEnv.step()ï¼‰
      terminated = self.__check_convergence()  # ä»·æ ¼åšå¼ˆå‡è¡¡æ£€æµ‹
      truncated = self.current_step >= self.max_steps  # å•episodeæˆªæ–­
      
      # MADRLå±‚ï¼šæ— éœ€æ”¶æ•›æ£€æµ‹ï¼Œä¸“æ³¨ç­–ç•¥å­¦ä¹ 
      ```

## å¼€å‘å‘½ä»¤

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è¿è¡Œä¸»ä»¿çœŸ
python main.py

# è¿è¡ŒæŠ€æœ¯éªŒè¯æµ‹è¯•
python src/technical_validation.py
```

## æ ¸å¿ƒæ¶æ„ä¸æ–‡ä»¶ç»“æ„

### ä¸»è¦ç»„ä»¶

- **`src/EVCSChargingGameEnv.py`**: **v3.0æ ¸å¿ƒç¯å¢ƒå®ç°** âœ… **å·²å®Œæˆ**
  - åŸºäºv2æ¡†æ¶ + è‡ªç¯å……ç”µé“¾è·¯ + é¢„å®šè·¯å¾„çš„æœ€ç»ˆè®¾è®¡æ–¹æ¡ˆ
  - å®Œæ•´PettingZoo ParallelEnvæ¥å£
  - é›†æˆMonkey Patchå¢å¼ºçš„UXSim + å¤šçŠ¶æ€å›¾è·¯å¾„ç®—æ³•
  - åŒ…å«å®Œæ•´çš„UE-DTAä»¿çœŸå¾ªç¯å’Œå¥–åŠ±è®¡ç®—ç³»ç»Ÿ

- **`src/patch.py`**: **UXSim Monkey Patchæ¨¡å—** âœ… **å·²å®Œæˆ**
  - ç»Ÿä¸€çš„UXSimå¢å¼ºè¡¥ä¸ï¼Œè§£å†³æ‰€æœ‰UXSimç›¸å…³é—®é¢˜
  - Analyzeræ–‡ä»¶å¤¹åˆ›å»ºé—®é¢˜ä¿®å¤
  - Vehicleé¢„å®šè·¯å¾„åŠŸèƒ½å¢å¼ºï¼ˆè½¬ç§»ç¡®è®¤æœºåˆ¶ï¼‰
  - Worldå¢å¼ºåŠŸèƒ½ï¼ˆaddVehicleå’Œadddemandæ–¹æ³•ï¼‰

- **`src/EVCSChargingGameEnv_v1.py`**: v1.0ç‰ˆæœ¬å®ç°
  - åŸºäºå……ç”µé“¾è·¯å¤åˆ¶çš„åˆå§‹æ–¹æ¡ˆ
  - å®Œæ•´çš„UE-DTAå’Œå¤šæ™ºèƒ½ä½“æ¥å£
  - å·²éªŒè¯çš„day-to-dayåŠ¨æ€å‡è¡¡ç®—æ³•

- **`src/EVCSChargingGameEnv_v2.py`**: v2.0ç‰ˆæœ¬å®ç°
  - åŸºäºèŠ‚ç‚¹åˆ†ç¦»çš„å®Œæ•´æ¡†æ¶è®¾è®¡
  - v3.0ç»§æ‰¿å…¶PettingZooæ¥å£å’ŒUE-DTAç®—æ³•æ¡†æ¶
  - æä¾›v3.0çš„æ•´ä½“ç»“æ„åŸºç¡€

- **`src/technical_validation.py`**: æŠ€æœ¯éªŒè¯è„šæœ¬
  - PredefinedRouteVehicleå’ŒPredefinedRouteWorldç±»å®ç°
  - UXSimè‡ªç¯é“¾è·¯å…¼å®¹æ€§éªŒè¯
  - è·¯å¾„æ‰§è¡Œå®Œæ•´æ€§æµ‹è¯•

- **`main.py`**: ç®€å•å…¥å£ç‚¹ï¼Œåˆå§‹åŒ–ç¯å¢ƒ

### ç½‘ç»œæ•°æ®ç»“æ„

ä»¿çœŸä½¿ç”¨å­˜å‚¨åœ¨ `siouxfalls/` ä¸­çš„Sioux Fallsç½‘ç»œæ•°æ®ï¼š
- `siouxfalls_nodes.csv`: å¸¦åæ ‡çš„ç½‘ç»œèŠ‚ç‚¹
- `siouxfalls_links.csv`: å¸¦å®¹é‡å’Œé€Ÿåº¦å‚æ•°çš„é“è·¯é“¾è·¯
- `siouxfalls_demand.csv`: èµ·ç»ˆç‚¹å¯¹ä¹‹é—´çš„äº¤é€šéœ€æ±‚
- `siouxfalls_settings.json`: é…ç½®æ–‡ä»¶ï¼ŒåŒ…æ‹¬å……ç”µèŠ‚ç‚¹å’Œä»·æ ¼è¾¹ç•Œ

## EVCSChargingGameEnv v3.0 è®¾è®¡æ–¹æ¡ˆ

### ğŸ¯ æ ¸å¿ƒè®¾è®¡ç†å¿µ

**æ•´ä½“æ¶æ„**: ç»§æ‰¿v2æ¡†æ¶ï¼ˆPettingZooæ¥å£ + UE-DTAç®—æ³• + ç±»ç»“æ„ï¼‰
**å……ç”µå»ºæ¨¡**: è‡ªç¯å……ç”µé“¾è·¯ + å¤šçŠ¶æ€å›¾è·¯å¾„ç®—æ³• + Monkey Patchå¢å¼ºUXSimä¸¥æ ¼æ‰§è¡Œ

**è®¾è®¡åŸåˆ™**:
1. **æ¡†æ¶ç»§æ‰¿**: é‡‡ç”¨v2ç‰ˆæœ¬çš„æˆç†ŸPettingZooæ¥å£å’ŒUE-DTAæ¡†æ¶
2. **æ‹“æ‰‘ç®€åŒ–**: ä½¿ç”¨è‡ªç¯å……ç”µé“¾è·¯ï¼Œé¿å…èŠ‚ç‚¹åˆ†ç¦»çš„ç½‘ç»œå¤æ‚æ€§
3. **è·¯å¾„ç²¾ç¡®æ§åˆ¶**: å¤šçŠ¶æ€å›¾ç¡®ä¿å……ç”µè½¦è¾†æ°å¥½å……ç”µä¸€æ¬¡
4. **æ‰§è¡Œä¿è¯**: Monkey Patchå¢å¼ºçš„Vehicleç¡®ä¿ä¸¥æ ¼æŒ‰é¢„å®šè·¯å¾„è¡Œé©¶
5. **è½¦è¾†æ•°ç»Ÿè®¡**: deltanæ¦‚å¿µç¡®ä¿æ‰€æœ‰ç»Ÿè®¡æ˜¾ç¤ºå®é™…è½¦è¾†æ•°é‡

### ğŸš— å…³é”®æ¦‚å¿µï¼šdeltanï¼ˆè½¦è¾†æ‰¹æ¬¡å¤§å°ï¼‰

**deltan**æ˜¯UXSimä¸­çš„å…³é”®å‚æ•°ï¼Œå®šä¹‰äº†æ¯ä¸ªVehicleå¯¹è±¡ä»£è¡¨å¤šå°‘è¾†å®é™…è½¦è¾†ã€‚è¿™æ˜¯ä¸€ä¸ª"platoon"ï¼ˆè½¦é˜Ÿï¼‰æ¦‚å¿µï¼š

- **å®šä¹‰**: `deltan` = æ¯ä¸ªVehicleå¯¹è±¡ä»£è¡¨çš„å®é™…è½¦è¾†æ•°é‡
- **ä½œç”¨**: å‡å°‘ä»¿çœŸè®¡ç®—é‡ï¼Œæé«˜å¤§è§„æ¨¡äº¤é€šç½‘ç»œä»¿çœŸæ•ˆç‡
- **ç»Ÿè®¡å½±å“**: æ‰€æœ‰è½¦è¾†æ•°ç»Ÿè®¡éƒ½éœ€è¦ä¹˜ä»¥deltanæ‰æ˜¯å®é™…è½¦è¾†æ•°é‡

**v3.0ä¸­çš„deltanå¤„ç†**:
- âœ… æ‰€æœ‰æ˜¾ç¤ºçš„è½¦è¾†æ•°ç»Ÿè®¡éƒ½æ˜¯å®é™…è½¦è¾†æ•°ï¼ˆVehicleå¯¹è±¡æ•° Ã— deltanï¼‰
- âœ… tqdmè¿›åº¦æ¡æ˜¾ç¤ºå®é™…è½¦è¾†æ•°
- âœ… æ”¶æ•›/æœªæ”¶æ•›æ¶ˆæ¯æ˜¾ç¤ºå®é™…è½¦è¾†æ•°
- âœ… æœ€ç»ˆç»Ÿè®¡è¡¨æ ¼æ˜¾ç¤ºå®é™…è½¦è¾†æ•°
- âœ… å……ç”µæµé‡ç»Ÿè®¡è€ƒè™‘deltanå½±å“

**ç¤ºä¾‹**:
```python
# å¦‚æœåˆ›å»ºäº†100ä¸ªVehicleå¯¹è±¡ï¼Œdeltan=5
vehicle_objects = 100
deltan = 5
actual_vehicles = vehicle_objects * deltan  # 500è¾†å®é™…è½¦è¾†

# v3.0ä¸­æ‰€æœ‰åœ°æ–¹æ˜¾ç¤ºçš„éƒ½æ˜¯500ï¼Œè€Œä¸æ˜¯100
```

### ğŸ§ª æŠ€æœ¯éªŒè¯ç»“æœ

#### âœ… UXSimè‡ªç¯é“¾è·¯å®Œå…¨å…¼å®¹
åŸºäº `src/technical_validation.py` çš„éªŒè¯æµ‹è¯•è¡¨æ˜ï¼š

```python
# æµ‹è¯•è·¯å¾„ï¼ˆåŒ…å«è‡ªç¯A-Aï¼‰
predefined_route = ["A-B", "B-A", "A-A", "A-B", "B-A", "A-C"]

# éªŒè¯ç»“æœ
Actual route:     ['A-B', 'B-A', 'A-A', 'A-B', 'B-A', 'A-C'] 
Predefined route: ['A-B', 'B-A', 'A-A', 'A-B', 'B-A', 'A-C']
Completion progress: 100.0%
Unfinished vehicles: 0
```

#### âœ… PredefinedRouteVehicleå…³é”®ç‰¹æ€§
1. **100%è·¯å¾„åŒ¹é…**: å®é™…è¡Œé©¶è·¯å¾„ä¸é¢„å®šè·¯å¾„å®Œå…¨ä¸€è‡´
2. **è‡ªç¯æ”¯æŒ**: UXSimåŸç”Ÿæ”¯æŒè‡ªç¯é“¾è·¯ï¼ˆX-Xæ ¼å¼ï¼‰
3. **å®Œæˆç‡ä¿è¯**: æ‰€æœ‰è½¦è¾†æŒ‰é¢„å®šè·¯å¾„å®Œæˆè¡Œç¨‹
4. **è·¯å¾„çµæ´»æ€§**: æ”¯æŒä»»æ„å¤æ‚çš„è·¯å¾„åºåˆ—ï¼ŒåŒ…æ‹¬è‡ªç¯

### ğŸ—ï¸ v3.0æ¶æ„è®¾è®¡

#### 1. è‡ªç¯å……ç”µé“¾è·¯åˆ›å»º
```python
# ä¸ºæ¯ä¸ªå……ç”µèŠ‚ç‚¹åˆ›å»ºè‡ªç¯å……ç”µé“¾è·¯
def _create_charging_links(self):
    for charging_node_idx in self._charging_nodes.keys():
        charging_link_name = f"charging_{charging_node_idx}"
        self.W.addLink(
            name=charging_link_name,
            start_node=charging_node_idx,
            end_node=charging_node_idx,  # è‡ªç¯ï¼šèµ·ç‚¹=ç»ˆç‚¹
            length=self._charging_link_length,
            free_flow_speed=self._charging_link_free_flow_speed,
            attribute={"charging_link": True}
        )
```

#### 2. å¤šçŠ¶æ€å›¾è·¯å¾„ç®—æ³•
```python
def _enumerate_k_shortest_charge_routes(self, source, target, k):
    """åŸºäºå¤šçŠ¶æ€å›¾çš„å……ç”µè·¯å¾„æšä¸¾ï¼Œç¡®ä¿æ°å¥½ä¸€æ¬¡å……ç”µ"""
    G = nx.DiGraph()
    
    # æ„å»ºå¤šçŠ¶æ€å›¾
    for link in self.W.LINKS:
        start, end = link.start_node.name, link.end_node.name
        
        if link.attribute.get("charging_link", False):
            # è‡ªç¯å……ç”µé“¾è·¯ï¼šçŠ¶æ€è½¬æ¢ uncharged -> charged
            node_idx = start  # è‡ªç¯èµ·ç‚¹=ç»ˆç‚¹
            G.add_edge(f"uncharged_{node_idx}", f"charged_{node_idx}", 
                      weight=link.length/link.u, link_name=link.name)
        else:
            # æ™®é€šé“¾è·¯ï¼šçŠ¶æ€ä¿æŒ
            G.add_edge(f"uncharged_{start}", f"uncharged_{end}", 
                      weight=link.length/link.u, link_name=link.name)
            G.add_edge(f"charged_{start}", f"charged_{end}", 
                      weight=link.length/link.u, link_name=link.name)
    
    # è·¯å¾„æœç´¢ï¼šuncharged_source -> charged_target
    paths = nx.shortest_simple_paths(G, f"uncharged_{source}", f"charged_{target}", weight='weight')
    
    # è½¬æ¢ä¸ºå®é™…é“¾è·¯åºåˆ—ï¼Œä¾›PredefinedRouteVehicleä½¿ç”¨
    routes = []
    for path in islice(paths, k):
        route = [G[path[i]][path[i+1]]['link_name'] for i in range(len(path)-1)]
        routes.append(route)
    
    return routes
```

#### 3. PredefinedRouteVehicleé›†æˆ
```python
# ç»§æ‰¿v2æ¡†æ¶ï¼Œé›†æˆé¢„å®šè·¯å¾„è½¦è¾†
class EVCSGameEnv(ParallelEnv):  # ç»§æ‰¿v2çš„åŸºç¡€ç»“æ„
    def _create_simulation_world(self):
        """åˆ›å»ºæ”¯æŒé¢„å®šè·¯å¾„çš„ä»¿çœŸä¸–ç•Œ"""
        W = PredefinedRouteWorld(name="EVCS_Simulation", **simulation_params)
        
        # å¤åˆ¶ç½‘ç»œç»“æ„ï¼ˆåŒ…æ‹¬è‡ªç¯å……ç”µé“¾è·¯ï¼‰
        self._copy_network_to_world(W)
        
        # ä¸ºæ¯è¾†è½¦åˆ†é…é¢„å®šè·¯å¾„
        for vehicle_id, route_links in self.routes_specified.items():
            veh = W.addVehicle(
                predefined_route=route_links,  # å®Œæ•´é“¾è·¯åºåˆ—ï¼ŒåŒ…å«è‡ªç¯charging_links
                departure_time=self.departure_times[vehicle_id],
                name=vehicle_id
            )
```

#### 4. å……ç”µè¡Œä¸ºç‰©ç†å»ºæ¨¡
```python
# é€šè¿‡è‡ªç¯é“¾è·¯å‚æ•°æ§åˆ¶å……ç”µæ—¶é—´ï¼ˆä»…ä½¿ç”¨settings.jsonä¸­é…ç½®çš„å‚æ•°ï¼‰
charging_link_parameters = {
    "length": 3000,              # 3kmï¼Œæ¥è‡ªsettings.jsonçš„charging_link_length
    "free_flow_speed": 10,       # 10m/sï¼Œæ¥è‡ªsettings.jsonçš„charging_link_free_flow_speed
}
```

### ğŸ® PettingZooç¯å¢ƒæ¥å£è®¾è®¡ï¼ˆç»§æ‰¿v2æ¡†æ¶ï¼‰

#### è§‚æµ‹ç©ºé—´
```python
def observation_space(self, agent):
    return spaces.Dict({
        "last_round_all_prices": spaces.Box(low=0.0, high=1.0, shape=(self.n_agents, self.n_periods)),
        "own_charging_flow": spaces.Box(low=0, high=np.inf, shape=(self.n_periods,))
    })
```

#### åŠ¨ä½œç©ºé—´
```python
def action_space(self, agent):
    return spaces.Box(low=0.0, high=1.0, shape=(self.n_periods,))
```

#### å¥–åŠ±å‡½æ•°
```python
reward_i = Î£(æ—¶æ®µ_jçš„ä»·æ ¼ Ã— æ—¶æ®µ_jçš„å……ç”µè½¦è¾†æ•°é‡)  # å¯¹äºæ™ºèƒ½ä½“i
```

### ğŸ“Š Day-to-DayåŠ¨æ€å‡è¡¡ç®—æ³•ï¼ˆç»§æ‰¿v2æ¡†æ¶ï¼‰

#### UE-DTAæ±‚è§£æµç¨‹
1. **è·¯å¾„é¢„è®¡ç®—**: ä½¿ç”¨å¤šçŠ¶æ€å›¾ä¸ºæ‰€æœ‰ODå¯¹é¢„è®¡ç®—kæ¡å……ç”µ/éå……ç”µè·¯å¾„
2. **è·¯å¾„åˆå§‹åŒ–**: ä¸ºè½¦è¾†åˆ†é…åˆå§‹é¢„å®šè·¯å¾„ï¼ˆå……ç”µè½¦è¾†ç¡®ä¿åŒ…å«æ°å¥½ä¸€æ¡è‡ªç¯å……ç”µé“¾è·¯ï¼‰
3. **ä»¿çœŸæ‰§è¡Œ**: ä½¿ç”¨PredefinedRouteVehicleä¸¥æ ¼æŒ‰é¢„å®šè·¯å¾„ä»¿çœŸ
4. **æˆæœ¬è®¡ç®—**: åŸºäºå®é™…æ—…è¡Œæ—¶é—´å’Œå……ç”µä»·æ ¼è®¡ç®—æ€»æˆæœ¬
5. **è·¯å¾„åˆ‡æ¢**: æ ¹æ®æˆæœ¬å·®å¼‚å’Œéšæœºæ¦‚ç‡è¿›è¡Œè·¯å¾„è°ƒæ•´
6. **æ”¶æ•›åˆ¤æ–­**: åŸºäºå¹³å‡æˆæœ¬å·®åˆ¤æ–­æ˜¯å¦è¾¾åˆ°å‡è¡¡

#### æˆæœ¬å‡½æ•°
```python
# å……ç”µè½¦è¾†ï¼ˆæ£€æµ‹åˆ°è‡ªç¯charging_é“¾è·¯ï¼‰
æ€»æˆæœ¬ = time_value_coefficient Ã— å®é™…æ—…è¡Œæ—¶é—´ + å……ç”µä»·æ ¼ Ã— charging_demand_per_vehicle

# éå……ç”µè½¦è¾†  
æ€»æˆæœ¬ = time_value_coefficient Ã— å®é™…æ—…è¡Œæ—¶é—´
```

## ğŸƒ v3.0å½“å‰å®ç°çŠ¶æ€

### âœ… å·²å®Œæˆçš„æ ¸å¿ƒåŠŸèƒ½ï¼ˆæˆªè‡³2025å¹´1æœˆï¼‰

**âœ… å®Œæ•´çš„v3.0å®ç°ï¼ˆPhase 1-8å…¨éƒ¨å®Œæˆï¼‰ï¼š**

**PettingZooç¯å¢ƒæ¥å£å±‚ï¼ˆPhase 4ï¼‰ï¼š**
- âœ… ä»·æ ¼åˆå§‹åŒ–ç³»ç»Ÿï¼šæ”¯æŒéšæœºå’Œä¸­ç‚¹åˆå§‹åŒ–æ¨¡å¼
- âœ… è§‚æµ‹ç©ºé—´å®ç°ï¼šlast_round_all_prices + own_charging_flow
- âœ… åŠ¨ä½œç©ºé—´å®ç°ï¼š[0,1]å½’ä¸€åŒ–ä»·æ ¼åŠ¨ä½œ
- âœ… ä»·æ ¼å½’ä¸€åŒ–ï¼šå®é™…ä»·æ ¼ â†” [0,1]åŒºé—´æ˜ å°„  
- âœ… ç¯å¢ƒç”Ÿå‘½å‘¨æœŸï¼šreset()å’Œstep()æ–¹æ³•å®Œæ•´å®ç°
- âœ… æ”¶æ•›æ£€æµ‹ï¼šåŸºäºL2èŒƒæ•°çš„ä»·æ ¼å˜åŒ–æ£€æµ‹

**ç½‘ç»œå’Œè·¯å¾„åŸºç¡€è®¾æ–½ï¼ˆPhase 1-3ï¼‰ï¼š**
- âœ… è‡ªç¯å……ç”µé“¾è·¯åˆ›å»ºï¼šcharging_{node_id} (nodeâ†’node)
- âœ… å¤šçŠ¶æ€å›¾è·¯å¾„ç®—æ³•ï¼šuncharged/chargedçŠ¶æ€è½¬æ¢
- âœ… Monkey Patch Vehicleå¢å¼ºï¼šæŠ€æœ¯éªŒè¯å®Œæˆ
- âœ… å……ç”µ/éå……ç”µè·¯å¾„æšä¸¾ï¼škæœ€çŸ­è·¯å¾„è®¡ç®—

**è·¯å¾„åˆ†é…ä¸è½¦è¾†ç®¡ç†ï¼ˆPhase 5ï¼‰ï¼š**
- âœ… __create_simulation_worldï¼šåˆ›å»ºæ”¯æŒé¢„å®šè·¯å¾„çš„ä»¿çœŸä¸–ç•Œ
- âœ… __initialize_routesï¼šè½¦è¾†åˆ†ç±»ä¸åˆå§‹è·¯å¾„åˆ†é…
- âœ… __apply_routes_to_vehiclesï¼šå¢å¼ºVehicleè½¦è¾†åˆ›å»º

**UE-DTAä»¿çœŸé›†æˆï¼ˆPhase 6ï¼‰ï¼š**
- âœ… __run_simulationï¼šå®Œæ•´UE-DTAä»¿çœŸå¾ªç¯é›†æˆ
- âœ… __route_choice_updateï¼šè·¯å¾„é€‰æ‹©æ›´æ–°å’Œç»Ÿè®¡
- âœ… __estimate_route_costï¼šè·¯å¾„æˆæœ¬ä¼°ç®—
- âœ… __calculate_actual_vehicle_cost_and_flowï¼šæˆæœ¬è®¡ç®—å’Œå……ç”µæµé‡ç»Ÿè®¡

**å……ç”µæµé‡ç»Ÿè®¡ä¸å¥–åŠ±è®¡ç®—ï¼ˆPhase 7ï¼‰ï¼š**
- âœ… __calculate_rewardsï¼šåŸºäºä»·æ ¼Ã—æµé‡çš„å¥–åŠ±è®¡ç®—
- âœ… è‡ªç¯å……ç”µé“¾è·¯æ£€æµ‹ï¼šä»traveled_routeè¯†åˆ«å……ç”µè¡Œä¸º
- âœ… æŒ‰æ—¶æ®µç»Ÿè®¡å„å……ç”µç«™æµé‡ï¼šæ”¯æŒdeltanè½¦è¾†æ•°é‡æ¦‚å¿µ

**ç”¨æˆ·ä½“éªŒä¼˜åŒ–ï¼ˆPhase 8ï¼‰ï¼š**
- âœ… tqdmè¿›åº¦æ¡ï¼šè¯¦ç»†çš„UE-DTAæ”¶æ•›è¿‡ç¨‹æ˜¾ç¤º
- âœ… æ”¶æ•›/æœªæ”¶æ•›æ¶ˆæ¯ï¼šä½¿ç”¨pbar.write()æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
- âœ… æœ€ç»ˆç»Ÿè®¡è¡¨æ ¼ï¼šTexttableæ˜¾ç¤ºå®é™…è½¦è¾†æ•°é‡ç»Ÿè®¡
- âœ… å®é™…è½¦è¾†æ•°ç»Ÿè®¡ï¼šæ‰€æœ‰æ˜¾ç¤ºå‡ä¸ºVehicleå¯¹è±¡æ•°Ã—deltan

### ğŸ‰ v3.0å®ç°å®Œæˆï¼

EVCSChargingGameEnv v3.0å·²å®Œå…¨å®ç°ï¼ŒåŒ…æ‹¬ï¼š
- å®Œæ•´PettingZoo ParallelEnvæ¥å£
- è‡ªç¯å……ç”µé“¾è·¯å»ºæ¨¡
- å¤šçŠ¶æ€å›¾è·¯å¾„ç®—æ³•
- Monkey Patchå¢å¼ºVehicleä¸¥æ ¼è·¯å¾„æ‰§è¡Œ
- UE-DTAåŠ¨æ€å‡è¡¡ä»¿çœŸ
- å……ç”µæµé‡ç»Ÿè®¡å’Œå¥–åŠ±è®¡ç®—
- è¯¦ç»†çš„è¿›åº¦æ˜¾ç¤ºå’Œç»Ÿè®¡ä¿¡æ¯

## ğŸ‰ Monkey Patché‡æ„å®Œæˆï¼

### âœ… é‡æ„å®ŒæˆçŠ¶æ€ï¼ˆ2025å¹´1æœˆæ›´æ–°ï¼‰

**æ ¸å¿ƒé‡æ„**ï¼šæˆåŠŸå°†åŸºäºç»§æ‰¿çš„`PredefinedRouteVehicle`å’Œ`PredefinedRouteWorld`å®ç°é‡æ„ä¸º**Monkey Patch**æ–¹å¼ï¼Œç»Ÿä¸€è§£å†³äº†æ‰€æœ‰UXSimç›¸å…³é—®é¢˜ã€‚

### âœ… å·²å®Œæˆçš„é‡æ„ä»»åŠ¡

**Phase 1: ç»Ÿä¸€UXSimè¡¥ä¸è®¾è®¡** âœ…
- âœ… åˆ›å»ºç»Ÿä¸€çš„`patch_uxsim()`æ–¹æ³•åœ¨`src/patch.py`
- âœ… è¡¥ä¸åº”ç”¨æ—¶æœºï¼šåœ¨`EVCSChargingGameEnv.__init__()`ä¸­è°ƒç”¨
- âœ… åˆ†æå¹¶å®ç°æ‰€æœ‰å¿…è¦çš„UXSimç»„ä»¶è¡¥ä¸

**Phase 2: Analyzeræ–‡ä»¶å¤¹é—®é¢˜ä¿®å¤** âœ…
- âœ… Monkey patch `uxsim.Analyzer.__init__()`æ–¹æ³•
- âœ… è·³è¿‡`os.makedirs(f"out{s.W.name}", exist_ok=True)`è°ƒç”¨
- âœ… ä¿ç•™å¿…è¦å±æ€§å¹¶ç¡®ä¿å…¼å®¹æ€§

**Phase 3: é¢„å®šè·¯å¾„åŠŸèƒ½è¡¥ä¸** âœ…
- âœ… Monkey patch `uxsim.Vehicle.__init__()`æ·»åŠ é¢„å®šè·¯å¾„æ”¯æŒ
- âœ… æ–°å¢`assign_route(route_names)`æ–¹æ³•
- âœ… å®ç°è½¬ç§»ç¡®è®¤æœºåˆ¶çš„`route_next_link_choice()`è¡¥ä¸
- âœ… å®Œå–„é¢„å®šè·¯å¾„çŠ¶æ€ç®¡ç†çš„`update()`è¡¥ä¸

**Phase 4: Worldç±»å¢å¼º** âœ…
- âœ… æ–°å¢`addVehicle()`æ–¹æ³•æ”¯æŒé¢„å®šè·¯å¾„è½¦è¾†åˆ›å»º
- âœ… é‡å†™`adddemand()`æ–¹æ³•ä½¿ç”¨å¢å¼ºçš„Vehicle

**Phase 5: æ¸…ç†åŸæœ‰å®ç°** âœ…
- âœ… åˆ é™¤`PredefinedRouteVehicle`å’Œ`PredefinedRouteWorld`ç±»å®šä¹‰ï¼ˆçº¦200è¡Œä»£ç ï¼‰
- âœ… æ›´æ–°æ‰€æœ‰è°ƒç”¨ä»£ç ä½¿ç”¨æ ‡å‡†`uxsim.World`å’Œ`uxsim.Vehicle`

**Phase 6: æµ‹è¯•éªŒè¯** âœ…
- âœ… éªŒè¯è¡¥ä¸åçš„ç¯å¢ƒåˆå§‹åŒ–æ­£å¸¸
- âœ… æµ‹è¯•é¢„å®šè·¯å¾„è½¦è¾†ä¸¥æ ¼æŒ‰è·¯å¾„è¡Œé©¶
- âœ… ç¡®è®¤è½¬ç§»ç¡®è®¤æœºåˆ¶è§£å†³è‡ªç¯é—®é¢˜
- âœ… éªŒè¯ä¸å†åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
- âœ… å¤šè½®reset-stepå¾ªç¯æµ‹è¯•é€šè¿‡

### ğŸ¯ é‡æ„æ•ˆæœ

**ä»£ç ç®€åŒ–**ï¼š
- âœ… åˆ é™¤äº†çº¦200è¡Œçš„ç»§æ‰¿ç±»ä»£ç 
- âœ… ç»Ÿä¸€çš„è¡¥ä¸ç®¡ç†ï¼Œç»´æŠ¤æ›´ç®€å•

**åŠŸèƒ½ç»Ÿä¸€**ï¼š
- âœ… åœ¨`src/patch.py`ä¸­ç»Ÿä¸€è§£å†³æ‰€æœ‰UXSimç›¸å…³é—®é¢˜
- âœ… æ›´é€æ˜çš„å®ç°ï¼Œç›´æ¥ä½¿ç”¨æ ‡å‡†`uxsim.World`å’Œ`uxsim.Vehicle`

**æ€§èƒ½æå‡**ï¼š
- âœ… é¿å…æ–‡ä»¶å¤¹åˆ›å»ºçš„I/Oå¼€é”€
- âœ… æ›´è½»é‡çš„å¯¹è±¡åˆ›å»ºè¿‡ç¨‹

**æ¶æ„æ”¹è¿›**ï¼š
- âœ… æ¨¡å—åŒ–è®¾è®¡ï¼šè¡¥ä¸åŠŸèƒ½ç‹¬ç«‹äºä¸»è¦ç¯å¢ƒä»£ç 
- âœ… è¿è¡Œæ—¶å¢å¼ºï¼šé€šè¿‡Monkey Patchè€Œéç»§æ‰¿å®ç°åŠŸèƒ½æ‰©å±•
- âœ… ç»´æŠ¤ä¾¿åˆ©ï¼šæ‰€æœ‰UXSimå¢å¼ºé›†ä¸­åœ¨å•ä¸ªæ–‡ä»¶ä¸­

## ä¾èµ–é¡¹

- **UXSim**: äº¤é€šå¾®è§‚ä»¿çœŸæ¡†æ¶ (v1.8.2)
- **NetworkX**: è·¯å¾„è®¡ç®—çš„å›¾ç®—æ³•
- **PettingZoo**: å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç¯å¢ƒæ¡†æ¶
- **Gymnasium**: ç¯å¢ƒæ¥å£æ ‡å‡†
- **NumPy/Pandas**: æ•°æ®æ“ä½œå’Œåˆ†æ
- **PyTorch**: MADRLçš„æ·±åº¦å­¦ä¹ æ¡†æ¶
- **Matplotlib**: å¯è§†åŒ–å’Œç»˜å›¾

## é…ç½®æ–‡ä»¶å‚æ•°

ç¯å¢ƒä» `siouxfalls_settings.json` è¯»å–é…ç½®ï¼š

```json
{
    // åŸºç¡€åšå¼ˆé…ç½®
    "charging_nodes": {"5": [0.1, 1.0], "12": [0.1, 1.0], "14": [0.1, 1.0], "18": [0.1, 1.0]},
    "charging_periods": 8,
    "charging_car_rate": 0.3,
    "routes_per_od": 5,
    
    // è‡ªç¯å……ç”µé“¾è·¯ç‰©ç†å‚æ•°
    "charging_link_length": 3000,
    "charging_link_free_flow_speed": 10,
    
    // UE-DTAæ±‚è§£å‚æ•°
    "time_value_coefficient": 0.005,
    "charging_demand_per_vehicle": 50,
    "ue_convergence_threshold": 1.0,
    "ue_max_iterations": 100,
    "ue_swap_probability": 0.05,
    
    // ä»¿çœŸæ§åˆ¶å‚æ•°
    "simulation_time": 7200,
    "deltan": 5  // Platoonå¤§å°ï¼šæ¯ä¸ªVehicleå¯¹è±¡ä»£è¡¨çš„å®é™…è½¦è¾†æ•°
}
```

### é‡è¦æ¦‚å¿µè¯´æ˜

**deltanï¼ˆPlatoonæ¦‚å¿µï¼‰**ï¼š
- UXsimä¸­çš„å…³é”®å‚æ•°ï¼Œå®šä¹‰äº†æ¯ä¸ªVehicleå¯¹è±¡ä»£è¡¨å¤šå°‘è¾†å®é™…è½¦è¾†
- ä¾‹å¦‚deltan=5æ—¶ï¼Œä¸€ä¸ªVehicleå¯¹è±¡ä»£è¡¨5è¾†å®é™…è½¦è¾†
- åœ¨ç»Ÿè®¡è½¦è¾†æ•°é‡ã€å……ç”µæµé‡æ—¶éœ€è¦ä¹˜ä»¥deltanè·å¾—å®é™…æ•°å€¼
- å½±å“ä»¿çœŸç²¾åº¦ä¸è®¡ç®—æ•ˆç‡çš„å¹³è¡¡
```

## å…³é”®æŠ€æœ¯å†³ç­–è®°å½•

### v3.0 vs v1.0/v2.0 è®¾è®¡å¯¹æ¯”

| æ–¹é¢ | v1.0 (å……ç”µé“¾è·¯å¤åˆ¶) | v2.0 (èŠ‚ç‚¹åˆ†ç¦») | v3.0 (v2æ¡†æ¶+è‡ªç¯+é¢„å®šè·¯å¾„) |
|------|-------------------|----------------|------------------------------|
| **æ•´ä½“æ¶æ„** | åŸºç¡€å®ç° | å®Œæ•´PettingZooæ¡†æ¶ | ç»§æ‰¿v2æ¡†æ¶ |
| **å……ç”µé“¾è·¯** | å¤åˆ¶ç°æœ‰é“¾è·¯ä¸ºcharging_ç‰ˆæœ¬ | å†…éƒ¨é“¾è·¯è¿æ¥åˆ†ç¦»èŠ‚ç‚¹ | çœŸæ­£è‡ªç¯ï¼šidx->idx |
| **è·¯å¾„æ§åˆ¶** | ä¾èµ–UXSimè·¯å¾„é€‰æ‹© | å¤šçŠ¶æ€å›¾è·¯å¾„æ§åˆ¶ | å¤šçŠ¶æ€å›¾+Monkey Patchå¢å¼ºVehicle |
| **ç½‘ç»œæ‹“æ‰‘** | é“¾è·¯æ•°ç¿»å€ | èŠ‚ç‚¹æ•°ç¿»å€+å†…éƒ¨é“¾è·¯ | ä»…å¢åŠ è‡ªç¯ï¼Œæœ€ç®€æ´ |
| **å……ç”µä¿è¯** | åŒå±‚å›¾æ¦‚ç‡ä¿è¯ | å¼ºåˆ¶æ°å¥½ä¸€æ¬¡ | é¢„å®šè·¯å¾„100%ä¿è¯ |
| **å®ç°å¤æ‚åº¦** | ä¸­ç­‰ | é«˜ | ä¸­ç­‰ï¼ˆç»§æ‰¿v2æ¡†æ¶ï¼‰ |

### v3.0æ ¸å¿ƒä¼˜åŠ¿

1. **æ¶æ„æˆç†Ÿ**: ç»§æ‰¿v2çš„å®Œæ•´PettingZooæ¥å£å’ŒUE-DTAæ¡†æ¶ï¼Œé¿å…é‡å¤å¼€å‘
2. **æ‹“æ‰‘æœ€ç®€**: è‡ªç¯å……ç”µé“¾è·¯æ˜¯æ‰€æœ‰æ–¹æ¡ˆä¸­ç½‘ç»œæ‹“æ‰‘æœ€ç®€æ´çš„
3. **æ§åˆ¶ç²¾ç¡®**: å¤šçŠ¶æ€å›¾+é¢„å®šè·¯å¾„åŒé‡ä¿è¯å……ç”µè¡Œä¸ºçš„ç¡®å®šæ€§
4. **æ‰©å±•ä¾¿åˆ©**: åŸºäºv2æ¡†æ¶ï¼Œæ˜“äºåç»­åŠŸèƒ½æ‰©å±•å’Œç®—æ³•é›†æˆ
5. **éªŒè¯å……åˆ†**: Monkey Patchå¢å¼ºVehicleå·²é€šè¿‡æŠ€æœ¯éªŒè¯ï¼Œç¡®ä¿è·¯å¾„æ‰§è¡Œå¯é æ€§

### éšæœºç§å­ç®¡ç†ç­–ç•¥

- **ç¯å¢ƒåˆå§‹åŒ–**: ä¼ å…¥ç»Ÿä¸€random_seedå‚æ•°æ§åˆ¶æ‰€æœ‰éšæœºæ“ä½œ
- **ä»·æ ¼åˆå§‹åŒ–**: ä½¿ç”¨ç§å­æ§åˆ¶åˆå§‹ä»·æ ¼ç”Ÿæˆ
- **UXSimä»¿çœŸ**: æ¯è½®è®¾ç½®ç›¸åŒç§å­ç¡®ä¿å¯å¤ç°æ€§
- **è·¯å¾„åˆ‡æ¢**: UEç®—æ³•ä¸­çš„éšæœºåˆ‡æ¢ä½¿ç”¨ç§å­æ§åˆ¶

## å¼€å‘æ³¨æ„äº‹é¡¹

- é¡¹ç›®å…¨ç¨‹ä½¿ç”¨ä¸­æ–‡æ³¨é‡Šå’Œå˜é‡å
- é»˜è®¤é…ç½®INFOçº§åˆ«æ—¥å¿—ï¼Œå…³é”®æ­¥éª¤è¾“å‡ºè°ƒè¯•ä¿¡æ¯
- ç¯å¢ƒæ”¯æŒå……ç”µå’Œéå……ç”µè½¦è¾†æ··åˆä»¿çœŸ
- ä½¿ç”¨ç¯å¢ƒåˆå§‹åŒ–å‚æ•°ä¸­çš„éšæœºç§å­è¿›è¡Œå®Œå…¨å¯å¤ç°çš„ä»¿çœŸ
- v3.0å®ç°ä½äº `src/EVCSChargingGameEnv.py`ï¼Œä½¿ç”¨`src/patch.py`æä¾›UXSimå¢å¼ºï¼Œä¿æŒä¸é…ç½®æ–‡ä»¶æ ¼å¼å®Œå…¨å…¼å®¹
- ä¸¥æ ¼éµå¾ªPettingZoo ParallelEnvæ¥å£è§„èŒƒï¼Œç¡®ä¿MADRLç®—æ³•å…¼å®¹æ€§
- å……ç”µé“¾è·¯å‘½åè§„èŒƒï¼š`charging_{node_idx}` ç”¨äºè‡ªç¯é“¾è·¯ `{node_idx} -> {node_idx}`

## âœ… é¢„å®šè·¯å¾„Vehicleé—®é¢˜å·²è§£å†³ï¼ˆ2025å¹´1æœˆæ›´æ–°ï¼‰

### ğŸ¯ é—®é¢˜è§£å†³çŠ¶æ€

**æ ¸å¿ƒé—®é¢˜**ï¼šé¢„å®šè·¯å¾„Vehicleçš„è·¯å¾„æ‰§è¡Œä¸å‡†ç¡®é—®é¢˜å·²é€šè¿‡å®æ–½**è½¬ç§»ç¡®è®¤æœºåˆ¶**å®Œå…¨è§£å†³ï¼Œç°åœ¨é€šè¿‡Monkey Patchæ–¹å¼æä¾›è¯¥åŠŸèƒ½ã€‚

### ğŸš€ æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼šè½¬ç§»ç¡®è®¤æœºåˆ¶

åŸºäºå¯¹UXSimå†…éƒ¨è½¬ç§»æœºåˆ¶çš„æ·±åº¦ç†è§£ï¼ŒæˆåŠŸå®æ–½äº†æœ€ä¼˜è§£å†³æ–¹æ¡ˆï¼š

#### æ ¸å¿ƒæ€æƒ³
é€šè¿‡æ¯”è¾ƒ`route_next_link`ä¸å½“å‰`link`æ¥ç¡®è®¤è½¬ç§»æ˜¯å¦æˆåŠŸï¼Œåªæœ‰ç¡®è®¤è½¬ç§»æˆåŠŸåæ‰é€’å¢`route_index`

#### å®ç°å…³é”®ï¼ˆå·²åœ¨src/patch.pyä¸­å®ç°ï¼‰
```python
def route_next_link_choice(s):
    """
    é€‰æ‹©ä¸‹ä¸€ä¸ªé“¾è·¯ - ä½¿ç”¨è½¬ç§»ç¡®è®¤æœºåˆ¶ç¡®ä¿ä¸¥æ ¼æŒ‰ç…§é¢„å®šè·¯å¾„æ‰§è¡Œ
    
    æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡æ¯”è¾ƒroute_next_linkä¸å½“å‰linkæ¥ç¡®è®¤è½¬ç§»æ˜¯å¦æˆåŠŸï¼Œ
    åªæœ‰ç¡®è®¤è½¬ç§»æˆåŠŸåæ‰é€’å¢route_index
    """

    # æ­¥éª¤1ï¼šæ£€æŸ¥è½¬ç§»ç¡®è®¤
    if (s.route_next_link is not None and 
        s.link is not None and 
        s.route_next_link == s.link):
        # å½“å‰è·¯å¾„å·²å®Œæˆ, é€’å¢ route_index
        s.route_index += 1
    
    # æ­¥éª¤2ï¼šé€‰æ‹©ä¸‹ä¸€ä¸ªç›®æ ‡é“¾è·¯
    if s.route_index >= len(s.predefined_route_links):
        # è·¯å¾„å®Œæˆ, è®¾ç½®route_next_linkä¸ºNone
        s.route_next_link = None
        return
    
    # è®¾ç½®ä¸‹ä¸€ä¸ªé¢„å®šé“¾è·¯ä½œä¸ºç›®æ ‡
    s.route_next_link = s.predefined_route_links[s.route_index]
```

### âœ… è§£å†³æ–¹æ¡ˆä¼˜åŠ¿

1. **é€»è¾‘ç®€æ´**ï¼šæ ¸å¿ƒé€»è¾‘åªéœ€å‡ è¡Œä»£ç ï¼ŒåŸºäºç®€å•çš„å¯¹è±¡å¼•ç”¨æ¯”è¾ƒ
2. **è‡ªåŠ¨çº é”™**ï¼šèŠ‚ç‚¹é˜»å¡æ—¶è‡ªåŠ¨ä¿æŒçŠ¶æ€ä¸å˜ï¼Œè½¬ç§»æˆåŠŸæ—¶æ‰å‰è¿›
3. **ä¸UXSimåŒæ­¥**ï¼šå®Œå…¨ç¬¦åˆUXSimå†…éƒ¨è½¬ç§»æœºåˆ¶ï¼Œæ— éœ€é¢å¤–çŠ¶æ€ç®¡ç†
4. **æ— éœ€é˜²é‡å¤é€»è¾‘**ï¼šåŸºäºå®é™…è½¬ç§»ç»“æœåˆ¤æ–­ï¼Œè‡ªç„¶é¿å…é‡å¤è°ƒç”¨é—®é¢˜

### ğŸ¯ è§£å†³çš„å…³é”®é—®é¢˜

1. **è‡ªç¯é“¾è·¯é‡å¤è½¬ç§»è¯·æ±‚**ï¼šé€šè¿‡è½¬ç§»ç¡®è®¤æœºåˆ¶è‡ªç„¶é˜²æ­¢é‡å¤å¤„ç†
2. **route_indexå¼‚å¸¸è·³è·ƒ**ï¼šåªæœ‰ç¡®è®¤è½¬ç§»æˆåŠŸæ—¶æ‰é€’å¢ç´¢å¼•ï¼Œé¿å…è·³è·ƒ
3. **èŠ‚ç‚¹é˜»å¡åœºæ™¯**ï¼šè½¬ç§»å¤±è´¥æ—¶route_indexä¿æŒä¸å˜ï¼Œè‡ªåŠ¨ç­‰å¾…ä¸‹æ¬¡æœºä¼š
4. **è·¯å¾„æ‰§è¡Œç²¾ç¡®æ€§**ï¼šç¡®ä¿è½¦è¾†ä¸¥æ ¼æŒ‰é¢„å®šè·¯å¾„è¡Œé©¶ï¼Œ100%è·¯å¾„åŒ¹é…

### ğŸ”¬ é—®é¢˜æ ¹æºå›é¡¾

é€šè¿‡æ·±åº¦è°ƒè¯•å‘ç°çš„æ ¹æœ¬åŸå› ï¼š
1. **UXSimè‡ªç¯é“¾è·¯**è§¦å‘å¤šæ¬¡è½¬ç§»è¯·æ±‚ï¼ˆå†…éƒ¨æœºåˆ¶ç‰¹æ€§ï¼‰
2. **route_indexç®¡ç†é—®é¢˜**ï¼šåœ¨æŸäº›æƒ…å†µä¸‹è¢«æ„å¤–ä¿®æ”¹æˆ–é‡å¤é€’å¢
3. **è½¬ç§»æ—¶åºé—®é¢˜**ï¼šè½¬ç§»è¯·æ±‚ä¸ç¡®è®¤ä¹‹é—´çš„æ—¶åºä¸ä¸€è‡´

### ğŸ“Š å®æ–½æ•ˆæœéªŒè¯

**é¢„æœŸæ•ˆæœ**ï¼ˆå·²å®ç°ï¼‰ï¼š
- âœ… é˜²æ­¢è‡ªç¯å……ç”µé“¾è·¯çš„é‡å¤è½¬ç§»é—®é¢˜
- âœ… é˜²æ­¢`route_index`å¼‚å¸¸è·³è·ƒ
- âœ… ç¡®ä¿è½¦è¾†ä¸¥æ ¼æŒ‰é¢„å®šè·¯å¾„è¡Œé©¶
- âœ… æä¾›æœ€ç®€æ´ã€å¯é çš„å®ç°æ–¹æ¡ˆ

### ğŸ† æŠ€æœ¯æ–¹æ¡ˆè¯„ä¼°

**æœ€ç»ˆé€‰æ‹©**ï¼šğŸ¥‡ **è½¬ç§»ç¡®è®¤æœºåˆ¶**ï¼ˆå·²å®æ–½ï¼‰
- **ç®€æ´æ€§**ï¼šæ ¸å¿ƒé€»è¾‘ä»…éœ€å‡ è¡Œä»£ç 
- **å¯é æ€§**ï¼šåŸºäºUXSimå†…éƒ¨æœºåˆ¶ï¼Œå®Œå…¨åŒæ­¥
- **æ˜“ç»´æŠ¤æ€§**ï¼šé€»è¾‘æ¸…æ™°ï¼Œæ— å¤æ‚çŠ¶æ€ç®¡ç†

**æ›¿ä»£æ–¹æ¡ˆ**ï¼š
- ğŸ¥ˆ é˜²é‡å¤è°ƒç”¨æœºåˆ¶ - å¤æ‚åº¦è¾ƒé«˜ï¼Œéœ€è¦é¢å¤–çŠ¶æ€è·Ÿè¸ª
- ğŸ¥‰ ç´¢å¼•ç®¡ç†åŠ å›º - æ²»æ ‡ä¸æ²»æœ¬ï¼Œæ— æ³•ä»æ ¹æºè§£å†³é—®é¢˜

### ç›¸å…³å®ç°æ–‡ä»¶

- **è§£å†³æ–¹æ¡ˆå®ç°**ï¼š`src/patch.py` - _patched_vehicle_route_next_link_choice()æ–¹æ³•ï¼ˆè½¬ç§»ç¡®è®¤æœºåˆ¶ï¼‰
- **ä¸»è¦ç¯å¢ƒ**ï¼š`src/EVCSChargingGameEnv.py` - åº”ç”¨è¡¥ä¸çš„v3.0ç¯å¢ƒå®ç°
- **æŠ€æœ¯éªŒè¯å‚è€ƒ**ï¼š`src/technical_validation.py` - åŸå§‹å·¥ä½œç‰ˆæœ¬
- **æµ‹è¯•éªŒè¯**ï¼š`main.py` - ç¯å¢ƒæµ‹è¯•å…¥å£

## ğŸ¯ MADDPGç®€åŒ–è®­ç»ƒæ¡†æ¶è®¾è®¡ï¼ˆ2025å¹´1æœˆæ›´æ–°ï¼‰

### ğŸ¨ æ•´ä½“è®¾è®¡ç†å¿µ

**æ ¸å¿ƒç›®æ ‡**ï¼šä¸“æ³¨äºæ±‚è§£å……ç”µç«™ä»·æ ¼åšå¼ˆçš„çº³ä»€å‡è¡¡ï¼Œæ‘’å¼ƒä¼ ç»ŸRLçš„"æ³›åŒ–å­¦ä¹ "æ€ç»´

**è®¾è®¡åŸåˆ™**ï¼š
1. **æ˜ç¡®å±‚æ¬¡åˆ†ç¦»**ï¼šEpisodeï¼ˆåšå¼ˆå°è¯•ï¼‰ â†’ Stepï¼ˆç­–ç•¥è°ƒæ•´ï¼‰ â†’ UE-DTAï¼ˆç¯å¢ƒå“åº”ï¼‰
2. **æ”¶æ•›å¯¼å‘**ï¼šä»¥çº³ä»€å‡è¡¡æ”¶æ•›ä¸ºä¸»è¦ç»ˆæ­¢æ¡ä»¶ï¼Œè€Œéå›ºå®šepisodeæ•°
3. **è¿›åº¦é€æ˜**ï¼šä¸‰å±‚è¿›åº¦ç›‘æ§ï¼Œè®©ç”¨æˆ·æ¸…æ¥šäº†è§£æ±‚è§£è¿‡ç¨‹
4. **é…ç½®é©±åŠ¨**ï¼šé€šè¿‡MADDPGConfigå’ŒTrainingConfigå®ç°å®Œå…¨å¯é…ç½®

### ğŸ—ï¸ SimplifiedTraineræ¶æ„è®¾è®¡

#### æ ¸å¿ƒç±»ç»“æ„
```python
class SimplifiedTrainer:
    """
    MADDPGå……ç”µç«™ä»·æ ¼åšå¼ˆç®€åŒ–è®­ç»ƒå™¨
    
    ä¸“æ³¨äºçº³ä»€å‡è¡¡æ±‚è§£ï¼Œé‡‡ç”¨æ¸…æ™°çš„ä¸‰å±‚ç»“æ„ï¼š
    - Episodeå±‚ï¼šåšå¼ˆæ±‚è§£å°è¯•
    - Stepå±‚ï¼šæ™ºèƒ½ä½“ç­–ç•¥è°ƒæ•´  
    - UE-DTAå±‚ï¼šäº¤é€šä»¿çœŸå“åº”
    """
    
    def __init__(self, config: TrainingConfig, logger):
        self.config = config
        self.logger = logger
        self.env = None
        self.agents = {}
        
        # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.episode_rewards = []
        self.convergence_episodes = []
        self.total_ue_iterations = 0
    
    def train(self) -> Dict:
        """ä¸»è®­ç»ƒå¾ªç¯ï¼šå¯»æ‰¾çº³ä»€å‡è¡¡"""
        
    def evaluate(self, num_episodes: int = 20) -> Dict:
        """è¯„ä¼°è®­ç»ƒæ•ˆæœï¼šæµ‹è¯•å‡è¡¡ç¨³å®šæ€§"""
        
    def get_nash_equilibrium(self) -> Dict:
        """è·å–å½“å‰çº³ä»€å‡è¡¡è§£"""
```

#### ä¸‰å±‚è¿›åº¦ç›‘æ§ç³»ç»Ÿ
```python
# ç¬¬ä¸€å±‚ï¼šEpisodeè¿›åº¦ï¼ˆå¤–å±‚è¿›åº¦æ¡ï¼‰
episode_pbar = tqdm(
    total=config.max_episodes,
    desc="ğŸ¯ å¯»æ‰¾çº³ä»€å‡è¡¡",
    unit="episode"
)

# ç¬¬äºŒå±‚ï¼šStepè¿›åº¦ï¼ˆä¸­å±‚è¿›åº¦æ¡ï¼‰ 
step_pbar = tqdm(
    total=config.max_steps_per_episode,
    desc=f"ğŸ“ˆ Episode {episode}",
    unit="step",
    leave=False
)

# ç¬¬ä¸‰å±‚ï¼šUE-DTAè¿›åº¦ï¼ˆå†…å±‚ï¼Œåœ¨ç¯å¢ƒå†…éƒ¨ï¼‰
# ç”±EVCSChargingGameEnvæä¾›çš„UE-DTAä»¿çœŸè¿›åº¦
ue_pbar = tqdm(
    desc="ğŸš— UE-DTAä»¿çœŸ",
    unit="iteration"
)
```

### ğŸ”„ è®­ç»ƒä¸»å¾ªç¯è®¾è®¡

#### Episodeå±‚ï¼šåšå¼ˆæ±‚è§£å°è¯•
```python
def train(self) -> Dict:
    """
    Episodeå±‚é€»è¾‘ï¼šæ¯ä¸ªepisodeéƒ½æ˜¯å¯¹åŒä¸€ä»·æ ¼åšå¼ˆçš„æ±‚è§£å°è¯•
    æˆåŠŸæ ‡å‡†ï¼šåœ¨å•ä¸ªepisodeä¸­è¾¾åˆ°çº³ä»€å‡è¡¡ï¼ˆä»·æ ¼æ”¶æ•›ï¼‰
    """
    
    convergence_count = 0
    
    with tqdm(total=self.config.max_episodes, desc="ğŸ¯ å¯»æ‰¾çº³ä»€å‡è¡¡", unit="episode") as episode_pbar:
        
        for episode in range(self.config.max_episodes):
            
            # é‡æ–°åˆå§‹åŒ–åŒä¸€åšå¼ˆï¼ˆä¸æ˜¯æ–°åšå¼ˆï¼‰
            observations = self.env.reset()
            episode_rewards = {agent: 0.0 for agent in self.agents.keys()}
            
            # Stepå±‚ï¼šç­–ç•¥è°ƒæ•´å¾ªç¯
            converged_in_episode = self._run_episode(episode, episode_rewards)
            
            if converged_in_episode:
                convergence_count += 1
                self.convergence_episodes.append(episode)
                episode_pbar.set_postfix({
                    "æ”¶æ•›æ¬¡æ•°": convergence_count,
                    "æ”¶æ•›ç‡": f"{convergence_count/(episode+1):.1%}"
                })
                
                # å¯é€‰ï¼šå¦‚æœè¿ç»­å¤šæ¬¡æ”¶æ•›ï¼Œå¯æå‰ç»“æŸ
                if self._check_stable_convergence():
                    self.logger.info(f"è¿ç»­æ”¶æ•›ï¼Œè®­ç»ƒæå‰ç»“æŸäºepisode {episode}")
                    break
            
            self.episode_rewards.append(episode_rewards)
            episode_pbar.update(1)
    
    return self._generate_training_results()
```

#### Stepå±‚ï¼šç­–ç•¥è°ƒæ•´
```python
def _run_episode(self, episode: int, episode_rewards: Dict) -> bool:
    """
    Stepå±‚é€»è¾‘ï¼šåœ¨å•ä¸ªepisodeå†…è°ƒæ•´æ™ºèƒ½ä½“ç­–ç•¥ç›´åˆ°æ”¶æ•›æˆ–è¶…æ—¶
    è¿”å›ï¼šæ˜¯å¦åœ¨è¯¥episodeå†…æ”¶æ•›åˆ°çº³ä»€å‡è¡¡
    """
    
    with tqdm(total=self.config.max_steps_per_episode, 
              desc=f"ğŸ“ˆ Episode {episode}", unit="step", leave=False) as step_pbar:
        
        for step in range(self.config.max_steps_per_episode):
            
            # æ™ºèƒ½ä½“å†³ç­–ï¼ˆåŸºäºè§‚æµ‹é€‰æ‹©åŠ¨ä½œï¼‰
            actions = {agent: agent_obj.act(observations[agent]) 
                      for agent, agent_obj in self.agents.items()}
            
            # UE-DTAå±‚ï¼šç¯å¢ƒå“åº”ï¼ˆåŒ…å«å®Œæ•´çš„äº¤é€šä»¿çœŸï¼‰
            observations, rewards, terminations, truncations, infos = self.env.step(actions)
            
            # ç´¯ç§¯å¥–åŠ±
            for agent in self.agents.keys():
                episode_rewards[agent] += rewards[agent]
            
            # æ™ºèƒ½ä½“å­¦ä¹ ï¼ˆæ›´æ–°ç­–ç•¥ç½‘ç»œï¼‰
            for agent, agent_obj in self.agents.items():
                agent_obj.learn()
            
            # æ£€æŸ¥æ˜¯å¦åœ¨ç¯å¢ƒå±‚é¢æ”¶æ•›ï¼ˆçº³ä»€å‡è¡¡ï¼‰
            if terminations.get('__all__', False):
                self.logger.info(f"Episode {episode} Step {step}: è¾¾åˆ°çº³ä»€å‡è¡¡!")
                step_pbar.set_postfix({"çŠ¶æ€": "âœ…æ”¶æ•›"})
                step_pbar.update(self.config.max_steps_per_episode - step)
                return True
            
            # æ›´æ–°è¿›åº¦æ¡
            step_pbar.set_postfix({
                "å¹³å‡å¥–åŠ±": f"{np.mean(list(rewards.values())):.2f}",
                "UEè¿­ä»£": infos.get('ue_iterations', 0)
            })
            step_pbar.update(1)
    
    # Episodeè¶…æ—¶ï¼Œæœªæ”¶æ•›
    step_pbar.set_postfix({"çŠ¶æ€": "â±ï¸è¶…æ—¶"})
    return False
```

#### UE-DTAå±‚ï¼šç¯å¢ƒå“åº”
```python
# UE-DTAå±‚ç”±EVCSChargingGameEnv.step()å®ç°
# ä¸»è¦èŒè´£ï¼š
# 1. æ¥æ”¶ä»·æ ¼åŠ¨ä½œï¼Œæ›´æ–°å……ç”µç«™å®šä»·
# 2. æ‰§è¡Œå®Œæ•´çš„UE-DTAä»¿çœŸï¼ˆå¤šè½®è·¯å¾„è°ƒæ•´ç›´åˆ°äº¤é€šå‡è¡¡ï¼‰
# 3. è®¡ç®—å……ç”µæµé‡å’Œæ”¶ç›Šå¥–åŠ±
# 4. åˆ¤æ–­ä»·æ ¼æ˜¯å¦æ”¶æ•›åˆ°çº³ä»€å‡è¡¡
# 5. æä¾›å†…å±‚UE-DTAä»¿çœŸè¿›åº¦æ¡

def step(self, actions):
    """ç¯å¢ƒstepæ–¹æ³•ä¸­çš„UE-DTAä»¿çœŸ"""
    # è®¾ç½®æ–°ä»·æ ¼
    self._update_charging_prices(actions)
    
    # æ‰§è¡ŒUE-DTAä»¿çœŸï¼ˆå†…å±‚è¿›åº¦æ¡ï¼‰
    with tqdm(desc="ğŸš— UE-DTAä»¿çœŸ", unit="iteration") as ue_pbar:
        ue_converged = self._run_ue_simulation(ue_pbar)
    
    # è®¡ç®—å¥–åŠ±å’Œè§‚æµ‹
    rewards = self._calculate_rewards()
    observations = self._get_observations()
    
    # åˆ¤æ–­çº³ä»€å‡è¡¡æ”¶æ•›
    nash_converged = self._check_nash_convergence()
    
    return observations, rewards, {'__all__': nash_converged}, {}, {'ue_iterations': self.ue_iterations}
```

### ğŸ“Š æ”¶æ•›æ£€æµ‹è®¾è®¡

#### ç¯å¢ƒå±‚æ”¶æ•›ï¼ˆNashå‡è¡¡ï¼‰
```python
def _check_nash_convergence(self) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦è¾¾åˆ°çº³ä»€å‡è¡¡ï¼ˆåœ¨EVCSChargingGameEnvä¸­å®ç°ï¼‰
    åŸºäºä»·æ ¼å˜åŒ–çš„ç›¸å¯¹é˜ˆå€¼åˆ¤æ–­
    """
    if len(self.price_history) < 2:
        return False
    
    current_prices = self.price_history[-1]
    previous_prices = self.price_history[-2]
    
    relative_changes = np.abs(current_prices - previous_prices) / (previous_prices + 1e-8)
    avg_relative_change = np.mean(relative_changes)
    
    return avg_relative_change < self.config.convergence_threshold
```

#### è®­ç»ƒç¨³å®šæ€§è¯„ä¼°
```python
def _check_stable_convergence(self) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç¨³å®šæ”¶æ•›ï¼ˆè¿ç»­å¤šæ¬¡episodeéƒ½æ”¶æ•›ï¼‰
    å¯ç”¨äºæå‰ç»ˆæ­¢è®­ç»ƒ
    """
    if len(self.convergence_episodes) < 3:
        return False
    
    # æ£€æŸ¥æœ€è¿‘3æ¬¡episodeæ˜¯å¦éƒ½æ”¶æ•›
    recent_episodes = list(range(len(self.episode_rewards)))[-3:]
    return all(ep in self.convergence_episodes for ep in recent_episodes)
```

### ğŸ¯ å…³é”®è®¾è®¡ç‰¹ç‚¹

#### 1. å±‚æ¬¡æ¸…æ™°çš„è¿›åº¦ç›‘æ§
- **Episodeå±‚**ï¼šæ˜¾ç¤ºåšå¼ˆæ±‚è§£æ€»ä½“è¿›åº¦å’Œæ”¶æ•›ç»Ÿè®¡
- **Stepå±‚**ï¼šæ˜¾ç¤ºå•æ¬¡åšå¼ˆå†…çš„ç­–ç•¥è°ƒæ•´è¿‡ç¨‹
- **UE-DTAå±‚**ï¼šæ˜¾ç¤ºäº¤é€šä»¿çœŸçš„å†…éƒ¨è¿­ä»£è¿‡ç¨‹

#### 2. çº³ä»€å‡è¡¡å¯¼å‘çš„ç»ˆæ­¢æ¡ä»¶
```python
# ä¸»è¦ç»ˆæ­¢æ¡ä»¶ï¼šç¯å¢ƒå±‚çº³ä»€å‡è¡¡æ”¶æ•›
if terminations.get('__all__', False):
    return "æ‰¾åˆ°çº³ä»€å‡è¡¡"

# è¾…åŠ©ç»ˆæ­¢æ¡ä»¶ï¼šè¿ç»­ç¨³å®šæ”¶æ•›
if self._check_stable_convergence():
    return "è®­ç»ƒç¨³å®šæ”¶æ•›"

# ä¿æŠ¤ç»ˆæ­¢æ¡ä»¶ï¼šæœ€å¤§episodeæ•°
if episode >= self.config.max_episodes:
    return "è¾¾åˆ°æœ€å¤§è®­ç»ƒè½®æ•°"
```

#### 3. ä¸°å¯Œçš„è®­ç»ƒç»Ÿè®¡
```python
def _generate_training_results(self) -> Dict:
    """ç”Ÿæˆå®Œæ•´çš„è®­ç»ƒç»“æœç»Ÿè®¡"""
    return {
        'total_episodes': len(self.episode_rewards),
        'total_convergences': len(self.convergence_episodes),
        'convergence_rate': len(self.convergence_episodes) / len(self.episode_rewards),
        'average_reward': np.mean([sum(ep_reward.values()) for ep_reward in self.episode_rewards]),
        'average_episode_length': np.mean(self.episode_lengths),
        'total_ue_iterations': self.total_ue_iterations,
        'convergence_episodes': self.convergence_episodes,
        'final_nash_equilibrium': self.get_nash_equilibrium()
    }
```

### ğŸ”§ é›†æˆé…ç½®ç³»ç»Ÿ

#### MADDPGConfigé›†æˆ
```python
def _create_agents(self):
    """åŸºäºé…ç½®åˆ›å»ºMADDPGæ™ºèƒ½ä½“"""
    maddpg_config = self.config.maddpg_config
    
    for agent_id in self.env.agents:
        agent = MADDPGAgent(
            obs_dim=self.env.observation_space(agent_id).shape[0],
            action_dim=self.env.action_space(agent_id).shape[0],
            actor_hidden_sizes=maddpg_config.actor_hidden_sizes,
            critic_hidden_sizes=maddpg_config.critic_hidden_sizes,
            actor_lr=maddpg_config.actor_lr,
            critic_lr=maddpg_config.critic_lr,
            # ... å…¶ä»–é…ç½®å‚æ•°
        )
        self.agents[agent_id] = agent
```

#### TrainingConfigé›†æˆ
```python
def _setup_environment(self):
    """åŸºäºè®­ç»ƒé…ç½®è®¾ç½®ç¯å¢ƒ"""
    self.env = EVCSChargingGameEnv(
        network_dir=self.config.network_dir,
        network_name=self.config.network_name,
        max_steps_per_episode=self.config.max_steps_per_episode,
        convergence_threshold=self.config.convergence_threshold,
        random_seed=self.config.seed,
        device=self.config.device
    )
```

### ğŸ‰ æ¡†æ¶ä¼˜åŠ¿æ€»ç»“

1. **èŒè´£æ¸…æ™°**ï¼šä¸‰å±‚ç»“æ„å„å¸å…¶èŒï¼ŒEpisodeæ±‚è§£åšå¼ˆâ†’Stepè°ƒæ•´ç­–ç•¥â†’UE-DTAå“åº”ç¯å¢ƒ
2. **ç›®æ ‡æ˜ç¡®**ï¼šä¸“æ³¨çº³ä»€å‡è¡¡æ±‚è§£ï¼Œä¸è¿½æ±‚ä¼ ç»ŸRLçš„æ³›åŒ–æ€§èƒ½
3. **è¿›åº¦é€æ˜**ï¼šå¤šå±‚è¿›åº¦æ¡è®©ç”¨æˆ·äº†è§£è®­ç»ƒçš„æ¯ä¸ªé˜¶æ®µ
4. **é…ç½®çµæ´»**ï¼šå®Œå…¨åŸºäºé…ç½®æ–‡ä»¶ï¼Œæ˜“äºè°ƒæ•´å’Œå®éªŒ
5. **ç»Ÿè®¡ä¸°å¯Œ**ï¼šæä¾›è¯¦å°½çš„æ”¶æ•›ç»Ÿè®¡å’Œå‡è¡¡åˆ†æ
6. **æ˜“äºæ‰©å±•**ï¼šæ¨¡å—åŒ–è®¾è®¡ä¾¿äºæ·»åŠ æ–°åŠŸèƒ½æˆ–ç®—æ³•å¯¹æ¯”

### ğŸ” çº³ä»€å‡è¡¡è§£éªŒè¯æ–¹æ¡ˆè®¾è®¡

#### å•æ–¹é¢åç¦»æµ‹è¯•ï¼ˆUnilateral Deviation Testï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šéªŒè¯çº³ä»€å‡è¡¡çš„å®šä¹‰ - åœ¨å‡è¡¡ç‚¹ï¼Œä»»ä½•æ™ºèƒ½ä½“å•æ–¹é¢æ”¹å˜ç­–ç•¥éƒ½ä¸ä¼šè·å¾—æ›´é«˜æ”¶ç›Šã€‚

**å®ç°æ­¥éª¤**ï¼š

1. **è·å–å‡è¡¡è§£**ï¼š
   ```python
   equilibrium_prices = {
       'agent1': [0.3, 0.4, 0.5, ...],  # 8ä¸ªæ—¶æ®µä»·æ ¼
       'agent2': [0.6, 0.7, 0.8, ...],
       'agent3': [0.2, 0.3, 0.4, ...], 
       'agent4': [0.5, 0.6, 0.7, ...]
   }
   equilibrium_rewards = {...}  # å¯¹åº”çš„å‡è¡¡æ”¶ç›Š
   ```

2. **ç”Ÿæˆåç¦»ç­–ç•¥**ï¼š
   ```python
   def generate_deviation_strategies(agent_id, equilibrium_prices):
       current_strategy = equilibrium_prices[agent_id]
       deviation_strategies = []
       
       # ç­–ç•¥1ï¼šéšæœºåç¦»ï¼ˆ10ç§ï¼‰
       for _ in range(10):
           random_strategy = np.random.uniform(0.1, 1.0, 8)
           deviation_strategies.append(random_strategy)
       
       # ç­–ç•¥2ï¼šç³»ç»Ÿæ€§åç¦»ï¼ˆä»·æ ¼è°ƒæ•´Â±5%, Â±10%ï¼‰
       for delta in [-0.1, -0.05, +0.05, +0.1]:
           adjusted_strategy = np.clip(current_strategy + delta, 0.1, 1.0)
           deviation_strategies.append(adjusted_strategy)
       
       # ç­–ç•¥3ï¼šå•æ—¶æ®µåç¦»ï¼ˆæ¯ä¸ªæ—¶æ®µæµ‹è¯•æå€¼ï¼‰
       for period in range(8):
           for new_price in [0.1, 0.5, 1.0]:
               modified_strategy = current_strategy.copy()
               modified_strategy[period] = new_price
               deviation_strategies.append(modified_strategy)
       
       return deviation_strategies
   ```

3. **æµ‹è¯•åç¦»æ”¶ç›Š**ï¼š
   ```python
   def test_single_deviation(deviating_agent, deviation_strategy, equilibrium_prices):
       # æ„å»ºæµ‹è¯•ä»·æ ¼ç»„åˆï¼šå…¶ä»–agentä¿æŒå‡è¡¡ä»·æ ¼ï¼Œå½“å‰agentä½¿ç”¨åç¦»ç­–ç•¥
       test_prices = equilibrium_prices.copy()
       test_prices[deviating_agent] = deviation_strategy
       
       # é€šè¿‡ç¯å¢ƒè®¡ç®—åç¦»æ”¶ç›Š
       deviation_reward = self.env.calculate_reward_for_prices(test_prices)
       
       return deviation_reward[deviating_agent]
   ```

4. **éªŒè¯çº³ä»€æ€§è´¨**ï¼š
   ```python
   def evaluate_nash_property():
       equilibrium = get_nash_equilibrium()
       violations = 0
       
       for agent in agent_ids:
           equilibrium_reward = equilibrium['rewards'][agent]
           deviation_strategies = generate_deviation_strategies(agent, equilibrium['prices'])
           
           for deviation_strategy in deviation_strategies:
               deviation_reward = test_single_deviation(agent, deviation_strategy, equilibrium['prices'])
               
               if deviation_reward > equilibrium_reward + tolerance:  # è€ƒè™‘æ•°å€¼è¯¯å·®
                   violations += 1
                   print(f"è¿åçº³ä»€å‡è¡¡ï¼šAgent {agent} é€šè¿‡åç¦»è·å¾—æ›´é«˜æ”¶ç›Š")
       
       return {
           'is_nash_equilibrium': violations == 0,
           'total_violations': violations,
           'test_count': len(agent_ids) * len(deviation_strategies)
       }
   ```

**å®ç°è¦æ±‚**ï¼š
- **ç¯å¢ƒæ¥å£æ‰©å±•**ï¼šéœ€è¦ç¯å¢ƒæä¾›ç›´æ¥è®¡ç®—ç‰¹å®šä»·æ ¼ç»„åˆæ”¶ç›Šçš„æ–¹æ³•
- **é‡‡æ ·ç­–ç•¥**ï¼šå¹³è¡¡æµ‹è¯•è¦†ç›–åº¦ä¸è®¡ç®—æ•ˆç‡
- **æ•°å€¼å®¹å¿åº¦**ï¼šè€ƒè™‘ä»¿çœŸè¯¯å·®ï¼Œè®¾ç½®åˆç†çš„æ”¶ç›Šæ¯”è¾ƒé˜ˆå€¼

**éªŒè¯æ ‡å‡†**ï¼š
- âœ… **ä¸¥æ ¼çº³ä»€å‡è¡¡**ï¼šæ‰€æœ‰åç¦»æ”¶ç›Š â‰¤ å‡è¡¡æ”¶ç›Š
- âš ï¸ **è¿‘ä¼¼çº³ä»€å‡è¡¡**ï¼šåç¦»æ”¶ç›Šæå‡ < å®¹å¿é˜ˆå€¼
- âŒ **éçº³ä»€å‡è¡¡**ï¼šå­˜åœ¨æ˜¾è‘—æ”¶ç›Šæå‡çš„åç¦»ç­–ç•¥

**é¢„æœŸå®ç°**ï¼šåœ¨MADDPGTrainer.evaluate()æ–¹æ³•ä¸­é›†æˆæ­¤éªŒè¯æ–¹æ¡ˆï¼Œä½œä¸ºè®­ç»ƒç»“æœè´¨é‡è¯„ä¼°çš„é‡è¦æŒ‡æ ‡ã€‚

## ğŸš€ MADDPGTrainerå®ç°è¿›åº¦ï¼ˆ2025å¹´1æœˆæ›´æ–°ï¼‰

### âœ… å·²å®Œæˆçš„ä»»åŠ¡

**Phase 1: æ¡†æ¶è®¾è®¡ä¸è®°å½•** âœ…
- âœ… ç¡®å®šSimplifiedTraineræ•´ä½“æ¶æ„ï¼šä¸‰å±‚ç»“æ„(Episodeâ†’Stepâ†’UE-DTA)
- âœ… å•æ–¹é¢åç¦»æµ‹è¯•æ–¹æ¡ˆè®¾è®¡å¹¶è®°å½•åˆ°æ–‡æ¡£
- âœ… åˆ›å»ºMADDPGTraineråŸºæœ¬ç±»ç»“æ„(src/trainer/MADDPGTrainer.py)

**Phase 2: ç¯å¢ƒå¢å¼º** âœ…
- âœ… ä¿®æ”¹EVCSChargingGameEnvç¯å¢ƒï¼Œå¢å¼ºinfosè¿”å›ä¿¡æ¯ï¼š
  - `ue_converged`: UE-DTAæ˜¯å¦æ”¶æ•› (bool)
  - `ue_iterations`: UE-DTAè¿­ä»£æ¬¡æ•° (int)
  - `ue_stats`: __route_choice_updateçš„æœ€åç»Ÿè®¡ä¿¡æ¯ (dict)
  - rewardsæœ¬èº«å°±æ˜¯raw_rewardsï¼Œæ— éœ€é¢å¤–å¤„ç†

**Phase 3: è®­ç»ƒç»Ÿè®¡è®¾è®¡** âœ…
- âœ… ç¡®å®šè®­ç»ƒè¿‡ç¨‹ç»Ÿè®¡å†…å®¹ï¼š
  ```python
  # æ¯æ­¥è¯¦ç»†è®°å½•
  self.step_records = []  # è®°å½•æ¯æ­¥çš„actions, actual_prices, raw_rewards, ue_info
  
  # Episodeçº§åˆ«ç»Ÿè®¡
  self.convergence_episodes = []   # æ”¶æ•›çš„episodeåˆ—è¡¨
  self.episode_lengths = []        # æ¯ä¸ªepisodeçš„é•¿åº¦
  ```

### ğŸ”§ å½“å‰å‘ç°çš„æŠ€æœ¯é—®é¢˜

**å…¨å±€è§‚æµ‹ç»´åº¦é‡å¤é—®é¢˜**ï¼š
- **é—®é¢˜**ï¼šæ¯ä¸ªagentè§‚æµ‹åŒ…å«`last_round_all_prices`(å…¨å±€ä»·æ ¼)ï¼Œå¯¼è‡´åœ¨ç»„ç»‡å…¨å±€çŠ¶æ€æ—¶é‡å¤4æ¬¡
- **å½“å‰å®ç°**ï¼š`organize_global_state`ç›´æ¥æ‹¼æ¥ï¼Œæ— å»é‡å¤„ç†
- **å½±å“**ï¼šå…¨å±€è§‚æµ‹ç»´åº¦192ç»´ï¼Œå®é™…éœ€è¦96ç»´ï¼Œæµªè´¹ä¸€åŠå‚æ•°é‡
- **è§£å†³æ–¹æ¡ˆ**ï¼šéœ€è¦ä¼˜åŒ–å…¨å±€è§‚æµ‹ç»„ç»‡æ–¹å¼ï¼Œå»é™¤é‡å¤ä¿¡æ¯

### ğŸ“‹ å¾…å®ç°ä»»åŠ¡

1. **ä¿®æ”¹å…¨å±€è§‚æµ‹è®¾è®¡**ï¼šä¼˜åŒ–å»é‡ï¼Œå‡å°‘ç½‘ç»œå‚æ•°é‡
2. **å®ç°MADDPGTrainer.__init__**ï¼šé›†æˆé…ç½®ç³»ç»Ÿï¼Œåˆ›å»ºç¯å¢ƒå’Œç®—æ³•
3. **å®ç°trainä¸»è®­ç»ƒå¾ªç¯**ï¼šä¸‰å±‚è¿›åº¦ç›‘æ§ï¼ŒEpisodeå±‚åšå¼ˆæ±‚è§£
4. **å®ç°_run_episodeæ–¹æ³•**ï¼šStepå±‚ç­–ç•¥è°ƒæ•´å¾ªç¯
5. **å®ç°get_nash_equilibriumæ–¹æ³•**ï¼šè·å–å½“å‰çº³ä»€å‡è¡¡è§£