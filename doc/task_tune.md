# MADDPG 收敛性调优方案 (Sioux Falls)

**日期**: 2026-01-03 ~ 2026-01-05
**状态**: 方案已确认，待实施
**背景**: 在 Sioux Falls 数据集上运行 MADDPG 训练时，观察到价格相对变化率 (`relative_change_rate`) 在 0.01 附近震荡，无法连续多个 step 低于阈值实现稳定收敛。

---

## 一、 问题诊断：如何判断"训练"质量？

由于本项目本质是**使用 RL 作为数值优化求解器**来寻找纳什均衡，因此评估标准不同于传统强化学习任务。

1.  **动作稳定性 (Action Stability) - [核心指标]**
    *   **目标**: 价格相对变化率应呈下降趋势，并最终**稳定维持**在阈值（如 0.01）以下。
    *   **现状**: 在 0.01 附近震荡。这通常意味着 Critic 无法分辨微小价格调整带来的收益差异，或被环境噪声淹没。

2.  **Critic Loss (Q值误差) - [诊断指标]**
    *   **目标**: 震荡下降，最终收敛到较小的稳定值。Critic 是求解器的"眼睛"，若 Loss 不收敛，说明其无法正确预估收益，Actor 只能盲目更新。

3.  **收益轨迹 (Reward Trajectory)**
    *   **目标**: 进入**"平台期" (Plateau)**。各智能体收益在剧烈变化后应同时进入平稳状态。
    *   **注意**: 若价格稳定但收益剧烈跳动，说明 UE-DTA 仿真存在随机性或未收敛（噪声过大）。

4.  **环境健康度 (UE-DTA Convergence)**
    *   **目标**: 每一轮 Step 中，UE-DTA 均能在限制次数内成功收敛。
    *   **风险**: 若 UE-DTA 经常不收敛，输入给 MADDPG 的 Demand/Flow 就是错误的，导致无法学习。

---

## 二、 Gemini 原始分析

### 2.1 核心原因分析

1.  **非平稳的奖励信号 (Non-stationary Reward Signal)**
    *   **问题**: 当前代码使用 `reward / max(current_round_rewards)` 进行动态归一化。分母（当轮最大值）是不断变化的，导致 Critic 面对的是一个移动的目标。即使表现变差，若分母也变小，归一化奖励可能不变，误导 Critic。

2.  **观测空间数值量级差异 (Input Scaling)**
    *   **问题**: State 包含 `Price` ([0, 1]) 和 `Flow` ([0, 100+])。数值上的巨大差异会导致神经网络对 `Flow` 特征过敏，而忽略 `Price` 的细微变化，导致梯度更新不稳定。

3.  **探索噪声衰减过快 (Premature Exploration Decay)**
    *   **问题**: `noise_decay = 0.99` 导致后期噪声几乎为 0 ($0.99^{1500} \approx 0$)。智能体失去微调探索能力，一旦陷入局部震荡（如边界震荡）无法跳出。

### 2.2 Gemini 建议的优化方案

1. **修正奖励归一化策略 (High Priority)** - 改为静态常数归一化
2. **观测空间标准化 (High Priority)** - 对流量除以常数 100.0 或 200.0
3. **调整探索噪声参数 (Medium Priority)** - `noise_decay: 0.99 → 0.9995`, `min_noise: 0.0005 → 0.01`
4. **增加 Critic Loss 监控 (Recommended)**

---

## 三、 Claude 代码审查与修正分析

### 3.1 代码实际情况核实

| Gemini 描述 | 实际代码 | 备注 |
|------------|---------|------|
| Price 范围 [0, 1] | **[0.5, 2.0]** | 见 `siouxfalls_settings.json` |
| 动态归一化 | ✅ 确认 | `maddpg.py:574-590` |
| noise_decay = 0.99 | ✅ 确认 | `config.py:40` |
| min_noise = 0.0005 | ✅ 确认 | `config.py:41` |

### 3.2 关键发现：MF-DDPG 已收敛

**MF-DDPG 在相同数据集上成功收敛**，这提供了重要的对照信息：

| 指标 | MF-DDPG 结果 |
|-----|-------------|
| Episode 0 | 385 steps 后收敛 |
| Episode 1 | 9 steps 后收敛 (快速复现) |
| Episode 2 | 27 steps 后收敛 |
| 总计 | 421 steps 完成 3 次收敛 |
| 最终 relative_change_rate | 0.00124 < 0.01 ✅ |

**收敛后的价格呈现 Bang-Bang 边界解**：
```
agent 5:  [2.0, 2.0, 2.0, 0.5, 2.0, 0.5, 2.0, 0.5]
agent 12: [2.0, 2.0, 0.5, 2.0, 0.5, 0.5, 0.5, 0.5]
agent 14: [2.0, 2.0, 0.5, 0.5, 2.0, 2.0, 2.0, 2.0]
agent 18: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]  (全高价)
```

**收益范围**：6,380 ~ 29,486（静态归一化常数建议用 **30000**）

### 3.3 MADDPG vs MF-DDPG 关键差异

| 特性 | MADDPG | MF-DDPG |
|-----|--------|---------|
| **奖励归一化** | 动态 (当轮最大值) | 动态 (当轮最大值) ✅ 相同 |
| **Critic 状态维度** | **96 维** | **32 维** |
| **经验回放** | 共享 Buffer | 独立 Buffer (每个 agent) |
| **Actor 输入** | 全部价格 + 自己流量 | 自己价格 + 自己流量 + 平均场价格 |

**关键结论**：MF-DDPG 使用**相同的动态归一化**也收敛了！

这说明 **Gemini 对动态归一化的诊断可能不是根本原因**。

### 3.4 修正后的原因假设

**真正可能导致 MADDPG 不收敛的原因**：

1. **Critic 网络输入维度过大** (96 vs 32，3倍差距)
   - MADDPG Critic 需要学习更复杂的输入空间
   - 在边界解（Bang-Bang）场景下更难学习

2. **流量未归一化在 MADDPG 中影响更大**
   - MADDPG 的流量维度：4×8 = 32 维未归一化
   - MF-DDPG 的流量维度：1×8 = 8 维未归一化
   - 尺度问题被放大 4 倍

3. **边界解处的梯度消失**
   - 纳什均衡是边界解（价格 0.5 或 2.0）
   - Actor 输出层 Sigmoid 在边界处梯度几乎为 0
   - 大网络更容易在梯度消失时停滞

### 3.5 修正后的优先级

| 优先级 | 修改项 | 原因 |
|-------|-------|------|
| **P0** | 流量观测归一化 | MADDPG 受影响更大（32维未归一化 vs MF-DDPG 8维） |
| **P1** | min_noise 提高到 0.01 | 保持探索能力，帮助跳出边界震荡 |
| **P1** | 记录 Critic Loss | 诊断必需 |
| **P2** | 奖励静态归一化 | MF-DDPG 证明动态归一化也能收敛，优先级降低 |
| **P2** | noise_decay 改为 0.9995 | 可选 |

---

## 四、 实验数据分析 (2026-01-05)

### 4.1 实验配置

| 算法 | Episodes | Steps | 数据文件 |
|-----|----------|-------|---------|
| MADDPG | 1 | 3000 | `results/step_records_maddpg.json` |
| MF-DDPG | 3 | 421 | `results/step_records_mfddpg.json` |

### 4.2 收敛指标对比

| 指标 | MADDPG | MF-DDPG | 差异 |
|-----|--------|---------|------|
| 低于阈值(0.01)的比例 | **0.6%** (17/3000) | 10.5% (44/421) | MF-DDPG 17倍 |
| 连续低于阈值最长 | **2 steps** | 5 steps | MADDPG 无法稳定 |
| 平均 relative_change_rate | 0.072 | 0.075 | 相近 |
| 最小 relative_change_rate | 0.00266 | 0.00114 | MF-DDPG 更低 |

**结论**：MADDPG 虽然有时能低于阈值，但**无法连续保持**（最长仅 2 步），而 MF-DDPG 可以连续 5 步触发收敛。

### 4.3 价格边界趋势对比 (关键发现)

| 指标 | MADDPG | MF-DDPG |
|-----|--------|---------|
| 前 50 步边界价格比例 | 57.9% | 73.0% |
| 后 50 步边界价格比例 | **56.9%** | **95.9%** |
| 趋势 | **无变化** | **明显向边界移动** |

**MADDPG 最后 step 价格**（卡在中间值）：
```
agent 5:  [1.13, 1.78, 1.63, 1.89, 1.98, 1.03, 0.5, 1.98]
agent 12: [1.52, 1.30, 1.20, 1.65, 0.97, 1.23, 1.03, 1.22]
agent 14: [2.0, 2.0, 0.5, 1.96, 0.5, 0.51, 0.53, 0.5]
agent 18: [0.5, 1.64, 0.51, 1.99, 2.0, 1.98, 0.54, 1.54]
```

**MF-DDPG 最后 step 价格**（全部边界解）：
```
agent 5:  [2.0, 2.0, 2.0, 0.5, 2.0, 0.5, 2.0, 0.5]
agent 12: [2.0, 2.0, 0.5, 2.0, 0.5, 0.5, 0.5, 0.5]
agent 14: [2.0, 2.0, 0.5, 0.5, 2.0, 2.0, 2.0, 2.0]
agent 18: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
```

### 4.4 相同 Step 数时的 relative_change_rate 对比

| Step | MADDPG | MF-DDPG | 倍数 |
|------|--------|---------|------|
| 50 | 0.125 | 0.131 | 1.0x |
| 100 | 0.195 | 0.056 | **3.5x** |
| 200 | 0.093 | 0.053 | 1.8x |
| 300 | 0.069 | 0.035 | 2.0x |
| 420 | 0.065 | 0.001 | **52x** |

**结论**：初期（step 50）两者相近，但 MF-DDPG 快速下降，MADDPG 在 0.065 附近停滞。

### 4.5 诊断结论

回答 3.4 节提出的问题：

1. **MADDPG 的价格变化是"震荡"还是"停滞"？**
   - **答案：停滞**。价格边界比例 3000 步几乎不变（57.9%→56.9%）
   - 说明 Actor 网络没有学到正确的策略方向

2. **价格是否有向边界移动的趋势？**
   - **答案：无**。MADDPG 价格卡在中间值（1.2~1.8），没有找到边界解
   - MF-DDPG 则明显向边界移动（73%→96%）

3. **根本原因确认**：
   - **流量未归一化** 是主要原因：MADDPG 有 32 维未归一化流量（0~100+），MF-DDPG 仅 8 维
   - **探索噪声耗尽**：`0.99^3000 ≈ 0`，MADDPG 后期完全没有探索能力
   - Critic 网络在高维输入下无法正确评估价格变化的影响

---

## 五、 修改方案与原理分析

基于第四节的实验数据分析，确认修改优先级和原理。

### 5.1 P0: 流量观测归一化 (最高优先级)

**修改位置**: `src/algorithms/maddpg/maddpg.py` -> `process_observations()`

```python
def process_observations(observation):
    last_prices = observation["last_round_all_prices"].flatten()
    own_flow = observation["own_charging_flow"].flatten()
    # 添加流量归一化
    own_flow_normalized = own_flow / 100.0
    return np.concatenate([last_prices, own_flow_normalized])
```

**为什么可能奏效**：

1. **数值量级平衡**：
   - 当前：价格 [0.5, 2.0]，流量 [0, 100+]
   - 修改后：价格 [0.5, 2.0]，流量 [0, 1.0+]
   - 神经网络对各特征的敏感度更均衡

2. **MADDPG 受影响更大**：
   - MADDPG Critic 输入：32 维价格 + **32 维流量** = 64 维（流量占 50%）
   - MF-DDPG Critic 输入：8 维价格 + 8 维平均场价格 + **8 维流量** = 24 维（流量占 33%）
   - 未归一化流量在 MADDPG 中的"噪声权重"更大

3. **梯度传播改善**：
   - 流量数值过大会导致某些神经元饱和
   - 归一化后梯度可以更均匀地传播到价格相关的权重

### 5.2 P1: 探索噪声参数调整

**修改位置**: `src/utils/config.py` -> `MADDPGConfig`

```python
@dataclass
class MADDPGConfig:
    # ...
    noise_decay: float = 0.9995  # 原 0.99
    min_noise: float = 0.01      # 原 0.0005
```

**为什么可能奏效**：

1. **当前问题**：
   - `0.99^500 ≈ 0.0066`，500 步后噪声几乎为零
   - `0.99^3000 ≈ 10^-13`，完全丧失探索能力
   - MADDPG 在第 100 步后就开始停滞（见 4.4 节数据）

2. **修改效果**：
   - `0.9995^3000 ≈ 0.22`，3000 步后仍有 22% 噪声
   - `min_noise = 0.01` 保证最低 1% 的探索能力
   - 足以帮助跳出局部停滞

3. **与边界解的关系**：
   - 纳什均衡是边界解（0.5 或 2.0）
   - Actor 输出层 Sigmoid 在边界处梯度接近 0
   - 需要噪声"推动"价格越过边界

### 5.3 P1: Critic Loss 监控 (诊断用)

**修改位置**: `src/algorithms/maddpg/maddpg.py` -> `learn()` 方法

```python
def learn(self):
    # ... 现有代码 ...

    # 收集 critic_loss 用于监控
    critic_losses = {}
    for agent_id in self.agent_ids:
        agent = self.agents[agent_id]
        critic_loss = self._update_critic(...)
        critic_losses[agent_id] = critic_loss

    # ... 现有代码 ...

    return True, {"critic_losses": critic_losses}
```

**为什么需要**：

1. **验证修改效果**：
   - 如果流量归一化有效，Critic Loss 应该下降更快
   - 如果 Critic Loss 不收敛，说明还有其他问题

2. **区分问题来源**：
   - Critic Loss 高 + 价格停滞 → Critic 学习问题
   - Critic Loss 低 + 价格停滞 → Actor 梯度问题

### 5.4 P2: 奖励静态归一化 (备选)

**修改位置**: `src/algorithms/maddpg/maddpg.py` -> `normalize_rewards()`

```python
def normalize_rewards(rewards):
    STATIC_CONSTANT = 50000.0  # 基于实验数据：最大收益 51031
    return {agent: reward / STATIC_CONSTANT for agent, reward in rewards.items()}
```

**为什么优先级降低**：

- MF-DDPG 使用**相同的动态归一化**也收敛了
- 说明动态归一化不是根本原因
- 但如果 P0+P1 修改无效，可以尝试此方案

**如果需要使用的原因**：
- 动态归一化可能在 MADDPG 的高维 Critic 中引入额外噪声
- 静态常数使 Critic 的目标更稳定

---

## 六、 边界解 (Bang-Bang) 合理性分析

**日期**: 2026-01-04

### 6.1 问题：为什么收敛到极端价格？

MF-DDPG 收敛后的价格全部落在边界 (0.5 或 2.0)，尤其是 agent 18 全时段高价 [2.0×8]。这是否合理？

### 6.2 成本结构分析

基于 `siouxfalls_settings.json` 的参数计算：

| 成本项 | 计算公式 | 数值 |
|-------|---------|------|
| 充电价格成本 | price × charging_demand (50) | **25~100 元** |
| 充电时间成本 | 0.005 × (3000m / 10m/s) | 1.5 元 |
| 绕路时间成本 | 假设绕路 500 秒 | ~2.5 元 |

**关键发现**：价格差异 (75元) >> 时间差异 (~5元)

**结论**：用户对价格**极度敏感**，愿意绕路很久去便宜的充电站。这使得价格战成为理性策略。

### 6.3 为什么 agent 18 可以全高价？

充电站地理位置分析（基于 `siouxfalls_nodes.csv`）：

- Node 5: x=-96.73 (北部)
- Node 12: x=-96.78 (西部)
- Node 14: x=-96.75 (中部)
- Node 18: x=-96.69 (**最东部**)

**原因**：Node 18 在网络东侧形成**局部垄断**。东侧用户去其他充电站的绕路成本太高，即使价格贵 75 元也没有替代选择。

### 6.4 充电站选址方式的影响

| 数据集 | 选址方式 | 充电站数 | 均衡特征 |
|-------|---------|---------|---------|
| **Sioux Falls** | 看地图选的（地理位置） | 4 | 局部垄断，易出现极端边界解 |
| Berlin | 基于节点流量 Top20 | 20 | 竞争更充分，流量覆盖 +93.5% |
| Anaheim | 基于节点流量 Top20 | 40 | 竞争更充分 |

**对比**：Berlin/Anaheim 采用智能选址（见 `doc/task_network_simplification.md`），确保每个充电站都有稳定客源，博弈更均衡。

### 6.5 概念澄清："模型"指什么？

| 层面 | 内容 | 是否影响边界解 |
|-----|------|--------------|
| **博弈环境设定** | 充电站位置、价格范围、time_value_coefficient | **是！直接决定均衡形态** |
| 学习算法 | MADDPG/MF-DDPG 的神经网络结构 | 否，只影响收敛速度 |

**重要结论**：边界解是**博弈环境的性质**，不是学习算法的问题。算法只是"发现"了这个均衡，而不是"创造"了它。

### 6.6 改进建议（如需非边界解）

如果希望 Sioux Falls 的均衡更"有张力"（非边界解），可尝试：

1. **重新选择充电站位置**：按流量选址，参考 Berlin 的方案
2. **增加充电站数量**：从 4 个增加到 6-8 个，增强竞争
3. **调整 time_value_coefficient**：从 0.005 提高到 0.01~0.015，让时间成本更有分量
4. **扩大价格范围**：从 [0.5, 2.0] 扩大到 [0.2, 3.0]，验证是否仍冲向边界

**注意**：如果目的是验证算法能找到纳什均衡，当前边界解本身是**正确的结果**。

---

## 七、 修改日志

| 日期 | 内容 |
|-----|------|
| 2026-01-03 | Gemini 初始分析 |
| 2026-01-03 | Claude 代码审查，发现 MF-DDPG 收敛，修正优先级 |
| 2026-01-03 | 启动 MADDPG 基线实验 (1 episode, 3000 steps) |
| 2026-01-04 | 边界解合理性分析：成本结构、充电站选址影响、概念澄清 |
| 2026-01-05 | **实验数据分析完成**：确认 MADDPG 价格"停滞"而非"震荡"，边界趋势无变化 |
| 2026-01-05 | **修改方案确认**：P0 流量归一化 + P1 噪声参数调整，附详细原理分析 |
