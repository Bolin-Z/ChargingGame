# 博弈问题定义与求解方法讨论记录

**日期**: 2026-01-14 ~ 2026-01-20
**参与者**: User, Antigravity, Claude (Opus)

---

## 一、问题定义

用户的场景描述如下：
> N 个 Player 在连续价格区间上选择价格，通过仿真环境（DUE）计算收益，目标是最大化自身收益。

**学术定义**:
这是一个 **N人非合作静态博弈 (N-player Non-cooperative Static Game)**。
具体归类为 **空间伯特兰德寡头竞争 (Spatial Bertrand Oligopoly)**：
*   **非合作**: 追求个人利益最大化。
*   **静态**: 同时决策（Simultaneous move）。
*   **连续动作空间**: 价格是连续变量。
*   **黑盒/随机收益**: 收益由复杂的交通仿真（DUE）计算，通常不可导且含有噪声。

---

## 二、求解方法探讨

### 1. 迭代最佳响应 (IBR)
*   **方法**: 轮流固定 $N-1$ 个人的策略，优化剩下 1 个人的策略。
*   **优缺点**: 实现简单，但可能无法收敛（进入极限环）。

### 2. 多智能体强化学习 (MARL) - 当前采用
*   **算法**: MADDPG, IDDPG, MFDDPG。
*   **原理**: Actor-Critic 结构，Critic 拟合黑盒收益面，Actor 进行策略梯度下降。
*   **优势**: 能够处理连续空间和随机性，Exploration 机制有助于跳出局部最优。

### 3. 演化计算 (EC)
*   **CMA-ES**: 适合连续空间、中小规模参数的黑盒优化。
*   **SAEA**: 利用代理模型减少仿真次数，适合仿真昂贵的场景。
*   **协同演化**: 将 N 个 Player 视为不同种群进行对抗性演化。

---

## 三、纳什均衡验证方法

### 单边偏离测试 (Unilateral Deviation)
1.  **固定对手**: 保持 $\mathbf{p}_{-i}$ 不变。
2.  **寻找最佳响应**: 搜索 Player $i$ 的最优策略 $p_i^*$。
3.  **计算后悔值**: $\text{Regret}_i = U_i(p_i^*) - U_i(p_i)$。
4.  **判定**: 若 $\sum \text{Regret}_i \approx 0$，则是均衡。

### 训练过程监控
*   **策略稳定性**: Action 变化率是否趋近于 0。
*   **Exploitability Monitor**: 定期计算 Nash Gap，只有 Nash Gap 收敛才算训练完成。

---

## 四、已确认问题（2026-01-19）

### 4.1 初期现状
*   MADDPG 收敛于中间值，MFDDPG 收敛于边界值。
*   **两个解都通不过单边偏离验证**：Nash Gap 很大，Player 可通过偏离大幅获利。

### 4.2 代码验证结论

| 问题 | 验证结果 |
|------|----------|
| **Actor 梯度断开** | ✅ MADDPG/IDDPG 的 `.detach().cpu().numpy()` 导致梯度为 0；MFDDPG 正常 |
| **探索衰减过快** | ✅ 约 30 episode 后噪音就到最小值，策略过早固化 |
| **奖励动态归一化** | ✅ 使用当步 max_reward，导致 Critic 学习不稳定 |
| **价格归一化** | ✅ 正确归一化到 [0,1] |
| **flow_scale_factor** | ✅ 作用位置正确，用于观测缩放 |

### 4.3 单边偏离测试结果（修复前）

**MADDPG 中间值解（价格 ~1.0-1.4）**：

| 智能体 | 基准收益 | 最优偏离收益 | 增益 |
|--------|----------|--------------|------|
| 5 | 12,184 | 25,421 | **+109%** |
| 12 | 7,105 | 19,979 | **+181%** |
| 14 | 9,733 | 29,089 | **+199%** |
| 18 | 30,317 | 44,236 | **+46%** |

**结论**：所有智能体最优偏离都指向中间价格（~1.2），"假收敛"现象确认。

---

## 五、已完成修复

### 5.1 修复清单

| 优先级 | 任务 | 状态 |
|--------|------|------|
| **P0** | 修复 MADDPG/IDDPG 的 Actor 梯度链路 | ✅ 完成 |
| **P1** | 添加学习诊断指标 | ✅ 完成 |
| **P2** | 固定 reward_scale | ✅ 完成 |

### 5.2 梯度链路修复（2026-01-19）

**修复方法**：
- MADDPG：新增 `_build_global_state_for_actor_update()` 方法，保持当前 agent 动作的梯度
- IDDPG：新增 `_build_local_state_for_actor_update()` 方法
- MFDDPG：无需修复

**修复前后对比**：

| 指标 | 修复前 | 修复后 |
|------|--------|--------|
| Actor 梯度范数 | `0.00e+00`（全为0） | `3.64e-02 ~ 5.44e-02` |
| Critic 梯度范数 | `0.27 ~ 0.56` | `0.31 ~ 0.49` |

### 5.3 学习诊断指标（2026-01-19）

添加的诊断指标：`critic_loss`, `actor_loss`, `actor_grad_norm`, `critic_grad_norm`, `q_value_mean/std/max/min`, `noise_sigma`

三个算法的 `learn()` 方法返回统一格式的诊断字典。

### 5.4 固定 reward_scale（2026-01-20）

**问题**：动态缩放导致相同收益在不同步被缩放成不同值，Critic 学习不稳定。

**修复**：基于历史数据（P99=41,771），设定 `reward_scale = 50,000`。

---

## 六、实验结果

### 6.1 修复后单边偏离测试（2026-01-20）

测试对象：修复后 MADDPG 学到的边界震荡解

| Agent | 基准收益 | 最优偏离收益 | 增益 |
|-------|---------|-------------|------|
| 5 | 16,842 | 24,052 | **+42.8%** |
| 12 | 9,574 | 10,885 | **+13.7%** |
| 14 | 18,935 | 33,750 | **+78.2%** |
| 18 | 42,826 | 45,800 | **+6.9%** |

**修复前后 Nash Gap 对比**：

| Agent | 修复前增益 | 修复后增益 | 变化 |
|-------|-----------|-----------|------|
| 5 | +220% | +42.8% | ✅ 改善 |
| 12 | +150% | +13.7% | ✅ 改善 |
| 14 | +74% | +78.2% | ❌ 无改善 |
| 18 | +26% | +6.9% | ✅ 改善 |

### 6.2 固定 reward_scale 实验结果（2026-01-20）

| 算法 | 总步数 | Episodes | 最终价格模式 | 总收益 |
|------|--------|----------|-------------|--------|
| MADDPG | 270 | 3 | 边界震荡（0.5/2.0交替） | 34,646 |
| IDDPG | 232 | 3 | 多数高价（~2.0） | 97,575 |
| MFDDPG | 206 | 3 | 多数高价（~2.0） | 42,003 |

**观察**：
1. 所有算法都倾向高价边界
2. 部分 Agent 出现边界震荡（Agent 5/12 在 0.5/2.0 交替）
3. 快速"收敛"：3 episode 内就达到收敛判据

### 6.3 当前结论

**梯度修复有效但不充分**：
- ✅ Actor 能正常学习（梯度非零）
- ✅ Nash Gap 整体有所下降（3/4 agent 改善）
- ❌ 仍未找到纳什均衡（所有 agent 都能偏离获利）
- ❌ 学到极端边界解而非经济学上合理的中间价格

---

## 七、待办事项

### 7.1 高优先级

| 任务 | 原因 |
|------|------|
| **调整探索策略** | 当前噪音衰减太快（按 step 衰减），策略过早固化在边界。建议：按 episode 衰减、更慢衰减率、更高 min_sigma |
| **引入 Exploitability Monitor** | 用 Nash Gap 替代价格变化率作为收敛指标 |
| **验证 learn_metrics 保存** | 上次实验中 learn_metrics 未正确保存，需确认修复 |

### 7.2 中优先级

| 任务 | 原因 |
|------|------|
| 尝试更长训练 | 当前仅 1 episode 就"收敛"，可能收敛判据太松 |
| 网格搜索验证 | 扫描价格空间找真正的收益最优区域 |
| flow_scale_factor 对照实验 | 按 P95 定标或使用 log1p 缩放 |

### 7.3 低优先级/备选方案

| 任务 | 说明 |
|------|------|
| 尝试 CMA-ES | 替代/辅助 MARL，适合仿真昂贵场景 |
| 调整 time_value_coefficient | 改变博弈结构，看是否存在更稳定均衡 |
| 混合策略均衡 | 如果纯策略均衡不存在，可能需要混合策略 |
