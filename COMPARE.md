# COMPARE.md

æœ¬æ–‡æ¡£è®°å½•ä¸ŽçŽ°æœ‰MADDPGç®—æ³•è¿›è¡Œå¯¹æ¯”çš„åŸºå‡†ç®—æ³•ï¼ˆBaseline Algorithmsï¼‰çš„è®¾è®¡ä¸Žå®žçŽ°æ–¹æ¡ˆã€‚

## æ–‡æ¡£ç›®æ ‡

- è®°å½•å¯¹æ¯”ç®—æ³•çš„ç†è®ºèƒŒæ™¯å’ŒæŠ€æœ¯ç»†èŠ‚
- æ˜Žç¡®ç®—æ³•é€‚é…åˆ°å½“å‰é¡¹ç›®çš„è®¾è®¡æ–¹æ¡ˆ
- è·Ÿè¸ªå®žçŽ°è¿›åº¦å’Œé‡åˆ°çš„é—®é¢˜
- ç¡®ä¿å¯¹æ¯”å®žéªŒçš„å…¬å¹³æ€§å’Œç§‘å­¦æ€§

---

## å¯¹æ¯”ç®—æ³•åˆ—è¡¨

### 1. Best Response Dynamic + PSO (BR-PSO)

#### 1.1 ç®—æ³•æ¥æº
**è®ºæ–‡**: "Real-Time Charging Navigation of Electric Vehicles to Fast Charging Stations: A Hierarchical Game Approach" (IEEE TSG, 2017)

**æ ¸å¿ƒæ€æƒ³**: ä½¿ç”¨PSOç®—æ³•æ±‚è§£æ¯ä¸ªå……ç”µç«™çš„Best Responseç­–ç•¥ï¼Œé€šè¿‡è¿­ä»£æ›´æ–°ç›´åˆ°è¾¾åˆ°è¿‘ä¼¼çº³ä»€å‡è¡¡ã€‚

#### 1.2 çŽ¯å¢ƒæŽ¥å£é€‚é…

**âœ… å¯ä»¥ç›´æŽ¥ä½¿ç”¨çŽ°æœ‰çŽ¯å¢ƒ**ï¼š
- è¾“å…¥ï¼š`actions = {agent_id: np.array([prices])}`
- è¾“å‡ºï¼š`rewards = {agent_id: revenue}`
- çŽ¯å¢ƒå†…éƒ¨è‡ªåŠ¨å¤„ç†UE-DTAä»¿çœŸ

**PSOç²’å­è¯„ä¼°**ï¼š
```python
def evaluate_particle(prices, agent_id, fixed_other_prices):
    actions = fixed_other_prices.copy()
    actions[agent_id] = prices  # å½“å‰ç²’å­ä»£è¡¨çš„ä»·æ ¼ç­–ç•¥
    _, rewards, _, _, _ = env.step(actions)
    return rewards[agent_id]
```

#### 1.3 çŽ¯å¢ƒçŠ¶æ€ç®¡ç†æ–¹æ¡ˆ

**æ ¸å¿ƒå†³ç­–ï¼šBest Responseè¿­ä»£å±‚é¢resetï¼Œç²’å­è¯„ä¼°å±‚é¢soft_reset**

**æœºåˆ¶è®¾è®¡**ï¼š
```python
def find_best_response(agent_id):
    # ðŸ”‘ æ¯è½®Best Responseå¼€å§‹æ—¶ï¼šå®Œå…¨reset
    env.reset()  # åˆå§‹åŒ–è·¯å¾„åˆ†é…ï¼ˆè´ªå¿ƒç­–ç•¥ï¼‰

    def objective_function(prices):
        # ðŸ”‘ PSOç²’å­è¯„ä¼°æ—¶ï¼šsoft_reset
        env.soft_reset()  # ä¿ç•™è·¯å¾„åˆ†é…ï¼Œæ¸…é™¤çŠ¶æ€å˜é‡

        actions = fixed_strategies.copy()
        actions[agent_id] = prices
        _, rewards, _, _, _ = env.step(actions)
        return -rewards[agent_id]

    # PSOä¼˜åŒ–ï¼ˆå¤šä¸ªç²’å­Ã—å¤šæ¬¡è¿­ä»£ï¼‰
    best_prices = pso.optimize(objective_function)
    return best_prices
```

**`soft_reset()` æ–¹æ³•è®¾è®¡**ï¼š
- æ¸…é™¤çŠ¶æ€å˜é‡ï¼š`current_step`, `price_history`, `convergence_counter`
- **ä¿ç•™è·¯å¾„åˆ†é…**ï¼š`current_routes_specified`ï¼ˆå…³é”®ï¼ï¼‰
- ç›®çš„ï¼šåŠ é€ŸUE-DTAæ”¶æ•›ï¼ˆä»ŽæŽ¥è¿‘å‡è¡¡çš„è·¯å¾„å¼€å§‹ï¼‰

**ç†ç”±åˆ†æž**ï¼š

1. **ä¸ºä»€ä¹ˆBRè¿­ä»£å±‚é¢éœ€è¦resetï¼Ÿ**
   - æ¯ä¸ªå……ç”µç«™çš„Best Responseæ±‚è§£åº”è¯¥ä»Žç›¸åŒåˆå§‹æ¡ä»¶å¼€å§‹
   - ç¡®ä¿ä¸åŒå……ç”µç«™ä¹‹é—´çš„å…¬å¹³æ€§

2. **ä¸ºä»€ä¹ˆç²’å­è¯„ä¼°å±‚é¢å¯ä»¥soft_resetï¼Ÿ**
   - UE-DTAçš„éšæœºæ€§æ˜¯**å›ºæœ‰çš„**ï¼ˆè·¯å¾„åˆ‡æ¢æ¦‚çŽ‡æœºåˆ¶ï¼‰
   - ä¸åŒä»·æ ¼æœ¬æ¥å°±åº”è¯¥å¯¼è‡´ä¸åŒçš„å‡è¡¡çŠ¶æ€
   - PSOèƒ½å¤Ÿå®¹å¿è¯„ä¼°å™ªéŸ³ï¼ˆç¾¤ä½“æ™ºèƒ½çš„é²æ£’æ€§ï¼‰
   - å¤§å¹…åŠ é€Ÿè®¡ç®—ï¼ˆ3-5å€ï¼‰ï¼šUE-DTAä»ŽæŽ¥è¿‘å‡è¡¡çš„è·¯å¾„å¼€å§‹

3. **UE-DTAéšæœºæ€§çš„åˆç†æ€§**ï¼š
   ```python
   # è·¯å¾„åˆ‡æ¢æœºåˆ¶ï¼ˆä»£ç ä¸­çš„è®¾è®¡ï¼‰
   if cost_gap > 0 and np.random.random() < ue_swap_probability:
       new_routes = best_route
   ```
   - æ¨¡æ‹ŸçœŸå®žç”¨æˆ·é€‰æ‹©çš„éšæœºæ€§
   - é¿å…ç³»ç»Ÿéœ‡è¡ï¼ŒæŽ¢ç´¢è·¯å¾„ç©ºé—´
   - ç›¸åŒä»·æ ¼è¾“å…¥å¯èƒ½äº§ç”Ÿç•¥å¾®ä¸åŒçš„å‡è¡¡ï¼ˆå¯æŽ¥å—ï¼‰

4. **PSOå¤„ç†å™ªéŸ³çš„èƒ½åŠ›**ï¼š
   - ç¾¤ä½“æŽ¢ç´¢ï¼š20ä¸ªç²’å­çš„é›†ä½“æ™ºèƒ½å¹³æ»‘å™ªéŸ³
   - è®°å¿†æœºåˆ¶ï¼špBestå’ŒgBestæä¾›ç¨³å®šæ–¹å‘
   - å¤šæ¬¡è¯„ä¼°ï¼šç›¸ä¼¼åŒºåŸŸä¼šè¢«å¤šæ¬¡æŽ¢ç´¢

**é¢„æœŸæ•ˆæžœ**ï¼š
- è®¡ç®—åŠ é€Ÿï¼šæ¯æ¬¡UE-DTAä»Ž10æ¬¡è¿­ä»£é™åˆ°3-5æ¬¡è¿­ä»£
- ç»“æžœè´¨é‡ï¼šPSOèƒ½åœ¨å™ªéŸ³çŽ¯å¢ƒä¸‹æ”¶æ•›åˆ°åˆç†çš„è¿‘ä¼¼çº³ä»€å‡è¡¡
- å¯æŽ¥å—ä»£ä»·ï¼šè¯„ä¼°å­˜åœ¨Â±5-10%çš„å™ªéŸ³ï¼ˆéœ€å®žéªŒéªŒè¯ï¼‰

#### 1.4 Best Responseè¿­ä»£æž¶æž„

**æ ¸å¿ƒå†³ç­–ï¼šå¹¶è¡Œæ›´æ–°ï¼ˆJacobié£Žæ ¼ï¼‰ï¼Œä¸ŽMADDPGä¿æŒä¸€è‡´**

**åŽŸè®ºæ–‡å®žçŽ°æ–¹å¼**ï¼š
- å¤–å±‚BRè¿­ä»£ï¼š~100è½®ï¼ˆä»ŽFigure 11å¯è§ï¼‰
- å†…å±‚æ›´æ–°æ–¹å¼ï¼šä¸²è¡Œæ›´æ–°ï¼ˆGauss-Seidelé£Žæ ¼ï¼‰
- PSOè¿­ä»£æ¬¡æ•°ï¼šæœªæ˜Žç¡®è¯´æ˜Ž

**æˆ‘ä»¬çš„å®žçŽ°æ–¹æ¡ˆï¼šå¹¶è¡Œæ›´æ–° + è®¡ç®—å¹¶è¡ŒåŒ–**

```python
class ParallelBRPSO:
    def __init__(self, network_dir, network_name, ...):
        # ðŸ”‘ æ¯ä¸ªagentç»´æŠ¤ä¸€ä¸ªç‹¬ç«‹çš„å®Œæ•´çŽ¯å¢ƒå¯¹è±¡
        self.agent_envs = {
            agent_id: EVCSChargingGameEnv(network_dir, network_name)
            for agent_id in agents
        }

for br_iteration in range(max_br_iterations):
    # ðŸ”‘ æ­¥éª¤1ï¼šä¿å­˜æœ¬è½®ç­–ç•¥å¿«ç…§ï¼ˆJacobiæ›´æ–°ï¼‰
    strategies_snapshot = {aid: current_strategies[aid].copy()
                          for aid in agents}

    # ðŸ”‘ æ­¥éª¤2ï¼šæ‰€æœ‰agentå¹¶è¡Œè®¡ç®—Best Responseï¼ˆè®¡ç®—å¹¶è¡Œï¼‰
    with ProcessPoolExecutor(max_workers=len(agents)) as executor:
        futures = {
            agent_id: executor.submit(
                solve_single_agent_best_response,
                agent_id,
                {aid: strategies_snapshot[aid] for aid in agents if aid != agent_id},
                agent_envs[agent_id],  # ä½¿ç”¨è¯¥agentçš„ç‹¬ç«‹çŽ¯å¢ƒ
                pso_config
            )
            for agent_id in agents
        }

        new_strategies = {aid: future.result() for aid, future in futures.items()}

    # ðŸ”‘ æ­¥éª¤3ï¼šåŒæ—¶æ›´æ–°æ‰€æœ‰agentç­–ç•¥
    current_strategies = new_strategies

    # æ­¥éª¤4ï¼šæ£€æŸ¥æ”¶æ•›
    if check_convergence(strategies_snapshot, new_strategies):
        break
```

**å¯¹æ¯”ï¼šä¸²è¡Œ vs å¹¶è¡Œæ›´æ–°**

| ç»´åº¦ | ä¸²è¡Œæ›´æ–°ï¼ˆGauss-Seidelï¼Œè®ºæ–‡æ–¹å¼ï¼‰ | å¹¶è¡Œæ›´æ–°ï¼ˆJacobiï¼Œæˆ‘ä»¬çš„æ–¹æ¡ˆï¼‰ |
|------|--------------------------------|----------------------------|
| **ä¿¡æ¯åŒæ­¥** | åŽé¢agentçœ‹åˆ°å‰é¢agentçš„æœ€æ–°ç­–ç•¥ | æ‰€æœ‰agentåŸºäºŽåŒä¸€è½®ç­–ç•¥ |
| **é¡ºåºä¾èµ–** | âŒ å­˜åœ¨ï¼ˆagent_1å’Œagent_4ä¿¡æ¯ä¸å¯¹ç§°ï¼‰ | âœ… æ— é¡ºåºåå·® |
| **åšå¼ˆå®šä¹‰** | âš ï¸ å¼•å…¥äº†åºè´¯ä¿¡æ¯ | âœ… ç¬¦åˆ"é™æ€åŒæ—¶åšå¼ˆ" |
| **ä¸ŽMADDPGä¸€è‡´æ€§** | âŒ ä¸ä¸€è‡´ | âœ… ä¸€è‡´ï¼ˆåŒæ­¥æ›´æ–°ï¼‰ |
| **è®¡ç®—æ—¶é—´** | æ— æ³•å¹¶è¡Œ | âœ… åŠ é€Ÿçº¦4å€ |
| **æ”¶æ•›é€Ÿåº¦** | ç†è®ºä¸Šå¯èƒ½ç¨å¿«1-2è½® | ç†è®ºä¸Šå¯èƒ½ç¨æ…¢1-2è½® |

**é€‰æ‹©å¹¶è¡Œæ›´æ–°çš„ç†ç”±**ï¼š
1. âœ… ç¬¦åˆé¡¹ç›®å®šä¹‰çš„"é™æ€åŒæ—¶åšå¼ˆ"
2. âœ… ä¸ŽMADDPGçš„åŒæ­¥æ›´æ–°é£Žæ ¼ä¸€è‡´ï¼Œä¾¿äºŽå…¬å¹³å¯¹æ¯”
3. âœ… æ— é¡ºåºåå·®ï¼Œæ‰€æœ‰agentä¿¡æ¯å¯¹ç§°
4. âœ… å®žçŽ°é€»è¾‘æ¸…æ™°ï¼Œæ˜“äºŽç†è§£å’Œè°ƒè¯•
5. âœ… **å¯ä»¥åˆ©ç”¨å¤šæ ¸CPUå¹¶è¡Œè®¡ç®—ï¼ŒåŠ é€Ÿçº¦4å€**

**å¹¶è¡ŒåŒ–å®žçŽ°çš„å…³é”®è¦ç‚¹**ï¼š

**1. æ¯ä¸ªagentç»´æŠ¤ç‹¬ç«‹çŽ¯å¢ƒå¯¹è±¡**
```python
# åˆå§‹åŒ–æ—¶åˆ›å»ºï¼Œæ•´ä¸ªBR-PSOè¿‡ç¨‹ä¸­æŒç»­ä½¿ç”¨
self.agent_envs = {
    agent_id: EVCSChargingGameEnv(network_dir, network_name)
    for agent_id in agents
}
```

**ä¼˜åŠ¿**ï¼š
- âœ… æ— çŽ¯å¢ƒæ‹·è´å¼€é”€ï¼ˆç›¸æ¯”åŠ¨æ€åˆ›å»ºå‰¯æœ¬ï¼‰
- âœ… çŠ¶æ€å¤©ç„¶éš”ç¦»ï¼ˆæ¯ä¸ªagentåœ¨è‡ªå·±çš„çŽ¯å¢ƒä¸­æ‰§è¡Œï¼‰
- âœ… å®žçŽ°ç®€å•ï¼ˆä¸éœ€è¦å¤æ‚çš„æ·±æ‹·è´é€»è¾‘ï¼‰
- âœ… è°ƒè¯•å‹å¥½ï¼ˆçŽ¯å¢ƒå¯¹è±¡ç”Ÿå‘½å‘¨æœŸæ¸…æ™°ï¼‰

**2. reset() vs soft_reset() çš„ä½¿ç”¨**

```python
def solve_single_agent_best_response(agent_id, other_strategies, env, pso_config):
    # æ¯è½®BRå¼€å§‹ï¼šå®Œå…¨reset
    env.reset()  # é‡æ–°åˆå§‹åŒ–è·¯å¾„åˆ†é…ï¼ˆè´ªå¿ƒç­–ç•¥ï¼‰

    def objective_function(prices):
        # æ¯ä¸ªç²’å­è¯„ä¼°ï¼šsoft_reset
        env.soft_reset()  # ä¿ç•™è·¯å¾„åˆ†é…ï¼Œæ¸…é™¤çŠ¶æ€å˜é‡

        actions = other_strategies.copy()
        actions[agent_id] = prices
        _, rewards, _, _, _ = env.step(actions)
        return -rewards[agent_id]

    # PSOä¼˜åŒ–
    best_prices = pso.optimize(objective_function)
    return best_prices
```

**`reset()` å®žçŽ°**ï¼š
```python
def reset(self):
    """å®Œå…¨é‡ç½®çŽ¯å¢ƒï¼ˆæ¯è½®BRå¼€å§‹æ—¶è°ƒç”¨ï¼‰"""
    self.current_step = 0
    self.price_history = []
    self.convergence_counter = 0
    self.current_routes_specified = self._initialize_routes_greedy()  # ðŸ”‘ é‡æ–°åˆå§‹åŒ–
    self._init_world()
    return self._get_observations()
```

**`soft_reset()` å®žçŽ°**ï¼š
```python
def soft_reset(self):
    """ä¿ç•™è·¯å¾„åˆ†é…ï¼Œé‡ç½®å…¶ä»–çŠ¶æ€ï¼ˆPSOç²’å­è¯„ä¼°æ—¶è°ƒç”¨ï¼‰"""
    self.current_step = 0
    self.price_history = []
    self.convergence_counter = 0
    # self.current_routes_specified ä¿æŒä¸å˜ ðŸ”‘ å…³é”®ï¼
    self._init_world()
```

**3. æ€§èƒ½æå‡åˆ†æž**

```python
# å‡è®¾å‚æ•°
n_br_iterations = 100
n_agents = 4
avg_pso_evals = 10_particles Ã— 20_iters = 200æ¬¡è¯„ä¼°/agent
t_ue_dta = 2s

# ä¸²è¡Œæ—¶é—´ï¼ˆè®ºæ–‡å¯èƒ½çš„æ–¹å¼ï¼‰
t_serial = 100 Ã— 4 Ã— 200 Ã— 2s = 160,000s â‰ˆ 44å°æ—¶

# å¹¶è¡Œæ—¶é—´ï¼ˆæˆ‘ä»¬çš„æ–¹æ¡ˆï¼‰
t_parallel = 100 Ã— 200 Ã— 2s = 40,000s â‰ˆ 11å°æ—¶

# ç†è®ºåŠ é€Ÿæ¯” = 4å€ï¼ˆ4ä¸ªagentï¼‰
# å®žé™…åŠ é€Ÿæ¯” â‰ˆ 3.5å€ï¼ˆè€ƒè™‘è¿›ç¨‹åˆ›å»ºå’Œåºåˆ—åŒ–å¼€é”€ï¼‰
```

**4. å®žçŽ°æ³¨æ„äº‹é¡¹**

- **çŽ¯å¢ƒåºåˆ—åŒ–**ï¼šEVCSChargingGameEnvéœ€è¦æ”¯æŒpickleï¼ˆProcessPoolExecutorè¦æ±‚ï¼‰
- **å†…å­˜å ç”¨**ï¼š4ä¸ªçŽ¯å¢ƒå¯¹è±¡å¸¸é©»å†…å­˜ï¼ˆé¢„è®¡æ¯ä¸ª100-500MBï¼Œæ€»è®¡~2GBï¼Œå¯æŽ¥å—ï¼‰
- **è°ƒè¯•ç­–ç•¥**ï¼šä¿ç•™ä¸²è¡Œç‰ˆæœ¬ç”¨äºŽè°ƒè¯•ï¼Œå¹¶è¡Œç‰ˆæœ¬ç”¨äºŽæœ€ç»ˆå®žéªŒ

#### 1.5 æ”¶æ•›åˆ¤åˆ«æ¡ä»¶

**ä¸¤å±‚æ”¶æ•›åˆ¤åˆ«è®¾è®¡**ï¼šBRå±‚ï¼ˆå¤–å±‚ï¼‰+ PSOå±‚ï¼ˆå†…å±‚ï¼‰

##### 1.5.1 BRå±‚æ”¶æ•›åˆ¤åˆ«

**è®¾è®¡å†³ç­–**ï¼šä¸ŽMADDPGä¿æŒå®Œå…¨ä¸€è‡´

```python
def check_br_convergence(old_strategies, new_strategies):
    """BRæ”¶æ•›åˆ¤æ–­ï¼ˆä¸ŽMADDPGä¸€è‡´ï¼‰"""
    relative_changes = []

    for agent_id in agents:
        old_prices = np.array(old_strategies[agent_id])
        new_prices = np.array(new_strategies[agent_id])

        # è®¡ç®—ç›¸å¯¹å˜åŒ–çŽ‡
        relative_change = np.abs(new_prices - old_prices) / (old_prices + 1e-8)
        avg_change = np.mean(relative_change)
        relative_changes.append(avg_change)

    avg_relative_change = np.mean(relative_changes)
    return avg_relative_change < convergence_threshold
```

**é…ç½®å‚æ•°**ï¼š
```python
BR_CONVERGENCE_CONFIG = {
    'convergence_threshold': 0.01,      # 1%å¹³å‡ç›¸å¯¹å˜åŒ–ï¼ˆä¸ŽMADDPGä¸€è‡´ï¼‰
    'stable_required': 5,               # è¿žç»­5è½®æ”¶æ•›åˆ™åœæ­¢ï¼ˆä¸ŽMADDPGä¸€è‡´ï¼‰
    'max_br_iterations': 100,           # æœ€å¤§BRè¿­ä»£æ•°
}
```

**ç†ç”±**ï¼š
- âœ… ä¸ŽMADDPGæ”¶æ•›æ ‡å‡†å®Œå…¨ä¸€è‡´ï¼Œç¡®ä¿å¯¹æ¯”å…¬å¹³æ€§
- âœ… åŸºäºŽä»·æ ¼ç›¸å¯¹å˜åŒ–çŽ‡ï¼Œç‰©ç†æ„ä¹‰æ¸…æ™°
- âœ… é€šè¿‡stable_requiredé¿å…å¶ç„¶æ³¢åŠ¨

##### 1.5.2 PSOå±‚æ”¶æ•›åˆ¤åˆ«

**åŽŸè®ºæ–‡æƒ…å†µ**ï¼š
- âŒ PSOåœæ­¢æ¡ä»¶æœªæ˜Žç¡®è¯´æ˜Ž
- âœ… BRå¤–å±‚è¿­ä»£çº¦100è½®ï¼ˆä»ŽFigure 11å¯è§ï¼‰

**è®¾è®¡å†³ç­–**ï¼šåˆšæ€§æœ€å¤§è¿­ä»£æ¬¡æ•° + gBeståœæ»ž

```python
class PSOConfig:
    """PSOæ”¶æ•›é…ç½®"""
    max_iterations: int = 20        # æœ€å¤§è¿­ä»£æ•°ï¼ˆå¯é…ç½®å‚æ•°ï¼‰
    min_iterations: int = 5         # æœ€å°è¿­ä»£æ•°ï¼ˆé¿å…è¿‡æ—©åœæ­¢ï¼‰
    patience: int = 5               # gBestæ— æ”¹è¿›å®¹å¿æ¬¡æ•°
    improvement_threshold: float = 0.01  # ç›¸å¯¹æ”¹è¿›é˜ˆå€¼ï¼ˆ1%ï¼‰

def check_pso_convergence(pso_state):
    """PSOæ”¶æ•›åˆ¤æ–­"""
    # æ¡ä»¶0ï¼šä¿è¯æœ€å°æŽ¢ç´¢
    if pso_state.iteration < min_iterations:
        return False

    # æ¡ä»¶1ï¼šè¾¾åˆ°æœ€å¤§è¿­ä»£ï¼ˆç¡¬æ€§åœæ­¢ï¼‰
    if pso_state.iteration >= max_iterations:
        return True

    # æ¡ä»¶2ï¼šgBeståœæ»žï¼ˆæ—©åœï¼‰
    if len(pso_state.gbest_history) >= patience:
        recent_gbest = pso_state.gbest_history[-patience:]
        improvement = abs(recent_gbest[-1] - recent_gbest[0])
        relative_improvement = improvement / (abs(recent_gbest[0]) + 1e-8)

        if relative_improvement < improvement_threshold:
            return True  # è¿žç»­patienceæ¬¡è¿­ä»£gBestæ”¹è¿›<1%

    return False
```

**åˆ¤æ®é€‰æ‹©ç†ç”±**ï¼š
1. âœ… **gBeståœæ»žæ˜¯PSOæœ€å¸¸ç”¨çš„æ”¶æ•›åˆ¤æ®**ï¼ˆæ–‡çŒ®å…±è¯†ï¼‰
2. âœ… **ä¸ŽBRå±‚é€»è¾‘ä¸€è‡´**ï¼šéƒ½åŸºäºŽ"è¿žç»­Næ­¥æ— æ˜¾è‘—æ”¹è¿›"
3. âœ… **è®¡ç®—ç®€å•**ï¼šæ— é¢å¤–å¼€é”€ï¼Œåªéœ€è®°å½•gBeståŽ†å²
4. âœ… **å¯¹å™ªéŸ³æœ‰å®¹å¿**ï¼šé€šè¿‡patienceå’Œthresholdå‚æ•°è°ƒèŠ‚
5. âœ… **é¿å…è¿‡æ—©/è¿‡æ™šåœæ­¢**ï¼šmin/max_iterationsæä¾›ä¿éšœ

**å‚æ•°è®¾ç½®å»ºè®®**ï¼ˆæ ¹æ®é¢„ç®—è°ƒæ•´ï¼‰ï¼š

| PSOé¢„ç®— | max_iterations | min_iterations | patience | threshold | è¯´æ˜Ž |
|---------|----------------|----------------|----------|-----------|------|
| å°ï¼ˆå¿«é€ŸæŽ¢ç´¢ï¼‰ | 10 | 3 | 3 | 0.02 | é€‚åˆåˆæ­¥æµ‹è¯• |
| ä¸­ï¼ˆå¹³è¡¡ï¼‰ | 20 | 5 | 5 | 0.01 | **æŽ¨èèµ·ç‚¹** |
| å¤§ï¼ˆå……åˆ†æŽ¢ç´¢ï¼‰ | 50 | 10 | 8 | 0.01 | è®¡ç®—é¢„ç®—å……è¶³æ—¶ |

**é¢„æœŸUE-DTAè°ƒç”¨æ¬¡æ•°**ï¼ˆä¸­ç­‰é¢„ç®—ï¼‰ï¼š
```python
# å•æ¬¡BRçš„PSOè¯„ä¼°æ¬¡æ•°
# æœ€åæƒ…å†µï¼š10 particles Ã— 20 iterations = 200æ¬¡
# Early stopé¢„æœŸï¼š10 particles Ã— 12 iterations = 120æ¬¡ï¼ˆå¹³å‡ï¼‰

# å®Œæ•´BR-PSOçš„UE-DTAè°ƒç”¨
# æœ€åï¼š100 BR Ã— 4 agents Ã— 200 = 80,000æ¬¡
# å¹¶è¡ŒåŽï¼š100 BR Ã— 200 = 20,000æ¬¡ï¼ˆå¢™ä¸Šæ—¶é’Ÿæ—¶é—´ï¼‰
# Early stopé¢„æœŸï¼š100 BR Ã— 120 = 12,000æ¬¡
```

**å¤‡é€‰åˆ¤æ®**ï¼ˆæš‚ä¸é‡‡ç”¨ï¼‰ï¼š
- **ç²’å­ç¾¤å¤šæ ·æ€§**ï¼šè®¡ç®—å¼€é”€å¤§ï¼Œé˜ˆå€¼éš¾è®¾å®š
- **é€‚åº”åº¦æ–¹å·®**ï¼šå¯èƒ½æ‰€æœ‰ç²’å­é™·å…¥åŒä¸€å±€éƒ¨æœ€ä¼˜
- **å¹³æ»‘å¤„ç†**ï¼šå¢žåŠ å¤æ‚åº¦ï¼Œç­‰å‘çŽ°å™ªéŸ³é—®é¢˜å†è€ƒè™‘

#### 1.6 å®žéªŒéªŒè¯è®¡åˆ’

**å®žéªŒ1ï¼šå™ªéŸ³æ°´å¹³æµ‹è¯•**
- å›ºå®šä»·æ ¼ç­–ç•¥ï¼Œè¯„ä¼°20æ¬¡ï¼Œè®¡ç®—å˜å¼‚ç³»æ•°CV
- ç›®æ ‡ï¼šCV < 10%ï¼ˆå¯æŽ¥å—èŒƒå›´ï¼‰

**å®žéªŒ2ï¼šreset vs soft_resetå¯¹æ¯”**
- åˆ†åˆ«è¿è¡Œä¸¤ç§ç­–ç•¥çš„BR-PSO
- å¯¹æ¯”æœ€ç»ˆçº³ä»€å‡è¡¡çš„ä»·æ ¼å·®å¼‚
- éªŒè¯soft_resetçš„å¯è¡Œæ€§

**å®žéªŒ3ï¼šè®¡ç®—æ•ˆçŽ‡å¯¹æ¯”**
- è®°å½•UE-DTAå¹³å‡è¿­ä»£æ¬¡æ•°
- è®°å½•æ€»è®¡ç®—æ—¶é—´
- é‡åŒ–åŠ é€Ÿæ•ˆæžœ

**å®žéªŒ4ï¼šPSOé¢„ç®—æ•æ„Ÿæ€§åˆ†æžï¼ˆæ–°å¢žï¼‰**

**ç›®æ ‡**ï¼šåœ¨æ€»UE-DTAè°ƒç”¨é¢„ç®—çº¦æŸä¸‹ï¼Œæ‰¾åˆ°PSOçš„æœ€ä¼˜é…ç½®

**æµ‹è¯•é…ç½®**ï¼š

| é…ç½®å | ç²’å­æ•° | PSOæœ€å¤§è¿­ä»£ | å•æ¬¡BRé¢„æœŸè°ƒç”¨ | è¯´æ˜Ž |
|--------|--------|------------|---------------|------|
| Ultra-Light | 5 | 10 | ~60 | æžé€Ÿæµ‹è¯• |
| Light | 10 | 20 | ~120 | æŽ¨èé…ç½® |
| Medium | 15 | 30 | ~200 | å¹³è¡¡é…ç½® |
| Heavy | 20 | 50 | ~400 | å……åˆ†æŽ¢ç´¢ |

**è¯„ä¼°æŒ‡æ ‡**ï¼š
1. æœ€ç»ˆçº³ä»€å‡è¡¡çš„ä»·æ ¼ç­–ç•¥
2. å„å……ç”µç«™çš„æ”¶ç›Š
3. å®žé™…BRæ”¶æ•›è½®æ•°
4. å®žé™…UE-DTAè°ƒç”¨æ€»æ¬¡æ•°
5. æ€»è®¡ç®—æ—¶é—´

**åˆ¤æ–­æ ‡å‡†**ï¼š
- å¦‚æžœUltra-Lightä¸ŽMediumçš„æœ€ç»ˆæ”¶ç›Šå·®å¼‚<3%ï¼Œé€‰æ‹©Ultra-Light
- å¦‚æžœé…ç½®Açš„è®¡ç®—æ—¶é—´æ˜¯é…ç½®Bçš„2å€ï¼Œä½†æ”¶ç›Šæ”¹è¿›<5%ï¼Œé€‰æ‹©é…ç½®B

#### 1.7 æ”¶æ•›æ›²çº¿å¯è§†åŒ–æ–¹æ¡ˆ

**ç›®æ ‡**ï¼šå¯¹æ¯”MADDPGå’ŒBR-PSOæ±‚è§£çº³ä»€å‡è¡¡è¿‡ç¨‹çš„æ”¶æ•›æ€§èƒ½

##### 1.7.1 æ¨ªè½´é€‰æ‹©

**ç»Ÿä¸€åŸºå‡†ï¼šç´¯ç§¯çŽ¯å¢ƒè¯„ä¼°æ¬¡æ•°**

```python
# MADDPG: æ¯ä¸ªstepå¯¹åº”1æ¬¡çŽ¯å¢ƒè¯„ä¼°
cumulative_evals_maddpg = step_number

# BR-PSO: ç´¯åŠ æ‰€æœ‰ç²’å­è¯„ä¼°
cumulative_evals_brpso = sum(n_particles Ã— n_pso_iters for each BR iteration)
```

**åŽŸå› **ï¼š
- âœ… å…¬å¹³å¯¹æ¯”è®¡ç®—æˆæœ¬ï¼ˆæ¯æ¬¡env.step()æˆæœ¬ç›¸åŒï¼‰
- âœ… ç¡¬ä»¶æ— å…³ï¼ˆä¸å—CPUæ ¸å¿ƒæ•°å½±å“ï¼‰
- âœ… å®žçŽ°æ— å…³ï¼ˆä¸å—ç®—æ³•å†…éƒ¨ç»“æž„å½±å“ï¼‰

##### 1.7.2 çºµè½´é€‰æ‹©

**ä¸»è¦æŒ‡æ ‡ï¼šç­–ç•¥å˜åŒ–çŽ‡ï¼ˆå¯¹æ•°åæ ‡ï¼‰**

```python
# æ‰€æœ‰å……ç”µç«™æ‰€æœ‰æ—¶æ®µçš„ä»·æ ¼ç›¸å¯¹å˜åŒ–çŽ‡
relative_change = |price_new[i,t] - price_old[i,t]| / (price_old[i,t] + 1e-8)
strategy_change_rate = mean(relative_change for all i,t)
```

**ç‰©ç†æ„ä¹‰**ï¼š
- è®­ç»ƒåˆæœŸï¼šå˜åŒ–çŽ‡é«˜ï¼ˆ0.1-0.5ï¼‰ï¼Œç­–ç•¥è°ƒæ•´å‰§çƒˆ
- æ”¶æ•›æ—¶ï¼šå˜åŒ–çŽ‡è¶‹è¿‘0ï¼ˆ<0.01ï¼‰ï¼Œç­–ç•¥ç¨³å®š

**MADDPGè®°å½•æ–¹å¼**ï¼š
- æ¯ä¸ªstepéƒ½è®¡ç®—å¹¶è®°å½•ï¼ˆè¿žç»­å¯†é›†æ›²çº¿ï¼‰

**BR-PSOè®°å½•æ–¹å¼**ï¼š
- **ä»…åœ¨BRè¿­ä»£å®Œæˆæ—¶è®°å½•**ï¼ˆç¦»æ•£ç¨€ç–ç‚¹/é˜¶è·ƒæ›²çº¿ï¼‰
- å•ä¸ªBRè¿­ä»£å†…çš„PSOæœç´¢ä¸è®°å½•ï¼ˆå› ä¸ºç­–ç•¥æœªçœŸæ­£æ›´æ–°ï¼‰

##### 1.7.3 å¯è§†åŒ–æŒ‘æˆ˜ä¸Žè§£å†³æ–¹æ¡ˆ

**æ ¸å¿ƒé—®é¢˜**ï¼šæ¨ªè½´èŒƒå›´å·®å¼‚å·¨å¤§
- MADDPGæ”¶æ•›ï¼š~400æ¬¡çŽ¯å¢ƒè¯„ä¼°
- BR-PSOæ”¶æ•›ï¼š~36,000æ¬¡çŽ¯å¢ƒè¯„ä¼°ï¼ˆ90å€å·®è·ï¼‰
- ç›´æŽ¥ç»˜åˆ¶ä¼šå¯¼è‡´MADDPGæ›²çº¿åŽ‹ç¼©åœ¨æœ€å·¦ä¾§10%åŒºåŸŸ

**æ–¹æ¡ˆ1ï¼šå¯¹æ•°æ¨ªè½´ï¼ˆæŽ¨è1ï¼‰**

```
ç­–ç•¥å˜åŒ–çŽ‡(log)
  â”‚
  â”‚     MADDPG          BR-PSO
  â”‚      åŒºåŸŸ            åŒºåŸŸ
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â”‚        â”‚      â”‚        â”‚
  â””â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â†’ çŽ¯å¢ƒè¯„ä¼°æ¬¡æ•°(log)
     10Â¹    10Â²     10Â³      10â´
```

**ä¼˜ç‚¹**ï¼š
- âœ… å•ä¸€è¿žç»­åæ ‡è½´
- âœ… ä¸¤ä¸ªç®—æ³•éƒ½æœ‰åˆç†çš„æ˜¾ç¤ºç©ºé—´
- âœ… è‡ªç„¶å¼ºè°ƒæ•°é‡çº§å·®å¼‚
- âœ… ç§‘ç ”è®ºæ–‡å¸¸ç”¨æ–¹å¼

**ç¼ºç‚¹**ï¼š
- âš ï¸ å¯¹æ•°åˆ»åº¦å¯èƒ½ä¸ç›´è§‚ï¼ˆéœ€åœ¨å›¾æ³¨ä¸­è¯´æ˜Žï¼‰

**æ–¹æ¡ˆ2ï¼šæ–­è½´å›¾**

```
ç­–ç•¥å˜åŒ–çŽ‡
  â”‚
  â”‚  MADDPGæ”¶æ•›åŒºåŸŸ     //    BR-PSOæ”¶æ•›åŒºåŸŸ
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       //    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â”‚å¯†é›†æ›²çº¿  â”‚       //    â”‚ç¨€ç–æ›²çº¿   â”‚
  â””â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€//â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â†’ çŽ¯å¢ƒè¯„ä¼°æ¬¡æ•°
     0        400      //    35,000  36,000
                    æ–­è½´æ ‡è®°
```

**ä¼˜ç‚¹**ï¼š
- âœ… ä¸¤ä¸ªç®—æ³•çš„æ”¶æ•›ç»†èŠ‚éƒ½æ¸…æ™°å¯è§
- âœ… ä¿æŒå…¬å¹³å¯¹æ¯”ï¼ˆä»ç„¶æ˜¯åŒä¸€ä¸ªç‰©ç†é‡ï¼‰
- âœ… æ˜Žç¡®æ ‡æ³¨æ–­è½´ï¼Œä¸è¯¯å¯¼è¯»è€…

**ç¼ºç‚¹**ï¼š
- âš ï¸ å®žçŽ°ç¨å¤æ‚ï¼ˆéœ€è¦matplotlibçš„broken_axisæˆ–æ‰‹åŠ¨ç»˜åˆ¶ä¸¤ä¸ªå­å›¾ï¼‰

**æ–¹æ¡ˆ3ï¼šåŒå­å›¾å¹¶åˆ—ï¼ˆæŽ¨è2ï¼‰**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (a) MADDPGç­–ç•¥å˜åŒ–çŽ‡          â”‚
â”‚     Xè½´: 0-500               â”‚
â”‚     å¯†é›†æ›²çº¿æ¸…æ™°å¯è§          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (b) BR-PSOç­–ç•¥å˜åŒ–çŽ‡          â”‚
â”‚     Xè½´: 0-40,000            â”‚
â”‚     ç¨€ç–ç‚¹/é˜¶è·ƒçº¿æ¸…æ™°å¯è§     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ¯ä¸ªç®—æ³•éƒ½æœ‰æœ€ä¼˜çš„å¯è§†åŒ–æ•ˆæžœ
- âœ… é¿å…æ¨ªè½´èŒƒå›´å†²çª
- âœ… å®žçŽ°ç®€å•

**ç¼ºç‚¹**ï¼š
- âš ï¸ æ— æ³•ç›´è§‚å¯¹æ¯”æ”¶æ•›é€Ÿåº¦ï¼ˆæ¨ªè½´ä¸ç»Ÿä¸€ï¼‰
- âš ï¸ éœ€è¦åœ¨å›¾æ³¨ä¸­æ˜Žç¡®è¯´æ˜Žè®¡ç®—æˆæœ¬å·®å¼‚

**æ–¹æ¡ˆ4ï¼šæ’å›¾æ”¾å¤§**

```
ç­–ç•¥å˜åŒ–çŽ‡
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â”‚ æ’å›¾ï¼š      â”‚
  â”‚  â”‚ MADDPGæ”¾å¤§ â”‚
  â”‚  â”‚ (0-500)    â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚                           BR-PSOæ›²çº¿
  â”‚  MADDPGæ›²çº¿ï¼ˆåŽ‹ç¼©ï¼‰         â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚  â”‚                        â”‚      â”‚
  â””â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â†’ çŽ¯å¢ƒè¯„ä¼°æ¬¡æ•°
     0                            36,000
```

**ä¼˜ç‚¹**ï¼š
- âœ… ä¿æŒå…¬å¹³å¯¹æ¯”çš„å®Œæ•´æ€§ï¼ˆå•ä¸€æ¨ªè½´ï¼‰
- âœ… é€šè¿‡æ’å›¾å±•ç¤ºMADDPGç»†èŠ‚
- âœ… è§†è§‰ä¸Šå¼ºè°ƒè®¡ç®—æ•ˆçŽ‡å·®å¼‚

**ç¼ºç‚¹**ï¼š
- âš ï¸ æ’å›¾å ç”¨ç©ºé—´ï¼Œä¸»å›¾ä¿¡æ¯å¯†åº¦é™ä½Ž

##### 1.7.4 å®žæ–½è®¡åˆ’

**é˜¶æ®µ1ï¼šå®žçŽ°å¤šç§æ–¹æ¡ˆ**
- ç»˜åˆ¶æ–¹æ¡ˆ1ï¼ˆå¯¹æ•°æ¨ªè½´ï¼‰
- ç»˜åˆ¶æ–¹æ¡ˆ2ï¼ˆæ–­è½´å›¾ï¼‰
- ç»˜åˆ¶æ–¹æ¡ˆ3ï¼ˆåŒå­å›¾ï¼‰
- ç»˜åˆ¶æ–¹æ¡ˆ4ï¼ˆæ’å›¾æ”¾å¤§ï¼‰

**é˜¶æ®µ2ï¼šæ•ˆæžœå¯¹æ¯”**
- è§†è§‰æ¸…æ™°åº¦å¯¹æ¯”
- ä¿¡æ¯ä¼ è¾¾æ•ˆæžœå¯¹æ¯”
- é€‰æ‹©æœ€ç»ˆæ–¹æ¡ˆç”¨äºŽè®ºæ–‡/æŠ¥å‘Š

**å€¾å‘æ–¹æ¡ˆ**ï¼š
- **ä¼˜å…ˆå°è¯•ï¼šæ–¹æ¡ˆ2ï¼ˆå¯¹æ•°æ¨ªè½´ï¼‰å’Œæ–¹æ¡ˆ3ï¼ˆåŒå­å›¾ï¼‰**
- æ ¹æ®å®žé™…ç»˜åˆ¶æ•ˆæžœå†³å®šæœ€ç»ˆé€‰æ‹©

##### 1.7.5 è¡¥å……æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰

é™¤äº†ç­–ç•¥å˜åŒ–çŽ‡ï¼Œè¿˜å¯ä»¥ç»˜åˆ¶ï¼š

**ç³»ç»Ÿæ€»æ”¶ç›Š**ï¼š
```python
total_reward = sum(reward_i for all agents)
```
- ç”¨äºŽæ¯”è¾ƒä¸¤ç§ç®—æ³•æ‰¾åˆ°çš„å‡è¡¡è§£çš„ç¤¾ä¼šæ€»ç¦åˆ©

**å¹³å‡ä»·æ ¼**ï¼š
```python
average_price = mean(price[i,t] for all i,t)
```
- ç”¨äºŽè§‚å¯Ÿä¸¤ç§ç®—æ³•æ‰¾åˆ°çš„å‡è¡¡ä»·æ ¼æ°´å¹³æ˜¯å¦ä¸€è‡´

**Nashé—´éš™**ï¼ˆç¨€ç–éªŒè¯ç‚¹ï¼‰ï¼š
```python
nash_gap = max(improvement_gap_i for all i)
```
- é‡åŒ–è·ç¦»çœŸå®žçº³ä»€å‡è¡¡çš„è·ç¦»
- ä»…åœ¨éªŒè¯ç‚¹ç»˜åˆ¶ï¼ˆä¸æ˜¯æ¯æ­¥éƒ½éªŒè¯ï¼‰

**æ³¨æ„**ï¼šä¸å»ºè®®ç»˜åˆ¶å•ä¸ªå……ç”µç«™çš„æ”¶ç›Šå˜åŒ–æ›²çº¿è¿›è¡Œç®—æ³•å¯¹æ¯”
- **åŽŸå› **ï¼šä¸åŒç®—æ³•æ‰¾åˆ°çš„å‡è¡¡è§£ä¸­ï¼Œå„å……ç”µç«™æ”¶ç›Šåˆ†é…å¯èƒ½å®Œå…¨ä¸åŒ
- **æ›¿ä»£**ï¼šä½¿ç”¨ç³»ç»Ÿæ€»æ”¶ç›Šè¿›è¡Œå¯¹æ¯”ï¼ˆå¯ä»¥åˆ¤æ–­å“ªä¸ªå‡è¡¡æ›´ä¼˜ï¼‰

---

## 2. Independent DDPG (IDDPG)

### 2.1 ç®—æ³•æ¥æºä¸Žæ ¸å¿ƒæ€æƒ³

**ç†è®ºåŸºç¡€**ï¼š
- **è®ºæ–‡å‚è€ƒ**: "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments" (MADDPGåŽŸè®ºæ–‡, 2017) ä¸­ä½œä¸ºä¸»è¦baseline
- **æ ¸å¿ƒæ€æƒ³**: æ¯ä¸ªagentç‹¬ç«‹è¿è¡Œè‡ªå·±çš„DDPGç®—æ³•ï¼Œå°†å…¶ä»–agentè§†ä¸ºçŽ¯å¢ƒçš„ä¸€éƒ¨åˆ†ï¼ˆNon-stationary Environmentï¼‰

**ä¸ŽMADDPGçš„æœ¬è´¨åŒºåˆ«**ï¼š
```python
# MADDPG: ä¸­å¿ƒåŒ–è®­ç»ƒï¼ŒCriticä½¿ç”¨å…¨å±€ä¿¡æ¯
Q_i(s_global, a_1, a_2, ..., a_N)

# IDDPG: åŽ»ä¸­å¿ƒåŒ–è®­ç»ƒï¼ŒCriticä»…ä½¿ç”¨å±€éƒ¨ä¿¡æ¯
Q_i(s_local, a_i)
```

**ç®—æ³•ç‰¹ç‚¹**ï¼š
- âœ… **å®Œå…¨åŽ»ä¸­å¿ƒåŒ–è®­ç»ƒ**ï¼šæ— éœ€ä¸­å¿ƒåŒ–åè°ƒï¼ŒçœŸæ­£çš„åˆ†å¸ƒå¼æ‰§è¡Œ
- âœ… **æ‰©å±•æ€§å¥½**ï¼šagentæ•°é‡å¢žåŠ ä¸å½±å“å•ä¸ªagentçš„ç½‘ç»œè§„æ¨¡
- âœ… **å®žçŽ°ç®€å•**ï¼šNä¸ªç‹¬ç«‹çš„DDPG agent
- âŒ **Non-stationaryçŽ¯å¢ƒé—®é¢˜**ï¼šå…¶ä»–agentç­–ç•¥æ›´æ–°å¯¼è‡´çŽ¯å¢ƒä¸ç¨³å®š
- âŒ **æ”¶æ•›æ€§è¾ƒå·®**ï¼šç¼ºä¹å…¨å±€ä¿¡æ¯ï¼Œå®¹æ˜“é™·å…¥æ¬¡ä¼˜å‡è¡¡

### 2.2 é€‚é…å½“å‰é¡¹ç›®çš„è®¾è®¡æ–¹æ¡ˆ

#### 2.2.1 Criticç½‘ç»œè¾“å…¥è®¾è®¡

**æ ¸å¿ƒå†³ç­–ï¼šå¼±åŽ»ä¸­å¿ƒåŒ–ï¼ˆæ–¹æ¡ˆBï¼‰**

æ ¹æ®é¡¹ç›®çš„åšå¼ˆåœºæ™¯ç‰¹ç‚¹ï¼ˆå……ç”µç«™å¯ä»¥è§‚æµ‹ç«žäº‰å¯¹æ‰‹ä»·æ ¼ï¼‰ï¼Œé‡‡ç”¨å¼±åŽ»ä¸­å¿ƒåŒ–æ–¹æ¡ˆï¼š

```python
# IDDPG Criticè¾“å…¥ç»„æˆ
critic_input = [
    global_prices_{t-1},     # ä¸Šä¸€è½®æ‰€æœ‰å……ç”µç«™ä»·æ ¼ï¼ˆå¯è§‚æµ‹ï¼Œ32ç»´ï¼‰
    own_flow_{t-1},          # ä¸Šä¸€è½®è‡ªèº«å……ç”µæµé‡ï¼ˆå±€éƒ¨ä¿¡æ¯ï¼Œ8ç»´ï¼‰
    own_action_t             # å½“å‰è½®è‡ªèº«åŠ¨ä½œï¼ˆå±€éƒ¨å†³ç­–ï¼Œ8ç»´ï¼‰
]
# æ€»ç»´åº¦ï¼š48ç»´ï¼ˆç›¸æ¯”MADDPGçš„96ç»´å‡å°‘ä¸€åŠï¼‰
```

**æ—¶é—´ç»´åº¦è¯´æ˜Ž**ï¼š
- `global_prices_{t-1}`: çŽ¯å¢ƒè§‚æµ‹ä¸­çš„`last_round_all_prices`ï¼ŒåŽ†å²ä¿¡æ¯
- `own_flow_{t-1}`: çŽ¯å¢ƒè§‚æµ‹ä¸­çš„`own_charging_flow`ï¼ŒåŽ†å²ä¿¡æ¯
- `own_action_t`: å½“å‰å†³ç­–çš„ä»·æ ¼åŠ¨ä½œï¼Œè¯„ä¼°å¯¹è±¡

**ä¸ŽMADDPGçš„å…³é”®åŒºåˆ«**ï¼š

| ä¿¡æ¯ç±»åž‹ | MADDPG | IDDPG |
|---------|--------|-------|
| å…¨å±€ä»·æ ¼åŽ†å² | âœ… (32ç»´) | âœ… (32ç»´) |
| æ‰€æœ‰agentå……ç”µæµé‡ | âœ… (32ç»´) | âŒ ä»…è‡ªèº« (8ç»´) |
| æ‰€æœ‰agentå½“å‰åŠ¨ä½œ | âœ… (32ç»´) | âŒ ä»…è‡ªèº« (8ç»´) |
| **æ€»è¾“å…¥ç»´åº¦** | **96ç»´** | **48ç»´** |

**é€‰æ‹©å¼±åŽ»ä¸­å¿ƒåŒ–çš„ç†ç”±**ï¼š
1. âœ… ç¬¦åˆçŽ°å®žåœºæ™¯ï¼šå……ç”µç«™å¯ä»¥è§‚æµ‹ç«žäº‰å¯¹æ‰‹å…¬å¼€ä»·æ ¼
2. âœ… ä¸Žé¡¹ç›®çŽ¯å¢ƒå®šä¹‰ä¸€è‡´ï¼š`last_round_all_prices`å·²è¢«æä¾›
3. âœ… ä¸ŽMADDPGåŽŸè®ºæ–‡ä¸­çš„IDDPG baselineå®žçŽ°æ–¹å¼ä¸€è‡´
4. âœ… ä¿æŒIDDPGçš„æ ¸å¿ƒç‰¹å¾ï¼š**Criticä¸ä½¿ç”¨å…¶ä»–agentçš„åŠ¨ä½œå’Œç§æœ‰çŠ¶æ€ï¼ˆæµé‡ï¼‰**

#### 2.2.2 ReplayBufferè®¾è®¡

**æ ¸å¿ƒå†³ç­–ï¼šæ¯ä¸ªagentç»´æŠ¤ç‹¬ç«‹çš„ReplayBuffer**

```python
class IndependentDDPG:
    def __init__(self, agent_ids, ...):
        # æ¯ä¸ªagentç‹¬ç«‹çš„ReplayBuffer
        self.replay_buffers = {
            agent_id: ReplayBuffer(capacity=10000)
            for agent_id in agent_ids
        }

        # æ¯ä¸ªagentç‹¬ç«‹çš„DDPGå®žä¾‹
        self.ddpg_agents = {
            agent_id: SingleDDPG(obs_dim, action_dim, local_state_dim, ...)
            for agent_id in agent_ids
        }
```

**å­˜å‚¨å†…å®¹å¯¹æ¯”**ï¼š

| ç»´åº¦ | MADDPG | IDDPG |
|------|--------|-------|
| Bufferæ•°é‡ | 1ä¸ªå…¨å±€å…±äº« | Nä¸ªç‹¬ç«‹Buffer |
| å­˜å‚¨ç»“æž„ | `(obs_all, actions_all, rewards_all, next_obs_all, dones_all)` | `(obs_i, action_i, reward_i, next_obs_i, done_i)` |
| é‡‡æ ·æ–¹å¼ | å…¨å±€ç»Ÿä¸€é‡‡æ · | å„agentç‹¬ç«‹é‡‡æ · |
| ä¿¡æ¯éš”ç¦» | æ‰€æœ‰agentä¿¡æ¯é›†ä¸­ | æ¯ä¸ªagentä¿¡æ¯éš”ç¦» |

**ä¼˜åŠ¿**ï¼š
- âœ… å®Œå…¨åŽ»ä¸­å¿ƒåŒ–ï¼Œæ— éœ€agenté—´é€šä¿¡ï¼ˆè®­ç»ƒæ—¶ï¼‰
- âœ… ç¬¦åˆIDDPG"ç‹¬ç«‹å­¦ä¹ "çš„ç†è®ºå®šä¹‰
- âœ… ä¾¿äºŽå¹¶è¡Œè®­ç»ƒï¼ˆå„agentå¯å¼‚æ­¥æ›´æ–°ï¼‰

#### 2.2.3 ç½‘ç»œæž¶æž„è®¾è®¡

**Actorç½‘ç»œï¼šä¸ŽMADDPGå®Œå…¨ç›¸åŒ**

```python
# IDDPG Actorï¼ˆä¸ŽMADDPGä¸€è‡´ï¼‰
class ActorNetwork:
    def __init__(self, obs_dim, action_dim=8, hidden_sizes=(64, 64)):
        # è¾“å…¥ï¼šå±€éƒ¨è§‚æµ‹ï¼ˆprocess_observationså¤„ç†åŽï¼Œ40ç»´ï¼‰
        # è¾“å‡ºï¼šå½’ä¸€åŒ–ä»·æ ¼ [0,1]^8
```

**Criticç½‘ç»œï¼šè¾“å…¥ç»´åº¦ä¸åŒï¼Œç»“æž„ç›¸åŒ**

```python
# MADDPG Critic
CriticNetwork(input_dim=96, hidden_sizes=(128, 64))

# IDDPG Critic
CriticNetwork(input_dim=48, hidden_sizes=(128, 64))  # è¾“å…¥ç»´åº¦å‡åŠï¼Œéšè—å±‚ä¿æŒä¸€è‡´
```

**è®¾è®¡åŽŸåˆ™**ï¼š
- âœ… Actorç½‘ç»œå®Œå…¨ç›¸åŒï¼Œç¡®ä¿åŠ¨ä½œç”Ÿæˆæœºåˆ¶ä¸€è‡´
- âœ… Criticéšè—å±‚ç»“æž„ä¿æŒä¸€è‡´ï¼Œç¡®ä¿ç½‘ç»œå®¹é‡å…¬å¹³
- âœ… ä»…Criticè¾“å…¥ç»´åº¦ä¸åŒï¼Œä½“çŽ°ç®—æ³•æ ¸å¿ƒå·®å¼‚

#### 2.2.4 è®­ç»ƒæ›´æ–°é€»è¾‘

**IDDPGçš„ç‹¬ç«‹æ›´æ–°æµç¨‹**ï¼š

```python
def learn(self):
    """å®Œå…¨åŽ»ä¸­å¿ƒåŒ–çš„å­¦ä¹ æ›´æ–°"""
    # æ¯ä¸ªagentç‹¬ç«‹ä»Žè‡ªå·±çš„Bufferé‡‡æ ·å’Œæ›´æ–°
    for agent_id in self.agent_ids:
        # 1. æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿç»éªŒ
        if len(self.replay_buffers[agent_id]) < min_buffer_size:
            continue

        # 2. ä»Žè¯¥agentçš„ç‹¬ç«‹Bufferé‡‡æ ·
        batch = self.replay_buffers[agent_id].sample(batch_size)

        # 3. æ›´æ–°è¯¥agentçš„Criticï¼ˆä½¿ç”¨å±€éƒ¨ä¿¡æ¯ï¼‰
        self._update_critic(agent_id, batch)

        # 4. æ›´æ–°è¯¥agentçš„Actorï¼ˆä½¿ç”¨å±€éƒ¨Criticï¼‰
        self._update_actor(agent_id, batch)

        # 5. è½¯æ›´æ–°è¯¥agentçš„ç›®æ ‡ç½‘ç»œ
        self.ddpg_agents[agent_id].soft_update(tau)
```

**ç›®æ ‡Qå€¼è®¡ç®—çš„å…³é”®åŒºåˆ«**ï¼š

```python
# MADDPG: ä½¿ç”¨æ‰€æœ‰agentçš„ç›®æ ‡Actorç”Ÿæˆnext_actions
def _compute_target_q_maddpg(batch_next_obs):
    next_actions = {}
    for aid in all_agents:
        next_actions[aid] = agents[aid].actor_target(batch_next_obs[aid])
    global_next_state = organize_global_state(batch_next_obs, next_actions)
    target_q = critic_target(global_next_state)

# IDDPG: ä»…ä½¿ç”¨è‡ªå·±çš„ç›®æ ‡Actor
def _compute_target_q_iddpg(agent_id, batch_next_obs):
    next_action = agents[agent_id].actor_target(batch_next_obs)
    local_next_state = organize_local_state(batch_next_obs, next_action)
    target_q = critic_target(local_next_state)
```

**æ ¸å¿ƒå·®å¼‚æ€»ç»“**ï¼š

| ç»´åº¦ | MADDPG | IDDPG |
|------|--------|-------|
| é‡‡æ ·æ¥æº | å…¨å±€Bufferï¼ˆæ‰€æœ‰agentå…±äº«ï¼‰ | ç‹¬ç«‹Bufferï¼ˆå„agentéš”ç¦»ï¼‰ |
| Criticè¾“å…¥ | å…¨å±€çŠ¶æ€+æ‰€æœ‰agentåŠ¨ä½œ | å±€éƒ¨çŠ¶æ€+è‡ªèº«åŠ¨ä½œ |
| ç›®æ ‡Qè®¡ç®— | éœ€è¦æ‰€æœ‰agentçš„actor_target | ä»…éœ€è‡ªå·±çš„actor_target |
| Actoræ¢¯åº¦æ¥æº | å…¨å±€Criticï¼ˆçŸ¥é“å…¶ä»–agentåŠ¨ä½œï¼‰ | å±€éƒ¨Criticï¼ˆä¸çŸ¥é“å…¶ä»–agentåŠ¨ä½œï¼‰ |
| æ›´æ–°è€¦åˆ | âœ… è€¦åˆï¼ˆéœ€åè°ƒï¼‰ | âŒ å®Œå…¨ç‹¬ç«‹ |

#### 2.2.5 è¾…åŠ©å‡½æ•°è®¾è®¡

**`organize_local_state` å‡½æ•°**ï¼ˆIDDPGä¸“ç”¨ï¼‰ï¼š

```python
def organize_local_state(observation, own_action):
    """
    ç»„ç»‡å•ä¸ªagentçš„å±€éƒ¨çŠ¶æ€ä¿¡æ¯ï¼ˆå¯¹åº”MADDPGçš„organize_global_stateï¼‰

    Args:
        observation (dict): å•ä¸ªagentçš„è§‚æµ‹
            - "last_round_all_prices": np.ndarray (n_agents, n_periods)
            - "own_charging_flow": np.ndarray (n_periods,)
        own_action (np.ndarray): å•ä¸ªagentçš„åŠ¨ä½œ (n_periods,)

    Returns:
        np.ndarray: å±€éƒ¨çŠ¶æ€å‘é‡ï¼Œå½¢çŠ¶ (48,)
    """
    # 1. å…¨å±€ä»·æ ¼åŽ†å²ï¼ˆå¯è§‚æµ‹ä¿¡æ¯ï¼‰
    global_prices = observation["last_round_all_prices"].flatten()  # 32ç»´

    # 2. è‡ªèº«å……ç”µæµé‡ï¼ˆå±€éƒ¨ä¿¡æ¯ï¼‰
    own_flow = observation["own_charging_flow"].flatten()           # 8ç»´

    # 3. è‡ªèº«å½“å‰åŠ¨ä½œï¼ˆè¯„ä¼°å¯¹è±¡ï¼‰
    own_action_flat = own_action.flatten()                          # 8ç»´

    return np.concatenate([global_prices, own_flow, own_action_flat])  # 48ç»´
```

**`process_observations` å‡½æ•°**ï¼šä¸ŽMADDPGå®Œå…¨ç›¸åŒï¼ˆå¤ç”¨é€»è¾‘ï¼‰

```python
def process_observations(observation):
    """
    å¤„ç†å•ä¸ªagentè§‚æµ‹æ•°æ®ä¸ºActorç½‘ç»œè¾“å…¥ï¼ˆä¸ŽMADDPGç›¸åŒï¼‰

    Returns:
        np.ndarray: å±•å¹³çš„è§‚æµ‹å‘é‡ï¼Œå½¢çŠ¶ (40,)
    """
    last_prices = observation["last_round_all_prices"].flatten()  # 32ç»´
    own_flow = observation["own_charging_flow"].flatten()         # 8ç»´
    return np.concatenate([last_prices, own_flow])                # 40ç»´
```

### 2.3 è¶…å‚æ•°é…ç½®å¯¹é½

**å…³é”®åŽŸåˆ™**ï¼šé™¤äº†ç½‘ç»œè¾“å…¥ç»´åº¦ï¼Œæ‰€æœ‰è¶…å‚æ•°ä¸ŽMADDPGå®Œå…¨ä¸€è‡´

```python
IDDPG_CONFIG = {
    # ç½‘ç»œç»“æž„
    'actor_hidden_sizes': (64, 64),      # ä¸ŽMADDPGä¸€è‡´
    'critic_hidden_sizes': (128, 64),    # ä¸ŽMADDPGä¸€è‡´ï¼ˆä»…è¾“å…¥ç»´åº¦ä¸åŒï¼‰

    # å­¦ä¹ çŽ‡
    'actor_lr': 0.001,                   # ä¸ŽMADDPGä¸€è‡´
    'critic_lr': 0.001,                  # ä¸ŽMADDPGä¸€è‡´

    # è®­ç»ƒå‚æ•°
    'gamma': 0.95,                       # ä¸ŽMADDPGä¸€è‡´
    'tau': 0.01,                         # ä¸ŽMADDPGä¸€è‡´
    'buffer_capacity': 10000,            # ä¸ŽMADDPGä¸€è‡´ï¼ˆä½†æ¯ä¸ªagentç‹¬ç«‹ï¼‰
    'batch_size': 64,                    # ä¸ŽMADDPGä¸€è‡´ï¼ˆåŠ¨æ€è°ƒæ•´ç­–ç•¥ç›¸åŒï¼‰

    # æŽ¢ç´¢ç­–ç•¥
    'noise_sigma': 0.2,                  # ä¸ŽMADDPGä¸€è‡´
    'noise_decay': 0.9995,               # ä¸ŽMADDPGä¸€è‡´
    'noise_min_sigma': 0.01,             # ä¸ŽMADDPGä¸€è‡´

    # å¥–åŠ±å½’ä¸€åŒ–
    'reward_normalization': 'max_affine' # ä¸ŽMADDPGä¸€è‡´ï¼ˆåšå¼ˆç‰¹å®šå½’ä¸€åŒ–ï¼‰
}
```

**å¯¹é½ç†ç”±**ï¼š
- âœ… ç¡®ä¿å…¬å¹³å¯¹æ¯”ï¼šæŽ’é™¤è¶…å‚æ•°å·®å¼‚çš„å½±å“
- âœ… éš”ç¦»ç®—æ³•å·®å¼‚ï¼šå¯¹æ¯”ç»“æžœä»…åæ˜ æž¶æž„å·®å¼‚ï¼ˆä¸­å¿ƒåŒ– vs åŽ»ä¸­å¿ƒåŒ–ï¼‰
- âœ… ç®€åŒ–å®žéªŒè®¾è®¡ï¼šæ— éœ€ä¸ºIDDPGå•ç‹¬è°ƒå‚

### 2.4 æ”¶æ•›åˆ¤åˆ«ä¸Žè¯„ä¼°

#### 2.4.1 æ”¶æ•›æ ‡å‡†ï¼ˆä¸ŽMADDPGå®Œå…¨ä¸€è‡´ï¼‰

```python
CONVERGENCE_CONFIG = {
    'convergence_threshold': 0.01,       # 1%å¹³å‡ç›¸å¯¹å˜åŒ–
    'stable_steps_required': 5,          # å•episodeå†…è¿žç»­æ”¶æ•›æ­¥æ•°
    'stable_episodes_required': 3,       # è®­ç»ƒç»ˆæ­¢çš„è¿žç»­æ”¶æ•›episodes
}
```

#### 2.4.2 å¯¹æ¯”è¯„ä¼°æŒ‡æ ‡

**1. æ”¶æ•›é€Ÿåº¦**ï¼š
- æ¨ªè½´ï¼šç´¯ç§¯çŽ¯å¢ƒè¯„ä¼°æ¬¡æ•°ï¼ˆä¸ŽBR-PSOå¯¹æ¯”æ–¹å¼ä¸€è‡´ï¼‰
- çºµè½´ï¼šç­–ç•¥å˜åŒ–çŽ‡ï¼ˆå¯¹æ•°åæ ‡ï¼‰
- é¢„æœŸï¼šIDDPGå¯èƒ½æ¯”MADDPGæ”¶æ•›æ›´æ…¢æˆ–æ›´ä¸ç¨³å®š

**2. å‡è¡¡è´¨é‡**ï¼š
- æœ€ç»ˆçº³ä»€å‡è¡¡çš„ä»·æ ¼ç­–ç•¥
- ç³»ç»Ÿæ€»æ”¶ç›Šï¼ˆç¤¾ä¼šç¦åˆ©ï¼‰
- å„å……ç”µç«™æ”¶ç›Šåˆ†é…
- Nashé—´éš™ï¼ˆä¸ŽçœŸå®žçº³ä»€å‡è¡¡çš„è·ç¦»ï¼‰

**3. è®­ç»ƒç¨³å®šæ€§**ï¼š
- Episodeé•¿åº¦åˆ†å¸ƒ
- æ”¶ç›Šæ›²çº¿æ³¢åŠ¨æ€§ï¼ˆæ ‡å‡†å·®ï¼‰
- æ˜¯å¦èƒ½ç¨³å®šæ”¶æ•›åˆ°å‡è¡¡

**4. è®¡ç®—æ•ˆçŽ‡**ï¼š
- å•æ¬¡updateå¹³å‡è€—æ—¶ï¼ˆIDDPGåº”è¯¥æ›´å¿«ï¼‰
- å†…å­˜å ç”¨ï¼ˆIDDPGç½‘ç»œæ›´å°ï¼‰
- è¾¾åˆ°æ”¶æ•›æ‰€éœ€çš„æ€»æ—¶é—´

### 2.5 é¢„æœŸå®žéªŒç»“æžœ

**ç†è®ºé¢„æœŸ**ï¼š

1. **æ”¶æ•›æ€§**ï¼š
   - âœ… IDDPGåº”è¯¥èƒ½æ”¶æ•›åˆ°æŸä¸ªçº³ä»€å‡è¡¡ï¼ˆé™æ€åšå¼ˆçŽ¯å¢ƒç›¸å¯¹ç®€å•ï¼‰
   - âš ï¸ æ”¶æ•›é€Ÿåº¦å¯èƒ½æ¯”MADDPGæ…¢ï¼ˆç¼ºä¹å…¨å±€ä¿¡æ¯ï¼Œéœ€è¦æ›´å¤šæŽ¢ç´¢ï¼‰
   - âš ï¸ å¯èƒ½é™·å…¥æ¬¡ä¼˜å‡è¡¡ï¼ˆç‰¹åˆ«æ˜¯å­˜åœ¨å¤šä¸ªçº³ä»€å‡è¡¡æ—¶ï¼‰

2. **å‡è¡¡è´¨é‡**ï¼š
   - âš ï¸ æ‰¾åˆ°çš„å‡è¡¡è§£å¯èƒ½åŠ£äºŽMADDPGï¼ˆç³»ç»Ÿæ€»æ”¶ç›Šæ›´ä½Žï¼‰
   - âš ï¸ å¯èƒ½æ”¶æ•›åˆ°ä¸åŒçš„å‡è¡¡ç‚¹ï¼ˆå¦‚æžœæœ‰å¤šä¸ªå‡è¡¡ï¼‰

3. **è®¡ç®—æ•ˆçŽ‡**ï¼š
   - âœ… å•æ¬¡æ›´æ–°é€Ÿåº¦æ›´å¿«ï¼ˆç½‘ç»œæ›´å°ï¼Œ48ç»´ vs 96ç»´ï¼‰
   - âš ï¸ ä½†å¯èƒ½éœ€è¦æ›´å¤šstepsæ‰èƒ½æ”¶æ•›
   - âœ… å†…å­˜å ç”¨æ›´å°‘ï¼ˆç‹¬ç«‹Bufferä½†ä¿¡æ¯æ›´ç®€å•ï¼‰

4. **è®­ç»ƒç¨³å®šæ€§**ï¼š
   - âš ï¸ å¯èƒ½å‡ºçŽ°æŒ¯è¡ï¼ˆNon-stationaryçŽ¯å¢ƒé—®é¢˜ï¼‰
   - âš ï¸ æ”¶ç›Šæ›²çº¿æ³¢åŠ¨å¯èƒ½æ›´å¤§

**æ–‡çŒ®æ”¯æŒ**ï¼š
- MADDPGåŽŸè®ºæ–‡åœ¨åˆä½œ-ç«žäº‰æ··åˆçŽ¯å¢ƒä¸­ï¼ŒIDDPGè¡¨çŽ°æ˜Žæ˜¾å¼±äºŽMADDPG
- ä½†åœ¨çº¯ç«žäº‰åšå¼ˆçŽ¯å¢ƒä¸­ï¼ŒIDDPGçš„æ€§èƒ½ä¸‹é™å¯èƒ½ä¸ä¼šå¤ªä¸¥é‡
- æœ¬é¡¹ç›®çš„ä»·æ ¼åšå¼ˆå±žäºŽçº¯ç«žäº‰åœºæ™¯ï¼ŒIDDPGå¯èƒ½æœ‰ä¸€å®šè¡¨çŽ°

### 2.6 å®žçŽ°æž¶æž„è®¾è®¡

#### 2.6.1 æ–‡ä»¶ç»“æž„ï¼ˆå®Œå…¨ç‹¬ç«‹å®žçŽ°ï¼‰

**æ ¸å¿ƒåŽŸåˆ™**ï¼šä¿æŒç®—æ³•æ¨¡å—çš„å®Œå…¨ç‹¬ç«‹æ€§ï¼Œå³ä½¿å­˜åœ¨ä»£ç é‡å¤

```
src/algorithms/
â”œâ”€â”€ maddpg/                        # çŽ°æœ‰MADDPGå®žçŽ°
â”‚   â”œâ”€â”€ maddpg.py                 # ReplayBuffer, GaussianNoise, DDPG, MADDPG
â”‚   â””â”€â”€ networks.py               # ActorNetwork, CriticNetwork
â””â”€â”€ iddpg/                         # æ–°å¢žIDDPGå®žçŽ°ï¼ˆå®Œå…¨ç‹¬ç«‹ï¼‰
    â”œâ”€â”€ iddpg.py                   # ReplayBuffer, GaussianNoise, SingleDDPG, IndependentDDPG
    â”‚                              # ï¼ˆå®Œæ•´å¤åˆ¶å·¥å…·ç±»ï¼Œç¡®ä¿æ¨¡å—ç‹¬ç«‹ï¼‰
    â””â”€â”€ networks.py                # ActorNetwork, CriticNetwork
                                   # ï¼ˆå¤åˆ¶ä¸€ä»½ï¼Œç¡®ä¿ç®—æ³•è‡ªåŒ…å«ï¼‰
```

**ä»£ç å¤ç”¨è¯´æ˜Ž**ï¼š
- âŒ **ä¸ä»Žmaddpgå¯¼å…¥ä»»ä½•ä»£ç **ï¼šç¡®ä¿ç®—æ³•æ¨¡å—å®Œå…¨ç‹¬ç«‹
- âœ… **å®Œæ•´å¤åˆ¶å¿…è¦çš„ç±»**ï¼šReplayBuffer, GaussianNoise, ActorNetwork, CriticNetwork
- âœ… **æ–°å®žçŽ°IDDPGä¸“å±žé€»è¾‘**ï¼šSingleDDPG, IndependentDDPG, organize_local_state

**ä¼˜åŠ¿**ï¼š
- âœ… æ¯ä¸ªç®—æ³•å®Œå…¨ç‹¬ç«‹ï¼Œä¿®æ”¹ä¸å½±å“å…¶ä»–ç®—æ³•
- âœ… ä»£ç è‡ªåŒ…å«ï¼Œæ˜“äºŽç†è§£å’Œç»´æŠ¤
- âœ… ä¾¿äºŽç‹¬ç«‹æµ‹è¯•å’Œè°ƒè¯•

**ä»£ä»·**ï¼š
- âš ï¸ ä»£ç å†—ä½™ï¼ˆå·¥å…·ç±»ä»£ç é‡å¤ï¼‰
- âš ï¸ å¤šå¤„ç»´æŠ¤ï¼ˆä¿®æ”¹bugéœ€è¦åŒæ­¥å¤šä¸ªæ–‡ä»¶ï¼‰

#### 2.6.2 è®­ç»ƒå™¨è®¾è®¡

**ç‹¬ç«‹å®žçŽ° `IDDPGTrainer`**ï¼ˆå‚è€ƒMADDPGTraineræž¶æž„ï¼‰ï¼š

```
src/trainer/
â”œâ”€â”€ MADDPGTrainer.py              # çŽ°æœ‰MADDPGè®­ç»ƒå™¨
â””â”€â”€ IDDPGTrainer.py               # æ–°å¢žIDDPGè®­ç»ƒå™¨ï¼ˆç‹¬ç«‹å®žçŽ°ï¼‰
```

**ä¿æŒä¸‰å±‚è®­ç»ƒæž¶æž„**ï¼š
- **Episodeå±‚**ï¼šåšå¼ˆæ±‚è§£å°è¯•ï¼ˆä¸ŽMADDPGä¸€è‡´ï¼‰
- **Stepå±‚**ï¼šç­–ç•¥è°ƒæ•´å¾ªçŽ¯ï¼ˆä¸ŽMADDPGä¸€è‡´ï¼‰
- **UE-DTAå±‚**ï¼šçŽ¯å¢ƒå“åº”ï¼ˆä¸ŽMADDPGä¸€è‡´ï¼‰

**å¤ç”¨é€»è¾‘**ï¼š
- æ”¶æ•›æ£€æµ‹æœºåˆ¶ï¼ˆç­–ç•¥ç›¸å¯¹å˜åŒ–çŽ‡ï¼‰
- è¿›åº¦ç›‘æŽ§æ˜¾ç¤ºï¼ˆä¸‰å±‚è¿›åº¦æ¡ï¼‰
- å®žéªŒæ•°æ®è®°å½•ï¼ˆJSONæ ¼å¼ï¼‰

### 2.7 å®žçŽ°ä¼˜å…ˆçº§ä¸Žè®¡åˆ’

**é˜¶æ®µ1ï¼šæ ¸å¿ƒç®—æ³•å®žçŽ°**
1. åˆ›å»º `src/algorithms/iddpg/` ç›®å½•
2. å¤åˆ¶å¹¶å®žçŽ° `iddpg.py`:
   - å¤åˆ¶ ReplayBuffer, GaussianNoise ç±»
   - å®žçŽ° SingleDDPG ç±»ï¼ˆå•agentçš„DDPGï¼‰
   - å®žçŽ° IndependentDDPG ç±»ï¼ˆç®¡ç†Nä¸ªSingleDDPGï¼‰
   - å®žçŽ° organize_local_state, process_observations å‡½æ•°
3. å¤åˆ¶ `networks.py`ï¼ˆActorNetwork, CriticNetworkï¼‰

**é˜¶æ®µ2ï¼šè®­ç»ƒå™¨å®žçŽ°**
1. åˆ›å»º `src/trainer/IDDPGTrainer.py`
2. å‚è€ƒ MADDPGTrainer å®žçŽ°è®­ç»ƒå¾ªçŽ¯
3. ä¿æŒä¸‰å±‚æž¶æž„å’Œæ”¶æ•›æ£€æµ‹é€»è¾‘

**é˜¶æ®µ3ï¼šå¯¹æ¯”å®žéªŒ**
1. ç›¸åŒé…ç½®ä¸‹è®­ç»ƒ MADDPG å’Œ IDDPG
2. è®°å½•æ”¶æ•›æ›²çº¿æ•°æ®
3. å¯¹æ¯”æœ€ç»ˆå‡è¡¡è§£è´¨é‡
4. åˆ†æžè®¡ç®—æ•ˆçŽ‡å·®å¼‚

**é˜¶æ®µ4ï¼šç»“æžœåˆ†æž**
1. ç»˜åˆ¶æ”¶æ•›æ›²çº¿å¯¹æ¯”å›¾
2. åˆ†æžå‡è¡¡è§£çš„å·®å¼‚åŽŸå› 
3. è®¨è®ºIDDPGçš„é€‚ç”¨æ€§å’Œå±€é™æ€§

### 2.8 å…³é”®æŠ€æœ¯è¦ç‚¹

#### 2.8.1 SingleDDPG vs MADDPGä¸­çš„DDPG

**åŒºåˆ«**ï¼š
- MADDPGä¸­çš„DDPGï¼šCriticæŽ¥æ”¶å…¨å±€çŠ¶æ€ï¼ˆ96ç»´ï¼‰
- IDDPGä¸­çš„SingleDDPGï¼šCriticæŽ¥æ”¶å±€éƒ¨çŠ¶æ€ï¼ˆ48ç»´ï¼‰

**å®žçŽ°æ³¨æ„**ï¼š
- SingleDDPGçš„Criticç½‘ç»œåˆå§‹åŒ–æ—¶ï¼Œ`global_obs_dim` å‚æ•°åº”è®¾ä¸º48
- ç›®æ ‡Qå€¼è®¡ç®—æ—¶ï¼Œä»…ä½¿ç”¨è‡ªå·±çš„next_actionï¼Œä¸è®¿é—®å…¶ä»–agentä¿¡æ¯

#### 2.8.2 ç»éªŒå­˜å‚¨ä¸Žé‡‡æ ·

```python
# IDDPGå­˜å‚¨ç»éªŒæ—¶
def store_experience(self, observations, actions, rewards, next_observations, dones):
    # å½’ä¸€åŒ–å¥–åŠ±ï¼ˆä¸ŽMADDPGä¸€è‡´ï¼‰
    normalized_rewards = normalize_rewards(rewards)

    # ä¸ºæ¯ä¸ªagentç‹¬ç«‹å­˜å‚¨
    for agent_id in self.agent_ids:
        local_experience = (
            observations[agent_id],
            actions[agent_id],
            normalized_rewards[agent_id],
            next_observations[agent_id],
            dones[agent_id]
        )
        self.replay_buffers[agent_id].add(local_experience)
```

#### 2.8.3 åŠ¨æ€æ‰¹æ¬¡å¤§å°è°ƒæ•´

**ä¸ŽMADDPGä¸€è‡´çš„åŠ¨æ€è°ƒæ•´ç­–ç•¥**ï¼š

```python
def _get_dynamic_batch_size(self, agent_id):
    """å„agentç‹¬ç«‹è°ƒæ•´æ‰¹æ¬¡å¤§å°"""
    buffer_size = len(self.replay_buffers[agent_id])
    max_batch_size = self.batch_size

    if buffer_size < max_batch_size // 2:
        return min(max_batch_size // 4, buffer_size)
    elif buffer_size < max_batch_size:
        return min(max_batch_size // 2, buffer_size)
    else:
        return min(max_batch_size, buffer_size)
```

### 2.9 ä¸Žå…¶ä»–å¯¹æ¯”ç®—æ³•çš„å…³ç³»

| ç®—æ³• | ç±»åž‹ | è®¡ç®—æˆæœ¬ | ç†è®ºæ”¶æ•›æ€§ | å®žçŽ°å¤æ‚åº¦ |
|------|------|---------|-----------|-----------|
| MADDPG | MADRL | ä¸­ç­‰ | å¥½ï¼ˆä¸­å¿ƒåŒ–è®­ç»ƒï¼‰ | ä¸­ç­‰ |
| **IDDPG** | MADRL | ä¸­ç­‰ | è¾ƒå·®ï¼ˆNon-stationaryï¼‰ | ç®€å• |
| BR-PSO | ç»å…¸åšå¼ˆè®º | é«˜ | å¥½ï¼ˆç†è®ºä¿è¯ï¼‰ | ä¸­ç­‰ |

**å¯¹æ¯”æ„ä¹‰**ï¼š
- IDDPG vs MADDPGï¼šé‡åŒ–ä¸­å¿ƒåŒ–è®­ç»ƒçš„ä»·å€¼
- IDDPG vs BR-PSOï¼šå¯¹æ¯”MADRLä¸Žç»å…¸æ–¹æ³•çš„å·®å¼‚

---

## 3. Mean Field Deep Deterministic Policy Gradient (MF-DDPG)

### 3.1 ç®—æ³•æ¥æºä¸Žæ ¸å¿ƒæ€æƒ³

**ç†è®ºåŸºç¡€**ï¼š
- **è®ºæ–‡å‚è€ƒ**: "Multi-Agent Deep Reinforcement Learning Method for EV Charging Station Game" (IEEE TPWRS, 2022)
- **æ ¸å¿ƒæ€æƒ³**: é€šè¿‡Mean Fieldè¿‘ä¼¼å°†å…¶ä»–agentçš„é›†ä½“è¡Œä¸ºåŽ‹ç¼©ä¸ºä½Žç»´åº¦ä¿¡æ¯ï¼Œå®žçŽ°çº¯ç²¹çš„ç‹¬ç«‹è®­ç»ƒèŒƒå¼

**ç®—æ³•ç‰¹ç‚¹**ï¼š
- âœ… **å®Œå…¨ç‹¬ç«‹è®­ç»ƒ**ï¼šæ¯ä¸ªagentç‹¬ç«‹å­¦ä¹ ï¼Œæ— ä¸­å¿ƒåŒ–åè°ƒ
- âœ… **Mean Fieldè¿‘ä¼¼**ï¼šç”¨å¹³å‡åœºä¿¡æ¯è¿‘ä¼¼å…¶ä»–agentè¡Œä¸º
- âœ… **æœ€ä½³æ‰©å±•æ€§**ï¼šä¸å—agentæ•°é‡å½±å“ï¼ŒO(1)å¤æ‚åº¦
- âœ… **ç†è®ºç®€æ´**ï¼šç¬¦åˆåŽ»ä¸­å¿ƒåŒ–å­¦ä¹ çš„çº¯ç²¹æ€§
- âš ï¸ **ä¿¡æ¯åŽ‹ç¼©æŸå¤±**ï¼šå¯èƒ½ä¸¢å¤±é‡è¦çš„ä¸ªä½“å·®å¼‚ä¿¡æ¯

### 3.2 è®­ç»ƒèŒƒå¼çš„ç†è®ºå®šä½

**æ ¸å¿ƒå†³ç­–ï¼šçº¯ç²¹ç‹¬ç«‹è®­ç»ƒèŒƒå¼**

ä¸Žå…¶ä»–ç®—æ³•çš„è®­ç»ƒèŒƒå¼å¯¹æ¯”ï¼š

| ç®—æ³• | è®­ç»ƒèŒƒå¼ | Criticä¿¡æ¯èŽ·å– | ç†è®ºç‰¹å¾ |
|------|---------|---------------|----------|
| **MADDPG** | ä¸­å¿ƒåŒ–è®­ç»ƒï¼ŒåŽ»ä¸­å¿ƒåŒ–æ‰§è¡Œ (CTDE) | å…¨å±€å®Œæ•´ä¿¡æ¯ | è®­ç»ƒæ•ˆçŽ‡é«˜ï¼Œæ‰§è¡Œç‹¬ç«‹ |
| **IDDPG** | å®Œå…¨ç‹¬ç«‹è®­ç»ƒ | å±€éƒ¨+åŽ†å²ä»·æ ¼ä¿¡æ¯ | å®Œå…¨åŽ»ä¸­å¿ƒåŒ–ï¼Œä¿¡æ¯æœ‰é™ |
| **MF-DDPG** | ç‹¬ç«‹è®­ç»ƒ + Mean Fieldè¿‘ä¼¼ | å±€éƒ¨+Mean Fieldä¿¡æ¯ | ä¿¡æ¯åŽ‹ç¼©çš„ç‹¬ç«‹å­¦ä¹  |

**é€‰æ‹©çº¯ç²¹ç‹¬ç«‹è®­ç»ƒçš„ç†ç”±**ï¼š
1. âœ… ç¬¦åˆMean Fieldç†è®ºçš„ç²¾ç¥žï¼šé€šè¿‡ç»Ÿè®¡å¹³å‡è¿‘ä¼¼ç¾¤ä½“è¡Œä¸º
2. âœ… ä¿æŒè®­ç»ƒèŒƒå¼çš„ä¸€è‡´æ€§ï¼šè®­ç»ƒå’Œæ‰§è¡Œéƒ½åŸºäºŽç›¸åŒä¿¡æ¯
3. âœ… éªŒè¯ä¿¡æ¯åŽ‹ç¼©æŠ€æœ¯ï¼šè¯„ä¼°Mean Fieldè¿‘ä¼¼åœ¨å°è§„æ¨¡åœºæ™¯çš„æ•ˆæžœ
4. âœ… å½¢æˆå®Œæ•´å¯¹æ¯”åºåˆ—ï¼šCTDE â†” çº¯ç‹¬ç«‹ â†” Mean Fieldç‹¬ç«‹

### 3.3 çŠ¶æ€ç©ºé—´è®¾è®¡

#### 3.3.1 æ ¸å¿ƒè®¾è®¡å†³ç­–ï¼šåŸºäºŽMean Fieldçš„çŠ¶æ€åŽ‹ç¼©

**è®¾è®¡å“²å­¦**ï¼šæ¯ä¸ªagentä»…åŸºäºŽè‡ªèº«åŽ†å²å’Œå…¶ä»–agentçš„å¹³å‡åœºä¿¡æ¯è¿›è¡Œå†³ç­–ï¼Œä¸ä½¿ç”¨ä»»ä½•"ç‰¹æƒ"ä¿¡æ¯ã€‚

#### 3.3.2 Actorç½‘ç»œçŠ¶æ€è®¾è®¡ï¼ˆ24ç»´ï¼‰

```python
# MF-DDPG ActorçŠ¶æ€ç»„æˆ
actor_state = [
    own_last_prices,      # 8ç»´ï¼šè‡ªèº«ä¸Šè½®ä»·æ ¼ï¼ˆå†³ç­–åŽ†å²ï¼‰
    own_last_flow,        # 8ç»´ï¼šè‡ªèº«ä¸Šè½®å……ç”µæµé‡ï¼ˆå¸‚åœºåé¦ˆï¼‰
    mean_field_prices     # 8ç»´ï¼šå…¶ä»–agentä»·æ ¼çš„å¹³å‡å€¼ï¼ˆç«žäº‰çŽ¯å¢ƒï¼‰
]
# æ€»ç»´åº¦ï¼š24ç»´
```

**çŠ¶æ€è®¡ç®—é€»è¾‘**ï¼š
```python
def compute_mf_actor_state(agent_id, observation):
    # 1. æå–è‡ªèº«åŽ†å²ä¿¡æ¯
    own_last_prices = observation["last_round_all_prices"][agent_id]  # 8ç»´
    own_last_flow = observation["own_charging_flow"]                  # 8ç»´

    # 2. è®¡ç®—Mean FieldçŠ¶æ€
    other_agent_prices = []
    for aid in all_agents:
        if aid != agent_id:
            other_agent_prices.append(observation["last_round_all_prices"][aid])

    mean_field_prices = np.mean(other_agent_prices, axis=0)  # 8ç»´

    # 3. ç»„åˆçŠ¶æ€
    return np.concatenate([own_last_prices, own_last_flow, mean_field_prices])
```

#### 3.3.3 Criticç½‘ç»œçŠ¶æ€è®¾è®¡ï¼ˆ32ç»´ï¼‰

```python
# MF-DDPG CriticçŠ¶æ€ç»„æˆ
critic_state = [
    actor_state,          # 24ç»´ï¼šActorçŠ¶æ€ï¼ˆå¦‚ä¸Šæ‰€è¿°ï¼‰
    current_action        # 8ç»´ï¼šå½“å‰è½®ä»·æ ¼å†³ç­–
]
# æ€»ç»´åº¦ï¼š32ç»´
```

#### 3.3.4 ä¸Žå…¶ä»–ç®—æ³•çš„çŠ¶æ€ç»´åº¦å¯¹æ¯”

| ç½‘ç»œç±»åž‹ | MADDPG | IDDPG | MF-DDPG | ä¿¡æ¯ç‰¹å¾ |
|----------|--------|-------|---------|----------|
| **Actorè¾“å…¥** | 40ç»´ | 40ç»´ | 24ç»´ | MFæœ€å°ï¼Œä¿¡æ¯æœ€åŽ‹ç¼© |
| **Criticè¾“å…¥** | 96ç»´ | 48ç»´ | 32ç»´ | MFæœ€å°ï¼Œè®¡ç®—æœ€é«˜æ•ˆ |
| **ä¿¡æ¯èŽ·å–** | å…¨å±€å®Œæ•´ | åŽ†å²ä»·æ ¼ | Mean Fieldè¿‘ä¼¼ | MFå¹³è¡¡ä¿¡æ¯ä¸Žæ•ˆçŽ‡ |

### 3.4 ç½‘ç»œæž¶æž„è®¾è®¡

#### 3.4.1 è¶…å‚æ•°å¯¹é½åŽŸåˆ™

**æ ¸å¿ƒåŽŸåˆ™**ï¼šé™¤è¾“å…¥ç»´åº¦å¤–ï¼Œæ‰€æœ‰è¶…å‚æ•°ä¸ŽMADDPGä¿æŒå®Œå…¨ä¸€è‡´ï¼Œç¡®ä¿å¯¹æ¯”å…¬å¹³æ€§ã€‚

```python
MF_DDPG_CONFIG = {
    # ä¸ŽMADDPGå¯¹é½çš„åŸºç¡€å‚æ•°
    'actor_lr': 0.001,                   # å­¦ä¹ çŽ‡å¯¹é½
    'critic_lr': 0.001,
    'gamma': 0.95,                       # æŠ˜æ‰£å› å­å¯¹é½
    'tau': 0.01,                         # è½¯æ›´æ–°ç³»æ•°å¯¹é½
    'buffer_capacity': 10000,            # ç¼“å†²åŒºå®¹é‡å¯¹é½
    'batch_size': 64,                    # æ‰¹æ¬¡å¤§å°å¯¹é½

    # æŽ¢ç´¢ç­–ç•¥å‚æ•°å¯¹é½
    'noise_sigma': 0.2,
    'noise_decay': 0.9995,
    'noise_min_sigma': 0.01,

    # ç½‘ç»œç»“æž„ï¼ˆä¿æŒéšè—å±‚ä¸€è‡´ï¼‰
    'actor_hidden_sizes': (64, 64),      # ä¸ŽMADDPGä¸€è‡´
    'critic_hidden_sizes': (128, 64),    # ä¸ŽMADDPGä¸€è‡´

    # æ”¶æ•›æ ‡å‡†å¯¹é½
    'convergence_threshold': 0.01,
    'stable_steps_required': 5,
    'stable_episodes_required': 3,

    # å¥–åŠ±å½’ä¸€åŒ–
    'reward_normalization': 'max_affine'  # åšå¼ˆç‰¹å®šå½’ä¸€åŒ–ä¸€è‡´
}
```

#### 3.4.2 ç½‘ç»œæž¶æž„å…·ä½“è®¾è®¡

**Actorç½‘ç»œ**ï¼š
- è¾“å…¥ç»´åº¦ï¼š24ç»´ï¼ˆè‡ªèº«åŽ†å² + Mean Fieldï¼‰
- éšè—å±‚ï¼š(64, 64)ï¼Œä¸ŽMADDPGä¿æŒä¸€è‡´
- è¾“å‡ºç»´åº¦ï¼š8ç»´ï¼ŒSigmoidæ¿€æ´»è¾“å‡º[0,1]å½’ä¸€åŒ–ä»·æ ¼
- æ¿€æ´»å‡½æ•°ï¼šéšè—å±‚ReLUï¼Œè¾“å‡ºå±‚Sigmoid

**Criticç½‘ç»œ**ï¼š
- è¾“å…¥ç»´åº¦ï¼š32ç»´ï¼ˆ24ç»´çŠ¶æ€ + 8ç»´åŠ¨ä½œï¼‰
- éšè—å±‚ï¼š(128, 64)ï¼Œä¸ŽMADDPGä¿æŒä¸€è‡´
- è¾“å‡ºç»´åº¦ï¼š1ç»´Qå€¼
- æ¿€æ´»å‡½æ•°ï¼šéšè—å±‚ReLUï¼Œè¾“å‡ºå±‚æ— æ¿€æ´»

### 3.5 è®­ç»ƒæœºåˆ¶è®¾è®¡

#### 3.5.1 ç‹¬ç«‹ç»éªŒå›žæ”¾ç­–ç•¥

**æ ¸å¿ƒå†³ç­–**ï¼šæ¯ä¸ªagentç»´æŠ¤ç‹¬ç«‹çš„ReplayBufferï¼Œå®Œå…¨ç¬¦åˆç‹¬ç«‹è®­ç»ƒèŒƒå¼ã€‚

```python
# ç»éªŒå­˜å‚¨ç»“æž„
experience = (
    local_state,           # 24ç»´ï¼šMF-DDPGçŠ¶æ€
    action,               # 8ç»´ï¼šä»·æ ¼åŠ¨ä½œ
    reward,               # 1ç»´ï¼šå½’ä¸€åŒ–å¥–åŠ±
    next_local_state,     # 24ç»´ï¼šä¸‹ä¸€çŠ¶æ€
    done                  # 1ç»´ï¼šç»“æŸæ ‡å¿—
)
```

#### 3.5.2 ç›®æ ‡Qå€¼è®¡ç®—æœºåˆ¶

**é‡‡ç”¨ç®€åŒ–æ–¹æ¡ˆ**ï¼šä½¿ç”¨å­˜å‚¨ç»éªŒæ—¶çš„Mean FieldçŠ¶æ€ï¼Œä¸é‡æ–°è®¡ç®—target Mean Fieldã€‚

**ç†ç”±åˆ†æž**ï¼š
1. âœ… **ä¿æŒç‹¬ç«‹æ€§**ï¼šé¿å…éœ€è¦å…¶ä»–agentçš„target policyä¿¡æ¯
2. âœ… **å®žçŽ°ç®€æ´**ï¼šä¸Žè®ºæ–‡å®žè·µä¿æŒä¸€è‡´
3. âœ… **ç†è®ºåˆç†**ï¼šMean Fieldæœ¬èº«å°±æ˜¯è¿‘ä¼¼æ–¹æ³•
4. âœ… **è®¡ç®—é«˜æ•ˆ**ï¼šé¿å…å¤æ‚çš„targetçŠ¶æ€é‡è®¡ç®—

#### 3.5.3 Mean Fieldæ›´æ–°é¢‘çŽ‡

**è®¾è®¡å†³ç­–**ï¼šæ¯ä¸ªstepéƒ½é‡æ–°è®¡ç®—Mean FieldçŠ¶æ€ï¼Œä¸Žè®ºæ–‡ä¿æŒä¸€è‡´ã€‚

**æ›´æ–°æœºåˆ¶**ï¼š
- çŽ¯å¢ƒstepå®ŒæˆåŽï¼Œç«‹å³åŸºäºŽæ–°è§‚æµ‹é‡æ–°è®¡ç®—Mean Field
- ä¸ä½¿ç”¨ç¼“å­˜æˆ–æ»žåŽçš„Mean Fieldä¿¡æ¯
- ç¡®ä¿Mean Fieldä¿¡æ¯çš„å®žæ—¶æ€§å’Œå‡†ç¡®æ€§

### 3.6 æ”¶æ•›æ£€æµ‹ä¸Žè¯„ä¼°

#### 3.6.1 æ”¶æ•›æ ‡å‡†ï¼ˆå®Œå…¨å¯¹é½ï¼‰

ä¸ŽMADDPGå’ŒIDDPGä½¿ç”¨ç›¸åŒçš„æ”¶æ•›æ£€æµ‹æœºåˆ¶ï¼š
- ä»·æ ¼ç›¸å¯¹å˜åŒ–çŽ‡é˜ˆå€¼ï¼š1%
- å•episodeå†…è¿žç»­æ”¶æ•›æ­¥æ•°ï¼š5æ­¥
- è®­ç»ƒç»ˆæ­¢è¿žç»­æ”¶æ•›episodesï¼š3è½®

#### 3.6.2 å¯¹æ¯”è¯„ä¼°æŒ‡æ ‡

**ä¸»è¦è¯„ä¼°ç»´åº¦**ï¼š
1. **æ”¶æ•›æ•ˆçŽ‡**ï¼šçŽ¯å¢ƒè¯„ä¼°æ¬¡æ•° vs ç­–ç•¥å˜åŒ–çŽ‡
2. **å‡è¡¡è´¨é‡**ï¼šæœ€ç»ˆçº³ä»€å‡è¡¡çš„ä»·æ ¼ç­–ç•¥å’Œæ”¶ç›Šåˆ†é…
3. **è®¡ç®—æ€§èƒ½**ï¼šè®­ç»ƒæ—¶é—´ã€å†…å­˜å ç”¨ã€ç½‘ç»œå‚æ•°é‡
4. **ç®—æ³•ç¨³å®šæ€§**ï¼šæ”¶æ•›ä¸€è‡´æ€§ã€æ”¶ç›Šæ³¢åŠ¨æ€§

### 3.7 ä¸ŽçŽ°æœ‰çŽ¯å¢ƒçš„é›†æˆ

#### 3.7.1 çŽ¯å¢ƒæŽ¥å£å…¼å®¹æ€§

**å®Œå…¨å…¼å®¹çŽ°æœ‰çŽ¯å¢ƒ**ï¼š
- âœ… ç›´æŽ¥ä½¿ç”¨EVCSChargingGameEnv
- âœ… å¤ç”¨çŽ°æœ‰è§‚æµ‹ç©ºé—´ç»“æž„
- âœ… ä½¿ç”¨ç›¸åŒçš„åŠ¨ä½œç©ºé—´å®šä¹‰
- âœ… å¤ç”¨åšå¼ˆç‰¹å®šå¥–åŠ±å½’ä¸€åŒ–

#### 3.7.2 è®­ç»ƒæ¡†æž¶å¤ç”¨

**é‡‡ç”¨ä¸‰å±‚è®­ç»ƒæž¶æž„**ï¼š
- **Episodeå±‚**ï¼šåšå¼ˆæ±‚è§£å°è¯•ï¼ˆä¸ŽMADDPGä¸€è‡´ï¼‰
- **Stepå±‚**ï¼šç­–ç•¥è°ƒæ•´å¾ªçŽ¯ï¼ˆä¸ŽMADDPGä¸€è‡´ï¼‰
- **UE-DTAå±‚**ï¼šçŽ¯å¢ƒå“åº”ï¼ˆä¸ŽMADDPGä¸€è‡´ï¼‰

### 3.8 å®žçŽ°æž¶æž„è®¾è®¡

#### 3.8.1 æ–‡ä»¶ç»“æž„ï¼ˆå®Œå…¨ç‹¬ç«‹å®žçŽ°ï¼‰

```
src/algorithms/mfddpg/
â”œâ”€â”€ mfddpg.py      # MFDDPGAgent, MFDDPGç±»ï¼Œç‹¬ç«‹å®žçŽ°
â””â”€â”€ networks.py    # ActorNetwork, CriticNetworkç±»ï¼Œå¤åˆ¶å®žçŽ°

src/trainer/
â””â”€â”€ MFDDPGTrainer.py  # ç‹¬ç«‹è®­ç»ƒå™¨ï¼Œå‚è€ƒMADDPGTraineræž¶æž„
```

**å®žçŽ°åŽŸåˆ™**ï¼š
- âŒ ä¸ä»Žå…¶ä»–ç®—æ³•æ¨¡å—å¯¼å…¥ä»£ç 
- âœ… å®Œæ•´å¤åˆ¶å¿…è¦çš„å·¥å…·ç±»ï¼ˆReplayBuffer, GaussianNoiseç­‰ï¼‰
- âœ… å®žçŽ°MF-DDPGä¸“å±žé€»è¾‘ï¼ˆMean FieldçŠ¶æ€è®¡ç®—ï¼‰
- âœ… ä¿æŒç®—æ³•æ¨¡å—çš„å®Œå…¨ç‹¬ç«‹æ€§

#### 3.8.2 å…³é”®å‡½æ•°è®¾è®¡

**Mean FieldçŠ¶æ€è®¡ç®—å‡½æ•°**ï¼š
```python
def compute_mean_field_state(agent_id, observations):
    """è®¡ç®—å•ä¸ªagentçš„Mean FieldçŠ¶æ€"""

def process_mf_observations(agent_id, observation):
    """å¤„ç†MF-DDPGçš„è§‚æµ‹æ•°æ®ä¸ºç½‘ç»œè¾“å…¥"""

def organize_mf_critic_state(actor_state, action):
    """ç»„ç»‡MF-DDPGçš„Criticç½‘ç»œè¾“å…¥"""
```

### 3.9 é¢„æœŸå®žéªŒæ•ˆæžœåˆ†æž

#### 3.9.1 ç†è®ºé¢„æœŸ

**è®¡ç®—æ•ˆçŽ‡ä¼˜åŠ¿**ï¼š
- âœ… æœ€å°çš„ç½‘ç»œè§„æ¨¡ï¼šå‚æ•°é‡çº¦ä¸ºMADDPGçš„1/3
- âœ… æœ€é«˜çš„è®­ç»ƒé€Ÿåº¦ï¼šç½‘ç»œè®¡ç®—å¼€é”€æœ€å°
- âœ… æœ€ä½³çš„æ‰©å±•æ€§ï¼šä¸å—agentæ•°é‡å½±å“

**æ”¶æ•›æ€§èƒ½é¢„æœŸ**ï¼š
- âš ï¸ å¯èƒ½æ¯”MADDPGæ”¶æ•›ç¨æ…¢ï¼šä¿¡æ¯åŽ‹ç¼©å¯¼è‡´çš„å­¦ä¹ æ•ˆçŽ‡æŸå¤±
- âš ï¸ åœ¨å°è§„æ¨¡åœºæ™¯ä¼˜åŠ¿ä¸æ˜Žæ˜¾ï¼šMean Fieldè¿‘ä¼¼çš„ç†è®ºä¼˜åŠ¿éœ€è¦å¤§è§„æ¨¡éªŒè¯
- âš ï¸ å¯èƒ½æ‰¾åˆ°ä¸åŒçš„å‡è¡¡è§£ï¼šä¿¡æ¯åŽ‹ç¼©å¯èƒ½å½±å“è§£çš„è´¨é‡

#### 3.9.2 å¯¹æ¯”å®žéªŒæ„ä¹‰

**ç®—æ³•è°±ç³»éªŒè¯**ï¼š
```
MADDPG (å…¨å±€ä¿¡æ¯) â†” IDDPG (åŽ†å²ä¿¡æ¯) â†” MF-DDPG (åŽ‹ç¼©ä¿¡æ¯) â†” BR-PSO (æ•°å­¦ä¼˜åŒ–)
```

**éªŒè¯å…³é”®é—®é¢˜**ï¼š
1. ä¸­å¿ƒåŒ–è®­ç»ƒçš„ä»·å€¼é‡åŒ–ï¼ˆMADDPG vs å…¶ä»–ï¼‰
2. ä¿¡æ¯åŽ‹ç¼©æŠ€æœ¯çš„æ•ˆæžœï¼ˆMF-DDPG vs IDDPGï¼‰
3. ç½‘ç»œè§„æ¨¡å¯¹æ±‚è§£æ•ˆçŽ‡çš„å½±å“
4. ä¸åŒè®­ç»ƒèŒƒå¼åœ¨å°è§„æ¨¡åšå¼ˆä¸­çš„é€‚ç”¨æ€§

### 3.10 å®žçŽ°ä¼˜å…ˆçº§ä¸Žè®¡åˆ’

**å®žçŽ°é˜¶æ®µ**ï¼š
1. **é˜¶æ®µ1**ï¼šæ ¸å¿ƒç®—æ³•å®žçŽ°ï¼ˆMFçŠ¶æ€è®¡ç®—ã€ç½‘ç»œæž¶æž„ã€è®­ç»ƒé€»è¾‘ï¼‰
2. **é˜¶æ®µ2**ï¼šè®­ç»ƒå™¨å®žçŽ°ï¼ˆå‚è€ƒMADDPGTrainerï¼Œä¿æŒä¸‰å±‚æž¶æž„ï¼‰
3. **é˜¶æ®µ3**ï¼šå¯¹æ¯”å®žéªŒï¼ˆä¸ŽMADDPGã€IDDPGã€BR-PSOå…¨é¢å¯¹æ¯”ï¼‰
4. **é˜¶æ®µ4**ï¼šç»“æžœåˆ†æžï¼ˆæ”¶æ•›æ›²çº¿ã€å‡è¡¡è´¨é‡ã€è®¡ç®—æ•ˆçŽ‡åˆ†æžï¼‰

### 3.11 ä¸Žå…¶ä»–å¯¹æ¯”ç®—æ³•çš„å…³ç³»

| ç®—æ³• | ç±»åž‹ | ä¿¡æ¯ç»´åº¦ | è®¡ç®—æˆæœ¬ | ç†è®ºæ”¶æ•›æ€§ | å®žçŽ°å¤æ‚åº¦ |
|------|------|---------|---------|-----------|-----------|
| MADDPG | MADRL-CTDE | é«˜ (96ç»´) | ä¸­ç­‰ | å¥½ | ä¸­ç­‰ |
| IDDPG | MADRL-ç‹¬ç«‹ | ä¸­ (48ç»´) | ä¸­ç­‰ | è¾ƒå·® | ç®€å• |
| **MF-DDPG** | MADRL-MF | ä½Ž (32ç»´) | æœ€ä½Ž | ä¸­ç­‰ | ç®€å• |
| BR-PSO | ç»å…¸åšå¼ˆè®º | - | æœ€é«˜ | å¥½ | ä¸­ç­‰ |

**å¯¹æ¯”æ„ä¹‰**ï¼š
- MF-DDPG vs MADDPGï¼šéªŒè¯ä¿¡æ¯åŽ‹ç¼©å¯¹æ€§èƒ½çš„å½±å“
- MF-DDPG vs IDDPGï¼šå¯¹æ¯”ä¸åŒä¿¡æ¯åˆ©ç”¨ç­–ç•¥çš„æ•ˆæžœ
- MF-DDPG vs BR-PSOï¼šè¯„ä¼°MADRLä¸Žç»å…¸æ–¹æ³•çš„æ•ˆçŽ‡å·®å¼‚
- å››ç®—æ³•ç»¼åˆå¯¹æ¯”ï¼šæž„å»ºä»Žé«˜ä¿¡æ¯åˆ°ä½Žè®¡ç®—æˆæœ¬çš„å®Œæ•´ç®—æ³•è°±ç³»
