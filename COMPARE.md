# COMPARE.md

æœ¬æ–‡æ¡£è®°å½•ä¸ŽçŽ°æœ‰MADDPGç®—æ³•è¿›è¡Œå¯¹æ¯”çš„åŸºå‡†ç®—æ³•ï¼ˆBaseline Algorithmsï¼‰çš„è®¾è®¡ä¸Žå®žçŽ°æ–¹æ¡ˆã€‚

## æ–‡æ¡£ç›®æ ‡

- è®°å½•å¯¹æ¯”ç®—æ³•çš„ç†è®ºèƒŒæ™¯å’ŒæŠ€æœ¯ç»†èŠ‚
- æ˜Žç¡®ç®—æ³•é€‚é…åˆ°å½“å‰é¡¹ç›®çš„è®¾è®¡æ–¹æ¡ˆ
- è·Ÿè¸ªå®žçŽ°è¿›åº¦å’Œé‡åˆ°çš„é—®é¢˜
- ç¡®ä¿å¯¹æ¯”å®žéªŒçš„å…¬å¹³æ€§å’Œç§‘å­¦æ€§

---

## å¯¹æ¯”ç®—æ³•åˆ—è¡¨

### 1. Best Response Dynamic + PSO (BR-PSO)

#### 1.1 ç®—æ³•æ¥æº
**è®ºæ–‡**: "Real-Time Charging Navigation of Electric Vehicles to Fast Charging Stations: A Hierarchical Game Approach" (IEEE TSG, 2017)

**æ ¸å¿ƒæ€æƒ³**: ä½¿ç”¨PSOç®—æ³•æ±‚è§£æ¯ä¸ªå……ç”µç«™çš„Best Responseç­–ç•¥ï¼Œé€šè¿‡è¿­ä»£æ›´æ–°ç›´åˆ°è¾¾åˆ°è¿‘ä¼¼çº³ä»€å‡è¡¡ã€‚

#### 1.2 çŽ¯å¢ƒæŽ¥å£é€‚é…

**âœ… å¯ä»¥ç›´æŽ¥ä½¿ç”¨çŽ°æœ‰çŽ¯å¢ƒ**ï¼š
- è¾“å…¥ï¼š`actions = {agent_id: np.array([prices])}`
- è¾“å‡ºï¼š`rewards = {agent_id: revenue}`
- çŽ¯å¢ƒå†…éƒ¨è‡ªåŠ¨å¤„ç†UE-DTAä»¿çœŸ

**PSOç²’å­è¯„ä¼°**ï¼š
```python
def evaluate_particle(prices, agent_id, fixed_other_prices):
    actions = fixed_other_prices.copy()
    actions[agent_id] = prices  # å½“å‰ç²’å­ä»£è¡¨çš„ä»·æ ¼ç­–ç•¥
    _, rewards, _, _, _ = env.step(actions)
    return rewards[agent_id]
```

#### 1.3 çŽ¯å¢ƒçŠ¶æ€ç®¡ç†æ–¹æ¡ˆ

**æ ¸å¿ƒå†³ç­–ï¼šBest Responseè¿­ä»£å±‚é¢resetï¼Œç²’å­è¯„ä¼°å±‚é¢soft_reset**

**æœºåˆ¶è®¾è®¡**ï¼š
```python
def find_best_response(agent_id):
    # ðŸ”‘ æ¯è½®Best Responseå¼€å§‹æ—¶ï¼šå®Œå…¨reset
    env.reset()  # åˆå§‹åŒ–è·¯å¾„åˆ†é…ï¼ˆè´ªå¿ƒç­–ç•¥ï¼‰

    def objective_function(prices):
        # ðŸ”‘ PSOç²’å­è¯„ä¼°æ—¶ï¼šsoft_reset
        env.soft_reset()  # ä¿ç•™è·¯å¾„åˆ†é…ï¼Œæ¸…é™¤çŠ¶æ€å˜é‡

        actions = fixed_strategies.copy()
        actions[agent_id] = prices
        _, rewards, _, _, _ = env.step(actions)
        return -rewards[agent_id]

    # PSOä¼˜åŒ–ï¼ˆå¤šä¸ªç²’å­Ã—å¤šæ¬¡è¿­ä»£ï¼‰
    best_prices = pso.optimize(objective_function)
    return best_prices
```

**`soft_reset()` æ–¹æ³•è®¾è®¡**ï¼š
- æ¸…é™¤çŠ¶æ€å˜é‡ï¼š`current_step`, `price_history`, `convergence_counter`
- **ä¿ç•™è·¯å¾„åˆ†é…**ï¼š`current_routes_specified`ï¼ˆå…³é”®ï¼ï¼‰
- ç›®çš„ï¼šåŠ é€ŸUE-DTAæ”¶æ•›ï¼ˆä»ŽæŽ¥è¿‘å‡è¡¡çš„è·¯å¾„å¼€å§‹ï¼‰

**ç†ç”±åˆ†æž**ï¼š

1. **ä¸ºä»€ä¹ˆBRè¿­ä»£å±‚é¢éœ€è¦resetï¼Ÿ**
   - æ¯ä¸ªå……ç”µç«™çš„Best Responseæ±‚è§£åº”è¯¥ä»Žç›¸åŒåˆå§‹æ¡ä»¶å¼€å§‹
   - ç¡®ä¿ä¸åŒå……ç”µç«™ä¹‹é—´çš„å…¬å¹³æ€§

2. **ä¸ºä»€ä¹ˆç²’å­è¯„ä¼°å±‚é¢å¯ä»¥soft_resetï¼Ÿ**
   - UE-DTAçš„éšæœºæ€§æ˜¯**å›ºæœ‰çš„**ï¼ˆè·¯å¾„åˆ‡æ¢æ¦‚çŽ‡æœºåˆ¶ï¼‰
   - ä¸åŒä»·æ ¼æœ¬æ¥å°±åº”è¯¥å¯¼è‡´ä¸åŒçš„å‡è¡¡çŠ¶æ€
   - PSOèƒ½å¤Ÿå®¹å¿è¯„ä¼°å™ªéŸ³ï¼ˆç¾¤ä½“æ™ºèƒ½çš„é²æ£’æ€§ï¼‰
   - å¤§å¹…åŠ é€Ÿè®¡ç®—ï¼ˆ3-5å€ï¼‰ï¼šUE-DTAä»ŽæŽ¥è¿‘å‡è¡¡çš„è·¯å¾„å¼€å§‹

3. **UE-DTAéšæœºæ€§çš„åˆç†æ€§**ï¼š
   ```python
   # è·¯å¾„åˆ‡æ¢æœºåˆ¶ï¼ˆä»£ç ä¸­çš„è®¾è®¡ï¼‰
   if cost_gap > 0 and np.random.random() < ue_swap_probability:
       new_routes = best_route
   ```
   - æ¨¡æ‹ŸçœŸå®žç”¨æˆ·é€‰æ‹©çš„éšæœºæ€§
   - é¿å…ç³»ç»Ÿéœ‡è¡ï¼ŒæŽ¢ç´¢è·¯å¾„ç©ºé—´
   - ç›¸åŒä»·æ ¼è¾“å…¥å¯èƒ½äº§ç”Ÿç•¥å¾®ä¸åŒçš„å‡è¡¡ï¼ˆå¯æŽ¥å—ï¼‰

4. **PSOå¤„ç†å™ªéŸ³çš„èƒ½åŠ›**ï¼š
   - ç¾¤ä½“æŽ¢ç´¢ï¼š20ä¸ªç²’å­çš„é›†ä½“æ™ºèƒ½å¹³æ»‘å™ªéŸ³
   - è®°å¿†æœºåˆ¶ï¼špBestå’ŒgBestæä¾›ç¨³å®šæ–¹å‘
   - å¤šæ¬¡è¯„ä¼°ï¼šç›¸ä¼¼åŒºåŸŸä¼šè¢«å¤šæ¬¡æŽ¢ç´¢

**é¢„æœŸæ•ˆæžœ**ï¼š
- è®¡ç®—åŠ é€Ÿï¼šæ¯æ¬¡UE-DTAä»Ž10æ¬¡è¿­ä»£é™åˆ°3-5æ¬¡è¿­ä»£
- ç»“æžœè´¨é‡ï¼šPSOèƒ½åœ¨å™ªéŸ³çŽ¯å¢ƒä¸‹æ”¶æ•›åˆ°åˆç†çš„è¿‘ä¼¼çº³ä»€å‡è¡¡
- å¯æŽ¥å—ä»£ä»·ï¼šè¯„ä¼°å­˜åœ¨Â±5-10%çš„å™ªéŸ³ï¼ˆéœ€å®žéªŒéªŒè¯ï¼‰

#### 1.4 Best Responseè¿­ä»£æž¶æž„

**æ ¸å¿ƒå†³ç­–ï¼šå¹¶è¡Œæ›´æ–°ï¼ˆJacobié£Žæ ¼ï¼‰ï¼Œä¸ŽMADDPGä¿æŒä¸€è‡´**

**åŽŸè®ºæ–‡å®žçŽ°æ–¹å¼**ï¼š
- å¤–å±‚BRè¿­ä»£ï¼š~100è½®ï¼ˆä»ŽFigure 11å¯è§ï¼‰
- å†…å±‚æ›´æ–°æ–¹å¼ï¼šä¸²è¡Œæ›´æ–°ï¼ˆGauss-Seidelé£Žæ ¼ï¼‰
- PSOè¿­ä»£æ¬¡æ•°ï¼šæœªæ˜Žç¡®è¯´æ˜Ž

**æˆ‘ä»¬çš„å®žçŽ°æ–¹æ¡ˆï¼šå¹¶è¡Œæ›´æ–° + è®¡ç®—å¹¶è¡ŒåŒ–**

```python
class ParallelBRPSO:
    def __init__(self, network_dir, network_name, ...):
        # ðŸ”‘ æ¯ä¸ªagentç»´æŠ¤ä¸€ä¸ªç‹¬ç«‹çš„å®Œæ•´çŽ¯å¢ƒå¯¹è±¡
        self.agent_envs = {
            agent_id: EVCSChargingGameEnv(network_dir, network_name)
            for agent_id in agents
        }

for br_iteration in range(max_br_iterations):
    # ðŸ”‘ æ­¥éª¤1ï¼šä¿å­˜æœ¬è½®ç­–ç•¥å¿«ç…§ï¼ˆJacobiæ›´æ–°ï¼‰
    strategies_snapshot = {aid: current_strategies[aid].copy()
                          for aid in agents}

    # ðŸ”‘ æ­¥éª¤2ï¼šæ‰€æœ‰agentå¹¶è¡Œè®¡ç®—Best Responseï¼ˆè®¡ç®—å¹¶è¡Œï¼‰
    with ProcessPoolExecutor(max_workers=len(agents)) as executor:
        futures = {
            agent_id: executor.submit(
                solve_single_agent_best_response,
                agent_id,
                {aid: strategies_snapshot[aid] for aid in agents if aid != agent_id},
                agent_envs[agent_id],  # ä½¿ç”¨è¯¥agentçš„ç‹¬ç«‹çŽ¯å¢ƒ
                pso_config
            )
            for agent_id in agents
        }

        new_strategies = {aid: future.result() for aid, future in futures.items()}

    # ðŸ”‘ æ­¥éª¤3ï¼šåŒæ—¶æ›´æ–°æ‰€æœ‰agentç­–ç•¥
    current_strategies = new_strategies

    # æ­¥éª¤4ï¼šæ£€æŸ¥æ”¶æ•›
    if check_convergence(strategies_snapshot, new_strategies):
        break
```

**å¯¹æ¯”ï¼šä¸²è¡Œ vs å¹¶è¡Œæ›´æ–°**

| ç»´åº¦ | ä¸²è¡Œæ›´æ–°ï¼ˆGauss-Seidelï¼Œè®ºæ–‡æ–¹å¼ï¼‰ | å¹¶è¡Œæ›´æ–°ï¼ˆJacobiï¼Œæˆ‘ä»¬çš„æ–¹æ¡ˆï¼‰ |
|------|--------------------------------|----------------------------|
| **ä¿¡æ¯åŒæ­¥** | åŽé¢agentçœ‹åˆ°å‰é¢agentçš„æœ€æ–°ç­–ç•¥ | æ‰€æœ‰agentåŸºäºŽåŒä¸€è½®ç­–ç•¥ |
| **é¡ºåºä¾èµ–** | âŒ å­˜åœ¨ï¼ˆagent_1å’Œagent_4ä¿¡æ¯ä¸å¯¹ç§°ï¼‰ | âœ… æ— é¡ºåºåå·® |
| **åšå¼ˆå®šä¹‰** | âš ï¸ å¼•å…¥äº†åºè´¯ä¿¡æ¯ | âœ… ç¬¦åˆ"é™æ€åŒæ—¶åšå¼ˆ" |
| **ä¸ŽMADDPGä¸€è‡´æ€§** | âŒ ä¸ä¸€è‡´ | âœ… ä¸€è‡´ï¼ˆåŒæ­¥æ›´æ–°ï¼‰ |
| **è®¡ç®—æ—¶é—´** | æ— æ³•å¹¶è¡Œ | âœ… åŠ é€Ÿçº¦4å€ |
| **æ”¶æ•›é€Ÿåº¦** | ç†è®ºä¸Šå¯èƒ½ç¨å¿«1-2è½® | ç†è®ºä¸Šå¯èƒ½ç¨æ…¢1-2è½® |

**é€‰æ‹©å¹¶è¡Œæ›´æ–°çš„ç†ç”±**ï¼š
1. âœ… ç¬¦åˆé¡¹ç›®å®šä¹‰çš„"é™æ€åŒæ—¶åšå¼ˆ"
2. âœ… ä¸ŽMADDPGçš„åŒæ­¥æ›´æ–°é£Žæ ¼ä¸€è‡´ï¼Œä¾¿äºŽå…¬å¹³å¯¹æ¯”
3. âœ… æ— é¡ºåºåå·®ï¼Œæ‰€æœ‰agentä¿¡æ¯å¯¹ç§°
4. âœ… å®žçŽ°é€»è¾‘æ¸…æ™°ï¼Œæ˜“äºŽç†è§£å’Œè°ƒè¯•
5. âœ… **å¯ä»¥åˆ©ç”¨å¤šæ ¸CPUå¹¶è¡Œè®¡ç®—ï¼ŒåŠ é€Ÿçº¦4å€**

**å¹¶è¡ŒåŒ–å®žçŽ°çš„å…³é”®è¦ç‚¹**ï¼š

**1. æ¯ä¸ªagentç»´æŠ¤ç‹¬ç«‹çŽ¯å¢ƒå¯¹è±¡**
```python
# åˆå§‹åŒ–æ—¶åˆ›å»ºï¼Œæ•´ä¸ªBR-PSOè¿‡ç¨‹ä¸­æŒç»­ä½¿ç”¨
self.agent_envs = {
    agent_id: EVCSChargingGameEnv(network_dir, network_name)
    for agent_id in agents
}
```

**ä¼˜åŠ¿**ï¼š
- âœ… æ— çŽ¯å¢ƒæ‹·è´å¼€é”€ï¼ˆç›¸æ¯”åŠ¨æ€åˆ›å»ºå‰¯æœ¬ï¼‰
- âœ… çŠ¶æ€å¤©ç„¶éš”ç¦»ï¼ˆæ¯ä¸ªagentåœ¨è‡ªå·±çš„çŽ¯å¢ƒä¸­æ‰§è¡Œï¼‰
- âœ… å®žçŽ°ç®€å•ï¼ˆä¸éœ€è¦å¤æ‚çš„æ·±æ‹·è´é€»è¾‘ï¼‰
- âœ… è°ƒè¯•å‹å¥½ï¼ˆçŽ¯å¢ƒå¯¹è±¡ç”Ÿå‘½å‘¨æœŸæ¸…æ™°ï¼‰

**2. reset() vs soft_reset() çš„ä½¿ç”¨**

```python
def solve_single_agent_best_response(agent_id, other_strategies, env, pso_config):
    # æ¯è½®BRå¼€å§‹ï¼šå®Œå…¨reset
    env.reset()  # é‡æ–°åˆå§‹åŒ–è·¯å¾„åˆ†é…ï¼ˆè´ªå¿ƒç­–ç•¥ï¼‰

    def objective_function(prices):
        # æ¯ä¸ªç²’å­è¯„ä¼°ï¼šsoft_reset
        env.soft_reset()  # ä¿ç•™è·¯å¾„åˆ†é…ï¼Œæ¸…é™¤çŠ¶æ€å˜é‡

        actions = other_strategies.copy()
        actions[agent_id] = prices
        _, rewards, _, _, _ = env.step(actions)
        return -rewards[agent_id]

    # PSOä¼˜åŒ–
    best_prices = pso.optimize(objective_function)
    return best_prices
```

**`reset()` å®žçŽ°**ï¼š
```python
def reset(self):
    """å®Œå…¨é‡ç½®çŽ¯å¢ƒï¼ˆæ¯è½®BRå¼€å§‹æ—¶è°ƒç”¨ï¼‰"""
    self.current_step = 0
    self.price_history = []
    self.convergence_counter = 0
    self.current_routes_specified = self._initialize_routes_greedy()  # ðŸ”‘ é‡æ–°åˆå§‹åŒ–
    self._init_world()
    return self._get_observations()
```

**`soft_reset()` å®žçŽ°**ï¼š
```python
def soft_reset(self):
    """ä¿ç•™è·¯å¾„åˆ†é…ï¼Œé‡ç½®å…¶ä»–çŠ¶æ€ï¼ˆPSOç²’å­è¯„ä¼°æ—¶è°ƒç”¨ï¼‰"""
    self.current_step = 0
    self.price_history = []
    self.convergence_counter = 0
    # self.current_routes_specified ä¿æŒä¸å˜ ðŸ”‘ å…³é”®ï¼
    self._init_world()
```

**3. æ€§èƒ½æå‡åˆ†æž**

```python
# å‡è®¾å‚æ•°
n_br_iterations = 100
n_agents = 4
avg_pso_evals = 10_particles Ã— 20_iters = 200æ¬¡è¯„ä¼°/agent
t_ue_dta = 2s

# ä¸²è¡Œæ—¶é—´ï¼ˆè®ºæ–‡å¯èƒ½çš„æ–¹å¼ï¼‰
t_serial = 100 Ã— 4 Ã— 200 Ã— 2s = 160,000s â‰ˆ 44å°æ—¶

# å¹¶è¡Œæ—¶é—´ï¼ˆæˆ‘ä»¬çš„æ–¹æ¡ˆï¼‰
t_parallel = 100 Ã— 200 Ã— 2s = 40,000s â‰ˆ 11å°æ—¶

# ç†è®ºåŠ é€Ÿæ¯” = 4å€ï¼ˆ4ä¸ªagentï¼‰
# å®žé™…åŠ é€Ÿæ¯” â‰ˆ 3.5å€ï¼ˆè€ƒè™‘è¿›ç¨‹åˆ›å»ºå’Œåºåˆ—åŒ–å¼€é”€ï¼‰
```

**4. å®žçŽ°æ³¨æ„äº‹é¡¹**

- **çŽ¯å¢ƒåºåˆ—åŒ–**ï¼šEVCSChargingGameEnvéœ€è¦æ”¯æŒpickleï¼ˆProcessPoolExecutorè¦æ±‚ï¼‰
- **å†…å­˜å ç”¨**ï¼š4ä¸ªçŽ¯å¢ƒå¯¹è±¡å¸¸é©»å†…å­˜ï¼ˆé¢„è®¡æ¯ä¸ª100-500MBï¼Œæ€»è®¡~2GBï¼Œå¯æŽ¥å—ï¼‰
- **è°ƒè¯•ç­–ç•¥**ï¼šä¿ç•™ä¸²è¡Œç‰ˆæœ¬ç”¨äºŽè°ƒè¯•ï¼Œå¹¶è¡Œç‰ˆæœ¬ç”¨äºŽæœ€ç»ˆå®žéªŒ

#### 1.5 æ”¶æ•›åˆ¤åˆ«æ¡ä»¶

**ä¸¤å±‚æ”¶æ•›åˆ¤åˆ«è®¾è®¡**ï¼šBRå±‚ï¼ˆå¤–å±‚ï¼‰+ PSOå±‚ï¼ˆå†…å±‚ï¼‰

##### 1.5.1 BRå±‚æ”¶æ•›åˆ¤åˆ«

**è®¾è®¡å†³ç­–**ï¼šä¸ŽMADDPGä¿æŒå®Œå…¨ä¸€è‡´

```python
def check_br_convergence(old_strategies, new_strategies):
    """BRæ”¶æ•›åˆ¤æ–­ï¼ˆä¸ŽMADDPGä¸€è‡´ï¼‰"""
    relative_changes = []

    for agent_id in agents:
        old_prices = np.array(old_strategies[agent_id])
        new_prices = np.array(new_strategies[agent_id])

        # è®¡ç®—ç›¸å¯¹å˜åŒ–çŽ‡
        relative_change = np.abs(new_prices - old_prices) / (old_prices + 1e-8)
        avg_change = np.mean(relative_change)
        relative_changes.append(avg_change)

    avg_relative_change = np.mean(relative_changes)
    return avg_relative_change < convergence_threshold
```

**é…ç½®å‚æ•°**ï¼š
```python
BR_CONVERGENCE_CONFIG = {
    'convergence_threshold': 0.01,      # 1%å¹³å‡ç›¸å¯¹å˜åŒ–ï¼ˆä¸ŽMADDPGä¸€è‡´ï¼‰
    'stable_required': 5,               # è¿žç»­5è½®æ”¶æ•›åˆ™åœæ­¢ï¼ˆä¸ŽMADDPGä¸€è‡´ï¼‰
    'max_br_iterations': 100,           # æœ€å¤§BRè¿­ä»£æ•°
}
```

**ç†ç”±**ï¼š
- âœ… ä¸ŽMADDPGæ”¶æ•›æ ‡å‡†å®Œå…¨ä¸€è‡´ï¼Œç¡®ä¿å¯¹æ¯”å…¬å¹³æ€§
- âœ… åŸºäºŽä»·æ ¼ç›¸å¯¹å˜åŒ–çŽ‡ï¼Œç‰©ç†æ„ä¹‰æ¸…æ™°
- âœ… é€šè¿‡stable_requiredé¿å…å¶ç„¶æ³¢åŠ¨

##### 1.5.2 PSOå±‚æ”¶æ•›åˆ¤åˆ«

**åŽŸè®ºæ–‡æƒ…å†µ**ï¼š
- âŒ PSOåœæ­¢æ¡ä»¶æœªæ˜Žç¡®è¯´æ˜Ž
- âœ… BRå¤–å±‚è¿­ä»£çº¦100è½®ï¼ˆä»ŽFigure 11å¯è§ï¼‰

**è®¾è®¡å†³ç­–**ï¼šåˆšæ€§æœ€å¤§è¿­ä»£æ¬¡æ•° + gBeståœæ»ž

```python
class PSOConfig:
    """PSOæ”¶æ•›é…ç½®"""
    max_iterations: int = 20        # æœ€å¤§è¿­ä»£æ•°ï¼ˆå¯é…ç½®å‚æ•°ï¼‰
    min_iterations: int = 5         # æœ€å°è¿­ä»£æ•°ï¼ˆé¿å…è¿‡æ—©åœæ­¢ï¼‰
    patience: int = 5               # gBestæ— æ”¹è¿›å®¹å¿æ¬¡æ•°
    improvement_threshold: float = 0.01  # ç›¸å¯¹æ”¹è¿›é˜ˆå€¼ï¼ˆ1%ï¼‰

def check_pso_convergence(pso_state):
    """PSOæ”¶æ•›åˆ¤æ–­"""
    # æ¡ä»¶0ï¼šä¿è¯æœ€å°æŽ¢ç´¢
    if pso_state.iteration < min_iterations:
        return False

    # æ¡ä»¶1ï¼šè¾¾åˆ°æœ€å¤§è¿­ä»£ï¼ˆç¡¬æ€§åœæ­¢ï¼‰
    if pso_state.iteration >= max_iterations:
        return True

    # æ¡ä»¶2ï¼šgBeståœæ»žï¼ˆæ—©åœï¼‰
    if len(pso_state.gbest_history) >= patience:
        recent_gbest = pso_state.gbest_history[-patience:]
        improvement = abs(recent_gbest[-1] - recent_gbest[0])
        relative_improvement = improvement / (abs(recent_gbest[0]) + 1e-8)

        if relative_improvement < improvement_threshold:
            return True  # è¿žç»­patienceæ¬¡è¿­ä»£gBestæ”¹è¿›<1%

    return False
```

**åˆ¤æ®é€‰æ‹©ç†ç”±**ï¼š
1. âœ… **gBeståœæ»žæ˜¯PSOæœ€å¸¸ç”¨çš„æ”¶æ•›åˆ¤æ®**ï¼ˆæ–‡çŒ®å…±è¯†ï¼‰
2. âœ… **ä¸ŽBRå±‚é€»è¾‘ä¸€è‡´**ï¼šéƒ½åŸºäºŽ"è¿žç»­Næ­¥æ— æ˜¾è‘—æ”¹è¿›"
3. âœ… **è®¡ç®—ç®€å•**ï¼šæ— é¢å¤–å¼€é”€ï¼Œåªéœ€è®°å½•gBeståŽ†å²
4. âœ… **å¯¹å™ªéŸ³æœ‰å®¹å¿**ï¼šé€šè¿‡patienceå’Œthresholdå‚æ•°è°ƒèŠ‚
5. âœ… **é¿å…è¿‡æ—©/è¿‡æ™šåœæ­¢**ï¼šmin/max_iterationsæä¾›ä¿éšœ

**å‚æ•°è®¾ç½®å»ºè®®**ï¼ˆæ ¹æ®é¢„ç®—è°ƒæ•´ï¼‰ï¼š

| PSOé¢„ç®— | max_iterations | min_iterations | patience | threshold | è¯´æ˜Ž |
|---------|----------------|----------------|----------|-----------|------|
| å°ï¼ˆå¿«é€ŸæŽ¢ç´¢ï¼‰ | 10 | 3 | 3 | 0.02 | é€‚åˆåˆæ­¥æµ‹è¯• |
| ä¸­ï¼ˆå¹³è¡¡ï¼‰ | 20 | 5 | 5 | 0.01 | **æŽ¨èèµ·ç‚¹** |
| å¤§ï¼ˆå……åˆ†æŽ¢ç´¢ï¼‰ | 50 | 10 | 8 | 0.01 | è®¡ç®—é¢„ç®—å……è¶³æ—¶ |

**é¢„æœŸUE-DTAè°ƒç”¨æ¬¡æ•°**ï¼ˆä¸­ç­‰é¢„ç®—ï¼‰ï¼š
```python
# å•æ¬¡BRçš„PSOè¯„ä¼°æ¬¡æ•°
# æœ€åæƒ…å†µï¼š10 particles Ã— 20 iterations = 200æ¬¡
# Early stopé¢„æœŸï¼š10 particles Ã— 12 iterations = 120æ¬¡ï¼ˆå¹³å‡ï¼‰

# å®Œæ•´BR-PSOçš„UE-DTAè°ƒç”¨
# æœ€åï¼š100 BR Ã— 4 agents Ã— 200 = 80,000æ¬¡
# å¹¶è¡ŒåŽï¼š100 BR Ã— 200 = 20,000æ¬¡ï¼ˆå¢™ä¸Šæ—¶é’Ÿæ—¶é—´ï¼‰
# Early stopé¢„æœŸï¼š100 BR Ã— 120 = 12,000æ¬¡
```

**å¤‡é€‰åˆ¤æ®**ï¼ˆæš‚ä¸é‡‡ç”¨ï¼‰ï¼š
- **ç²’å­ç¾¤å¤šæ ·æ€§**ï¼šè®¡ç®—å¼€é”€å¤§ï¼Œé˜ˆå€¼éš¾è®¾å®š
- **é€‚åº”åº¦æ–¹å·®**ï¼šå¯èƒ½æ‰€æœ‰ç²’å­é™·å…¥åŒä¸€å±€éƒ¨æœ€ä¼˜
- **å¹³æ»‘å¤„ç†**ï¼šå¢žåŠ å¤æ‚åº¦ï¼Œç­‰å‘çŽ°å™ªéŸ³é—®é¢˜å†è€ƒè™‘

#### 1.6 å®žéªŒéªŒè¯è®¡åˆ’

**å®žéªŒ1ï¼šå™ªéŸ³æ°´å¹³æµ‹è¯•**
- å›ºå®šä»·æ ¼ç­–ç•¥ï¼Œè¯„ä¼°20æ¬¡ï¼Œè®¡ç®—å˜å¼‚ç³»æ•°CV
- ç›®æ ‡ï¼šCV < 10%ï¼ˆå¯æŽ¥å—èŒƒå›´ï¼‰

**å®žéªŒ2ï¼šreset vs soft_resetå¯¹æ¯”**
- åˆ†åˆ«è¿è¡Œä¸¤ç§ç­–ç•¥çš„BR-PSO
- å¯¹æ¯”æœ€ç»ˆçº³ä»€å‡è¡¡çš„ä»·æ ¼å·®å¼‚
- éªŒè¯soft_resetçš„å¯è¡Œæ€§

**å®žéªŒ3ï¼šè®¡ç®—æ•ˆçŽ‡å¯¹æ¯”**
- è®°å½•UE-DTAå¹³å‡è¿­ä»£æ¬¡æ•°
- è®°å½•æ€»è®¡ç®—æ—¶é—´
- é‡åŒ–åŠ é€Ÿæ•ˆæžœ

**å®žéªŒ4ï¼šPSOé¢„ç®—æ•æ„Ÿæ€§åˆ†æžï¼ˆæ–°å¢žï¼‰**

**ç›®æ ‡**ï¼šåœ¨æ€»UE-DTAè°ƒç”¨é¢„ç®—çº¦æŸä¸‹ï¼Œæ‰¾åˆ°PSOçš„æœ€ä¼˜é…ç½®

**æµ‹è¯•é…ç½®**ï¼š

| é…ç½®å | ç²’å­æ•° | PSOæœ€å¤§è¿­ä»£ | å•æ¬¡BRé¢„æœŸè°ƒç”¨ | è¯´æ˜Ž |
|--------|--------|------------|---------------|------|
| Ultra-Light | 5 | 10 | ~60 | æžé€Ÿæµ‹è¯• |
| Light | 10 | 20 | ~120 | æŽ¨èé…ç½® |
| Medium | 15 | 30 | ~200 | å¹³è¡¡é…ç½® |
| Heavy | 20 | 50 | ~400 | å……åˆ†æŽ¢ç´¢ |

**è¯„ä¼°æŒ‡æ ‡**ï¼š
1. æœ€ç»ˆçº³ä»€å‡è¡¡çš„ä»·æ ¼ç­–ç•¥
2. å„å……ç”µç«™çš„æ”¶ç›Š
3. å®žé™…BRæ”¶æ•›è½®æ•°
4. å®žé™…UE-DTAè°ƒç”¨æ€»æ¬¡æ•°
5. æ€»è®¡ç®—æ—¶é—´

**åˆ¤æ–­æ ‡å‡†**ï¼š
- å¦‚æžœUltra-Lightä¸ŽMediumçš„æœ€ç»ˆæ”¶ç›Šå·®å¼‚<3%ï¼Œé€‰æ‹©Ultra-Light
- å¦‚æžœé…ç½®Açš„è®¡ç®—æ—¶é—´æ˜¯é…ç½®Bçš„2å€ï¼Œä½†æ”¶ç›Šæ”¹è¿›<5%ï¼Œé€‰æ‹©é…ç½®B

#### 1.7 æ”¶æ•›æ›²çº¿å¯è§†åŒ–æ–¹æ¡ˆ

**ç›®æ ‡**ï¼šå¯¹æ¯”MADDPGå’ŒBR-PSOæ±‚è§£çº³ä»€å‡è¡¡è¿‡ç¨‹çš„æ”¶æ•›æ€§èƒ½

##### 1.7.1 æ¨ªè½´é€‰æ‹©

**ç»Ÿä¸€åŸºå‡†ï¼šç´¯ç§¯çŽ¯å¢ƒè¯„ä¼°æ¬¡æ•°**

```python
# MADDPG: æ¯ä¸ªstepå¯¹åº”1æ¬¡çŽ¯å¢ƒè¯„ä¼°
cumulative_evals_maddpg = step_number

# BR-PSO: ç´¯åŠ æ‰€æœ‰ç²’å­è¯„ä¼°
cumulative_evals_brpso = sum(n_particles Ã— n_pso_iters for each BR iteration)
```

**åŽŸå› **ï¼š
- âœ… å…¬å¹³å¯¹æ¯”è®¡ç®—æˆæœ¬ï¼ˆæ¯æ¬¡env.step()æˆæœ¬ç›¸åŒï¼‰
- âœ… ç¡¬ä»¶æ— å…³ï¼ˆä¸å—CPUæ ¸å¿ƒæ•°å½±å“ï¼‰
- âœ… å®žçŽ°æ— å…³ï¼ˆä¸å—ç®—æ³•å†…éƒ¨ç»“æž„å½±å“ï¼‰

##### 1.7.2 çºµè½´é€‰æ‹©

**ä¸»è¦æŒ‡æ ‡ï¼šç­–ç•¥å˜åŒ–çŽ‡ï¼ˆå¯¹æ•°åæ ‡ï¼‰**

```python
# æ‰€æœ‰å……ç”µç«™æ‰€æœ‰æ—¶æ®µçš„ä»·æ ¼ç›¸å¯¹å˜åŒ–çŽ‡
relative_change = |price_new[i,t] - price_old[i,t]| / (price_old[i,t] + 1e-8)
strategy_change_rate = mean(relative_change for all i,t)
```

**ç‰©ç†æ„ä¹‰**ï¼š
- è®­ç»ƒåˆæœŸï¼šå˜åŒ–çŽ‡é«˜ï¼ˆ0.1-0.5ï¼‰ï¼Œç­–ç•¥è°ƒæ•´å‰§çƒˆ
- æ”¶æ•›æ—¶ï¼šå˜åŒ–çŽ‡è¶‹è¿‘0ï¼ˆ<0.01ï¼‰ï¼Œç­–ç•¥ç¨³å®š

**MADDPGè®°å½•æ–¹å¼**ï¼š
- æ¯ä¸ªstepéƒ½è®¡ç®—å¹¶è®°å½•ï¼ˆè¿žç»­å¯†é›†æ›²çº¿ï¼‰

**BR-PSOè®°å½•æ–¹å¼**ï¼š
- **ä»…åœ¨BRè¿­ä»£å®Œæˆæ—¶è®°å½•**ï¼ˆç¦»æ•£ç¨€ç–ç‚¹/é˜¶è·ƒæ›²çº¿ï¼‰
- å•ä¸ªBRè¿­ä»£å†…çš„PSOæœç´¢ä¸è®°å½•ï¼ˆå› ä¸ºç­–ç•¥æœªçœŸæ­£æ›´æ–°ï¼‰

##### 1.7.3 å¯è§†åŒ–æŒ‘æˆ˜ä¸Žè§£å†³æ–¹æ¡ˆ

**æ ¸å¿ƒé—®é¢˜**ï¼šæ¨ªè½´èŒƒå›´å·®å¼‚å·¨å¤§
- MADDPGæ”¶æ•›ï¼š~400æ¬¡çŽ¯å¢ƒè¯„ä¼°
- BR-PSOæ”¶æ•›ï¼š~36,000æ¬¡çŽ¯å¢ƒè¯„ä¼°ï¼ˆ90å€å·®è·ï¼‰
- ç›´æŽ¥ç»˜åˆ¶ä¼šå¯¼è‡´MADDPGæ›²çº¿åŽ‹ç¼©åœ¨æœ€å·¦ä¾§10%åŒºåŸŸ

**æ–¹æ¡ˆ1ï¼šå¯¹æ•°æ¨ªè½´ï¼ˆæŽ¨è1ï¼‰**

```
ç­–ç•¥å˜åŒ–çŽ‡(log)
  â”‚
  â”‚     MADDPG          BR-PSO
  â”‚      åŒºåŸŸ            åŒºåŸŸ
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â”‚        â”‚      â”‚        â”‚
  â””â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â†’ çŽ¯å¢ƒè¯„ä¼°æ¬¡æ•°(log)
     10Â¹    10Â²     10Â³      10â´
```

**ä¼˜ç‚¹**ï¼š
- âœ… å•ä¸€è¿žç»­åæ ‡è½´
- âœ… ä¸¤ä¸ªç®—æ³•éƒ½æœ‰åˆç†çš„æ˜¾ç¤ºç©ºé—´
- âœ… è‡ªç„¶å¼ºè°ƒæ•°é‡çº§å·®å¼‚
- âœ… ç§‘ç ”è®ºæ–‡å¸¸ç”¨æ–¹å¼

**ç¼ºç‚¹**ï¼š
- âš ï¸ å¯¹æ•°åˆ»åº¦å¯èƒ½ä¸ç›´è§‚ï¼ˆéœ€åœ¨å›¾æ³¨ä¸­è¯´æ˜Žï¼‰

**æ–¹æ¡ˆ2ï¼šæ–­è½´å›¾**

```
ç­–ç•¥å˜åŒ–çŽ‡
  â”‚
  â”‚  MADDPGæ”¶æ•›åŒºåŸŸ     //    BR-PSOæ”¶æ•›åŒºåŸŸ
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       //    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â”‚å¯†é›†æ›²çº¿  â”‚       //    â”‚ç¨€ç–æ›²çº¿   â”‚
  â””â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€//â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â†’ çŽ¯å¢ƒè¯„ä¼°æ¬¡æ•°
     0        400      //    35,000  36,000
                    æ–­è½´æ ‡è®°
```

**ä¼˜ç‚¹**ï¼š
- âœ… ä¸¤ä¸ªç®—æ³•çš„æ”¶æ•›ç»†èŠ‚éƒ½æ¸…æ™°å¯è§
- âœ… ä¿æŒå…¬å¹³å¯¹æ¯”ï¼ˆä»ç„¶æ˜¯åŒä¸€ä¸ªç‰©ç†é‡ï¼‰
- âœ… æ˜Žç¡®æ ‡æ³¨æ–­è½´ï¼Œä¸è¯¯å¯¼è¯»è€…

**ç¼ºç‚¹**ï¼š
- âš ï¸ å®žçŽ°ç¨å¤æ‚ï¼ˆéœ€è¦matplotlibçš„broken_axisæˆ–æ‰‹åŠ¨ç»˜åˆ¶ä¸¤ä¸ªå­å›¾ï¼‰

**æ–¹æ¡ˆ3ï¼šåŒå­å›¾å¹¶åˆ—ï¼ˆæŽ¨è2ï¼‰**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (a) MADDPGç­–ç•¥å˜åŒ–çŽ‡          â”‚
â”‚     Xè½´: 0-500               â”‚
â”‚     å¯†é›†æ›²çº¿æ¸…æ™°å¯è§          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (b) BR-PSOç­–ç•¥å˜åŒ–çŽ‡          â”‚
â”‚     Xè½´: 0-40,000            â”‚
â”‚     ç¨€ç–ç‚¹/é˜¶è·ƒçº¿æ¸…æ™°å¯è§     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ¯ä¸ªç®—æ³•éƒ½æœ‰æœ€ä¼˜çš„å¯è§†åŒ–æ•ˆæžœ
- âœ… é¿å…æ¨ªè½´èŒƒå›´å†²çª
- âœ… å®žçŽ°ç®€å•

**ç¼ºç‚¹**ï¼š
- âš ï¸ æ— æ³•ç›´è§‚å¯¹æ¯”æ”¶æ•›é€Ÿåº¦ï¼ˆæ¨ªè½´ä¸ç»Ÿä¸€ï¼‰
- âš ï¸ éœ€è¦åœ¨å›¾æ³¨ä¸­æ˜Žç¡®è¯´æ˜Žè®¡ç®—æˆæœ¬å·®å¼‚

**æ–¹æ¡ˆ4ï¼šæ’å›¾æ”¾å¤§**

```
ç­–ç•¥å˜åŒ–çŽ‡
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â”‚ æ’å›¾ï¼š      â”‚
  â”‚  â”‚ MADDPGæ”¾å¤§ â”‚
  â”‚  â”‚ (0-500)    â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚                           BR-PSOæ›²çº¿
  â”‚  MADDPGæ›²çº¿ï¼ˆåŽ‹ç¼©ï¼‰         â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚  â”‚                        â”‚      â”‚
  â””â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â†’ çŽ¯å¢ƒè¯„ä¼°æ¬¡æ•°
     0                            36,000
```

**ä¼˜ç‚¹**ï¼š
- âœ… ä¿æŒå…¬å¹³å¯¹æ¯”çš„å®Œæ•´æ€§ï¼ˆå•ä¸€æ¨ªè½´ï¼‰
- âœ… é€šè¿‡æ’å›¾å±•ç¤ºMADDPGç»†èŠ‚
- âœ… è§†è§‰ä¸Šå¼ºè°ƒè®¡ç®—æ•ˆçŽ‡å·®å¼‚

**ç¼ºç‚¹**ï¼š
- âš ï¸ æ’å›¾å ç”¨ç©ºé—´ï¼Œä¸»å›¾ä¿¡æ¯å¯†åº¦é™ä½Ž

##### 1.7.4 å®žæ–½è®¡åˆ’

**é˜¶æ®µ1ï¼šå®žçŽ°å¤šç§æ–¹æ¡ˆ**
- ç»˜åˆ¶æ–¹æ¡ˆ1ï¼ˆå¯¹æ•°æ¨ªè½´ï¼‰
- ç»˜åˆ¶æ–¹æ¡ˆ2ï¼ˆæ–­è½´å›¾ï¼‰
- ç»˜åˆ¶æ–¹æ¡ˆ3ï¼ˆåŒå­å›¾ï¼‰
- ç»˜åˆ¶æ–¹æ¡ˆ4ï¼ˆæ’å›¾æ”¾å¤§ï¼‰

**é˜¶æ®µ2ï¼šæ•ˆæžœå¯¹æ¯”**
- è§†è§‰æ¸…æ™°åº¦å¯¹æ¯”
- ä¿¡æ¯ä¼ è¾¾æ•ˆæžœå¯¹æ¯”
- é€‰æ‹©æœ€ç»ˆæ–¹æ¡ˆç”¨äºŽè®ºæ–‡/æŠ¥å‘Š

**å€¾å‘æ–¹æ¡ˆ**ï¼š
- **ä¼˜å…ˆå°è¯•ï¼šæ–¹æ¡ˆ2ï¼ˆå¯¹æ•°æ¨ªè½´ï¼‰å’Œæ–¹æ¡ˆ3ï¼ˆåŒå­å›¾ï¼‰**
- æ ¹æ®å®žé™…ç»˜åˆ¶æ•ˆæžœå†³å®šæœ€ç»ˆé€‰æ‹©

##### 1.7.5 è¡¥å……æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰

é™¤äº†ç­–ç•¥å˜åŒ–çŽ‡ï¼Œè¿˜å¯ä»¥ç»˜åˆ¶ï¼š

**ç³»ç»Ÿæ€»æ”¶ç›Š**ï¼š
```python
total_reward = sum(reward_i for all agents)
```
- ç”¨äºŽæ¯”è¾ƒä¸¤ç§ç®—æ³•æ‰¾åˆ°çš„å‡è¡¡è§£çš„ç¤¾ä¼šæ€»ç¦åˆ©

**å¹³å‡ä»·æ ¼**ï¼š
```python
average_price = mean(price[i,t] for all i,t)
```
- ç”¨äºŽè§‚å¯Ÿä¸¤ç§ç®—æ³•æ‰¾åˆ°çš„å‡è¡¡ä»·æ ¼æ°´å¹³æ˜¯å¦ä¸€è‡´

**Nashé—´éš™**ï¼ˆç¨€ç–éªŒè¯ç‚¹ï¼‰ï¼š
```python
nash_gap = max(improvement_gap_i for all i)
```
- é‡åŒ–è·ç¦»çœŸå®žçº³ä»€å‡è¡¡çš„è·ç¦»
- ä»…åœ¨éªŒè¯ç‚¹ç»˜åˆ¶ï¼ˆä¸æ˜¯æ¯æ­¥éƒ½éªŒè¯ï¼‰

**æ³¨æ„**ï¼šä¸å»ºè®®ç»˜åˆ¶å•ä¸ªå……ç”µç«™çš„æ”¶ç›Šå˜åŒ–æ›²çº¿è¿›è¡Œç®—æ³•å¯¹æ¯”
- **åŽŸå› **ï¼šä¸åŒç®—æ³•æ‰¾åˆ°çš„å‡è¡¡è§£ä¸­ï¼Œå„å……ç”µç«™æ”¶ç›Šåˆ†é…å¯èƒ½å®Œå…¨ä¸åŒ
- **æ›¿ä»£**ï¼šä½¿ç”¨ç³»ç»Ÿæ€»æ”¶ç›Šè¿›è¡Œå¯¹æ¯”ï¼ˆå¯ä»¥åˆ¤æ–­å“ªä¸ªå‡è¡¡æ›´ä¼˜ï¼‰
