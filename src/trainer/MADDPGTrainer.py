"""
MADDPGè®­ç»ƒå™¨

ä¸“æ³¨äºæ±‚è§£å……ç”µç«™ä»·æ ¼åšå¼ˆçš„çº³ä»€å‡è¡¡ï¼Œå®ç°ä¸‰å±‚ç»“æ„ï¼š
- Episodeå±‚ï¼šåšå¼ˆæ±‚è§£å°è¯•
- Stepå±‚ï¼šæ™ºèƒ½ä½“ç­–ç•¥è°ƒæ•´
- UE-DTAå±‚ï¼šäº¤é€šä»¿çœŸå“åº”ï¼ˆç”±ç¯å¢ƒæä¾›ï¼‰
"""

import sys
import os
import numpy as np
import torch
from typing import Dict, List, Tuple, Any, Optional
from tqdm import tqdm
import logging
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ¨¡å—
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)

from src.algorithms.maddpg.maddpg import MADDPG
from src.env.EVCSChargingGameEnv import EVCSChargingGameEnv
from src.utils.config import ExperimentTask, MonitorConfig
from src.utils.monitor import TrainingMonitor


class MADDPGTrainer:
    """
    MADDPGå……ç”µç«™ä»·æ ¼åšå¼ˆè®­ç»ƒå™¨

    ä¸“æ³¨äºçº³ä»€å‡è¡¡æ±‚è§£ï¼Œé‡‡ç”¨æ¸…æ™°çš„ä¸‰å±‚ç»“æ„ï¼š
    - Episodeå±‚ï¼šåšå¼ˆæ±‚è§£å°è¯•
    - Stepå±‚ï¼šæ™ºèƒ½ä½“ç­–ç•¥è°ƒæ•´
    - UE-DTAå±‚ï¼šäº¤é€šä»¿çœŸå“åº”
    """

    def __init__(self, task: ExperimentTask, monitor_config: Optional[MonitorConfig] = None):
        """
        åˆå§‹åŒ–MADDPGTrainer

        Args:
            task: å®éªŒä»»åŠ¡å•å…ƒï¼ŒåŒ…å«åœºæ™¯æ¡£æ¡ˆã€ç®—æ³•é…ç½®å’Œéšæœºç§å­
            monitor_config: å¯é€‰çš„ç›‘æ§é…ç½®ï¼ŒNoneåˆ™ä¸å¯ç”¨ç›‘æ§
        """
        self.task = task
        self.config = task.scenario
        self.maddpg_config = task.algo_config
        self.monitor_config = monitor_config or MonitorConfig(enabled=False)

        # å¤„ç†è®¾å¤‡é…ç½®
        if self.config.device == 'auto':
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = self.config.device

        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        if self.device == 'cuda':
            print(f"GPU: {torch.cuda.get_device_name(0)}")

        # 1. åˆ›å»ºç¯å¢ƒ
        self.env = EVCSChargingGameEnv(
            network_dir=self.config.network_dir,
            network_name=self.config.network_name,
            random_seed=task.seed,
            max_steps=self.config.max_steps_per_episode,
            convergence_threshold=self.config.convergence_threshold,
            stable_steps_required=self.config.stable_steps_required
        )

        # 2. ä»ç¯å¢ƒè·å–ç»´åº¦ä¿¡æ¯
        obs_space = self.env.observation_space(self.env.agents[0])
        obs_dim = sum(np.prod(space.shape) for space in obs_space.spaces.values())
        action_dim = self.env.action_space(self.env.agents[0]).shape[0]
        global_obs_dim = self.env.global_state_space().shape[0]

        # 3. åˆ›å»ºMADDPGç®—æ³•
        self.maddpg = MADDPG(
            agent_ids=self.env.agents,
            obs_dim=obs_dim,
            action_dim=action_dim,
            global_obs_dim=global_obs_dim,
            buffer_capacity=self.maddpg_config.buffer_capacity,
            max_batch_size=self.maddpg_config.max_batch_size,
            actor_lr=self.maddpg_config.actor_lr,
            critic_lr=self.maddpg_config.critic_lr,
            gamma=self.maddpg_config.gamma,
            tau=self.maddpg_config.tau,
            seed=task.seed,
            device=self.device,
            actor_hidden_sizes=self.maddpg_config.actor_hidden_sizes,
            critic_hidden_sizes=self.maddpg_config.critic_hidden_sizes,
            noise_sigma=self.maddpg_config.noise_sigma,
            noise_decay=self.maddpg_config.noise_decay,
            min_noise=self.maddpg_config.min_noise,
            flow_scale_factor=self.env.flow_scale_factor
        )

        # 4. è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.convergence_episodes = []      # æ”¶æ•›çš„episodeåˆ—è¡¨
        self.episode_lengths = []           # æ¯ä¸ªepisodeçš„é•¿åº¦
        self.step_records = []              # æ¯æ­¥è¯¦ç»†è®°å½•ï¼ˆåŒ…å«æ‰€æœ‰è®­ç»ƒæ•°æ®ï¼‰

        # 5. åˆ›å»ºç›‘æ§å™¨
        self.monitor = TrainingMonitor(
            config=self.monitor_config,
            experiment_name=task.name,
            n_agents=self.env.n_agents,
            agent_names=self.env.agents,
            convergence_threshold=self.config.convergence_threshold,
            ue_threshold=self.env.ue_convergence_threshold
        )

        # 6. è®¾ç½®UE-DTAå›è°ƒ
        if self.monitor_config.enabled:
            self.env.set_ue_callback(self.monitor.on_ue_iteration)
    
    def train(self) -> Dict:
        """
        ä¸»è®­ç»ƒå¾ªç¯ï¼šå¯»æ‰¾çº³ä»€å‡è¡¡
        
        Episodeå±‚é€»è¾‘ï¼šæ¯ä¸ªepisodeéƒ½æ˜¯å¯¹åŒä¸€ä»·æ ¼åšå¼ˆçš„æ±‚è§£å°è¯•
        æˆåŠŸæ ‡å‡†ï¼šåœ¨å•ä¸ªepisodeä¸­è¾¾åˆ°çº³ä»€å‡è¡¡ï¼ˆä»·æ ¼æ”¶æ•›ï¼‰
        
        Returns:
            Dict: è®­ç»ƒç»“æœç»Ÿè®¡
        """
        convergence_count = 0
        
        with tqdm(total=self.config.max_episodes, desc="å¯»æ‰¾çº³ä»€å‡è¡¡", unit="episode", dynamic_ncols=True) as episode_pbar:
            
            for episode in range(self.config.max_episodes):

                # é€šçŸ¥ç›‘æ§å™¨Episodeå¼€å§‹
                self.monitor.on_episode_start(episode)

                # é‡æ–°åˆå§‹åŒ–åŒä¸€åšå¼ˆï¼ˆä¸æ˜¯æ–°åšå¼ˆï¼‰
                observations, _ = self.env.reset()
                
                # Stepå±‚ï¼šç­–ç•¥è°ƒæ•´å¾ªç¯
                converged_in_episode, episode_length = self._run_episode(episode, observations)
                
                # è®°å½•episodeç»Ÿè®¡
                self.episode_lengths.append(episode_length)
                
                if converged_in_episode:
                    convergence_count += 1
                    self.convergence_episodes.append(episode)
                    episode_pbar.set_postfix({
                        "æ”¶æ•›æ¬¡æ•°": convergence_count,
                        "æ”¶æ•›ç‡": f"{convergence_count/(episode+1):.1%}"
                    })
                    
                    # å¯é€‰ï¼šå¦‚æœè¿ç»­å¤šæ¬¡æ”¶æ•›ï¼Œå¯æå‰ç»“æŸ
                    if self._check_stable_convergence():
                        print(f"è¿ç»­æ”¶æ•›ï¼Œè®­ç»ƒæå‰ç»“æŸäºepisode {episode}")
                        break

                episode_pbar.update(1)

        # ç”Ÿæˆè®­ç»ƒç»“æœ
        results = self._generate_training_results()

        # æ‰“å°è¯Šæ–­æ‘˜è¦
        self._print_diagnostics_summary(results.get('diagnostics', {}))

        # å…³é—­ç›‘æ§å™¨
        self.monitor.close()

        # è‡ªåŠ¨ä¿å­˜è®­ç»ƒæ•°æ®
        experiment_dir = self.save_training_data()
        results['experiment_dir'] = experiment_dir

        return results

    def _print_diagnostics_summary(self, diagnostics: Dict):
        """
        æ‰“å°å­¦ä¹ è¯Šæ–­æ‘˜è¦åˆ°æ§åˆ¶å°

        Args:
            diagnostics: è¯Šæ–­æ‘˜è¦æ•°æ®
        """
        print("\n" + "=" * 60)
        print("å­¦ä¹ è¯Šæ–­æ‘˜è¦")
        print("=" * 60)

        if diagnostics.get('status') == 'no_learning_data':
            print("æ²¡æœ‰å­¦ä¹ æŒ‡æ ‡æ•°æ®")
            return

        print(f"æ€»å­¦ä¹ æ­¥æ•°: {diagnostics.get('total_learn_steps', 0)}")
        print()

        # æ‰“å°æ¯ä¸ª agent çš„å…³é”®æŒ‡æ ‡
        for agent_id, agent_data in diagnostics.get('agents', {}).items():
            print(f"--- {agent_id} ---")

            # Actor æ¢¯åº¦ï¼ˆå…³é”®æŒ‡æ ‡ï¼‰
            actor_grad = agent_data.get('actor_grad_norm', {})
            if actor_grad:
                print(f"  Actor æ¢¯åº¦èŒƒæ•°: mean={actor_grad['mean']:.2e}, "
                      f"first={actor_grad['first']:.2e}, last={actor_grad['last']:.2e}")

            # Critic æ¢¯åº¦
            critic_grad = agent_data.get('critic_grad_norm', {})
            if critic_grad:
                print(f"  Critic æ¢¯åº¦èŒƒæ•°: mean={critic_grad['mean']:.2e}")

            # Q å€¼ç»Ÿè®¡
            q_mean = agent_data.get('q_value_mean', {})
            if q_mean:
                print(f"  Q å€¼å‡å€¼: mean={q_mean['mean']:.4f}, "
                      f"first={q_mean['first']:.4f}, last={q_mean['last']:.4f}")

            # æ¢ç´¢å™ªéŸ³
            noise = agent_data.get('noise_sigma', {})
            if noise:
                print(f"  æ¢ç´¢å™ªéŸ³ sigma: first={noise['first']:.4f}, last={noise['last']:.4f}")

            print()

        # æ‰“å°è¯Šæ–­ç»“è®º
        print("è¯Šæ–­ç»“è®º:")
        for issue in diagnostics.get('diagnosis', []):
            print(f"  {issue}")

        print("=" * 60 + "\n")
    
    def _run_episode(self, episode: int, observations: Dict) -> Tuple[bool, int]:
        """
        Stepå±‚é€»è¾‘ï¼šåœ¨å•ä¸ªepisodeå†…è°ƒæ•´æ™ºèƒ½ä½“ç­–ç•¥ç›´åˆ°æ”¶æ•›æˆ–è¶…æ—¶

        Args:
            episode: å½“å‰episodeç¼–å·
            observations: åˆå§‹è§‚æµ‹

        Returns:
            Tuple[bool, int]: (æ˜¯å¦æ”¶æ•›, episodeé•¿åº¦)
        """
        with tqdm(total=self.config.max_steps_per_episode,
                  desc=f"Episode {episode}", unit="step", leave=False, dynamic_ncols=True) as step_pbar:

            for step in range(self.config.max_steps_per_episode):

                # æ™ºèƒ½ä½“å†³ç­–
                actions = self.maddpg.take_action(observations, add_noise=True)

                # è·å–å®é™…ä»·æ ¼ï¼ˆåœ¨stepä¹‹å‰ï¼‰
                actual_prices = self.env.actions_to_prices_dict(actions)

                # ç¯å¢ƒå“åº”
                next_observations, rewards, terminations, truncations, infos = self.env.step(actions)

                # å­˜å‚¨ç»éªŒå¹¶å­¦ä¹ 
                self.maddpg.store_experience(observations, actions, rewards, next_observations, terminations)
                learn_metrics = self.maddpg.learn()

                # è®°å½•è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«è¯Šæ–­æŒ‡æ ‡ï¼‰
                step_record = {
                    'episode': episode,
                    'step': step,
                    'actions': actions.copy(),
                    'actual_prices': actual_prices.copy(),
                    'rewards': rewards.copy(),
                    'ue_info': infos,
                    'relative_change_rate': infos.get('relative_change_rate', float('inf'))
                }

                # æ·»åŠ å­¦ä¹ è¯Šæ–­æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
                if learn_metrics is not None:
                    step_record['learn_metrics'] = learn_metrics

                self.step_records.append(step_record)

                # é€šçŸ¥ç›‘æ§å™¨Stepç»“æŸ
                self.monitor.on_step_end(
                    step=step,
                    convergence_rate=infos.get('relative_change_rate', float('inf')),
                    rewards=rewards
                )

                # æ£€æŸ¥æ˜¯å¦æ”¶æ•›ï¼ˆçº³ä»€å‡è¡¡ï¼‰
                if all(terminations.values()):
                    step_pbar.set_postfix({"çŠ¶æ€": "æ”¶æ•›"})
                    step_pbar.update(self.config.max_steps_per_episode - step)
                    return True, step + 1

                # æ›´æ–°è§‚æµ‹
                observations = next_observations

                # æ›´æ–°è¿›åº¦æ¡ï¼ˆæ·»åŠ è¯Šæ–­ä¿¡æ¯ï¼‰
                postfix = {
                    "UEè¿­ä»£": infos.get('ue_iterations', 0),
                    "ç›¸å¯¹å˜åŒ–": f"{infos.get('relative_change_rate', float('inf')):.4f}"
                }
                # å¦‚æœæœ‰å­¦ä¹ æŒ‡æ ‡ï¼Œæ˜¾ç¤ºç¬¬ä¸€ä¸ª agent çš„ actor æ¢¯åº¦èŒƒæ•°
                if learn_metrics is not None:
                    first_agent = list(learn_metrics['agents'].keys())[0]
                    postfix["Actoræ¢¯åº¦"] = f"{learn_metrics['agents'][first_agent]['actor_grad_norm']:.2e}"
                step_pbar.set_postfix(postfix)
                step_pbar.update(1)

        # Episodeè¶…æ—¶æœªæ”¶æ•›
        step_pbar.set_postfix({"çŠ¶æ€": "è¶…æ—¶"})
        return False, self.config.max_steps_per_episode
    
    def get_nash_equilibrium(self) -> Dict:
        """
        è·å–æ‰€æœ‰çº³ä»€å‡è¡¡è§£ï¼ˆä»æ‰€æœ‰æ”¶æ•›çš„episodeçš„ç¨³å®šæ­¥éª¤ä¸­è®¡ç®—å¹³å‡å€¼ï¼‰
        
        Returns:
            Dict: åŒ…å«æ‰€æœ‰çº³ä»€å‡è¡¡è§£çš„ç»Ÿè®¡ï¼Œæ ¼å¼ä¸ºï¼š
                - status: 'converged' | 'no_convergence'
                - total_equilibria: æ‰¾åˆ°çš„å‡è¡¡è§£æ•°é‡
                - equilibria: List[Dict], æ¯ä¸ªå‡è¡¡è§£åŒ…å«ï¼š
                    - episode: episodeç¼–å·
                    - equilibrium_actions: å¹³å‡åŠ¨ä½œ
                    - equilibrium_prices: å¹³å‡ä»·æ ¼  
                    - equilibrium_rewards: å¹³å‡å¥–åŠ±
                    - stable_steps_count: ç¨³å®šæ­¥æ•°
        """
        if not self.convergence_episodes:
            return {
                'status': 'no_convergence',
                'message': 'æœªæ‰¾åˆ°æ”¶æ•›çš„episode',
                'total_equilibria': 0,
                'equilibria': []
            }
        
        equilibria = []
        
        # ä¸ºæ¯ä¸ªæ”¶æ•›çš„episodeè®¡ç®—çº³ä»€å‡è¡¡è§£
        for episode_idx in self.convergence_episodes:
            # ä»step_recordsä¸­æ‰¾åˆ°è¯¥episodeçš„æ”¶æ•›æ­¥éª¤
            convergence_steps = [
                record for record in self.step_records 
                if record['episode'] == episode_idx
            ]
            
            if not convergence_steps:
                continue
            
            # è·å–æ”¶æ•›æ—¶çš„ç¨³å®šæ­¥éª¤æ•°æ®ï¼ˆæœ€å stable_steps_required æ­¥ï¼‰
            stable_steps = convergence_steps[-self.config.stable_steps_required:]
            
            # è®¡ç®—ç¨³å®šæ­¥éª¤çš„å¹³å‡åŠ¨ä½œ
            equilibrium_actions = {}
            for agent in self.env.agents:
                agent_actions = []
                for step in stable_steps:
                    agent_actions.append(step['actions'][agent])
                # è®¡ç®—å¹³å‡åŠ¨ä½œ
                equilibrium_actions[agent] = np.mean(agent_actions, axis=0)
            
            # å°†å¹³å‡åŠ¨ä½œè½¬æ¢ä¸ºå®é™…ä»·æ ¼
            actual_prices = self.env.actions_to_prices_dict(equilibrium_actions)
            
            # è®¡ç®—ç¨³å®šæ­¥éª¤çš„å¹³å‡å¥–åŠ±
            equilibrium_rewards = {}
            for agent in self.env.agents:
                agent_rewards = []
                for step in stable_steps:
                    agent_rewards.append(step['rewards'][agent])
                equilibrium_rewards[agent] = float(np.mean(agent_rewards))
            
            # æ·»åŠ åˆ°å‡è¡¡è§£åˆ—è¡¨
            equilibria.append({
                'episode': episode_idx,
                'final_step': stable_steps[-1]['step'],
                'stable_steps_count': len(stable_steps),
                'equilibrium_actions': equilibrium_actions,
                'equilibrium_prices': actual_prices,
                'equilibrium_rewards': equilibrium_rewards,
                'environment_info': stable_steps[-1]['ue_info']
            })
        
        return {
            'status': 'converged',
            'total_equilibria': len(equilibria),
            'equilibria': equilibria,
            'latest_equilibrium': equilibria[-1] if equilibria else None
        }
    
    def evaluate(self, num_episodes: int = 20) -> Dict:
        """
        è¯„ä¼°è®­ç»ƒæ•ˆæœï¼šæµ‹è¯•å‡è¡¡ç¨³å®šæ€§
        
        Args:
            num_episodes: è¯„ä¼°è½®æ•°
        
        Returns:
            Dict: è¯„ä¼°ç»“æœ
            
        TODO: å®ç°å•æ–¹é¢åç¦»æµ‹è¯•ï¼ŒéªŒè¯çº³ä»€å‡è¡¡æ€§è´¨
        """
        # ğŸ”„ æš‚æ—¶ç•™ç©ºï¼Œåç»­å®ç°å•æ–¹é¢åç¦»æµ‹è¯•
        return {
            'status': 'not_implemented',
            'message': 'å°†å®ç°å•æ–¹é¢åç¦»æµ‹è¯•éªŒè¯çº³ä»€å‡è¡¡æ€§è´¨'
        }
    
    def _check_stable_convergence(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç¨³å®šæ”¶æ•›ï¼ˆè¿ç»­å¤šæ¬¡episodeéƒ½æ”¶æ•›ï¼‰
        
        Returns:
            bool: æ˜¯å¦è¿ç»­æ”¶æ•›ï¼Œå¯ç”¨äºæå‰ç»ˆæ­¢è®­ç»ƒ
        """
        if len(self.convergence_episodes) < self.config.stable_episodes_required:
            return False
        
        # æ£€æŸ¥æœ€è¿‘stable_episodes_requiredæ¬¡episodeæ˜¯å¦éƒ½æ”¶æ•›
        recent_episodes = list(range(len(self.episode_lengths)))[-self.config.stable_episodes_required:]
        return all(ep in self.convergence_episodes for ep in recent_episodes)
    
    def _generate_training_results(self) -> Dict:
        """
        ç”Ÿæˆå®Œæ•´çš„è®­ç»ƒç»“æœç»Ÿè®¡

        Returns:
            Dict: è®­ç»ƒç»Ÿè®¡ç»“æœ
        """
        total_episodes = len(self.episode_lengths)
        total_convergences = len(self.convergence_episodes)

        # ä»step_recordsè®¡ç®—æ€»UEè¿­ä»£æ¬¡æ•°
        total_ue_iterations = sum(
            record['ue_info'].get('ue_iterations', 0)
            for record in self.step_records
        )

        # ç”Ÿæˆå­¦ä¹ è¯Šæ–­æ‘˜è¦
        diagnostics = self._generate_diagnostics_summary()

        return {
            'total_episodes': total_episodes,
            'total_convergences': total_convergences,
            'convergence_rate': total_convergences / total_episodes if total_episodes > 0 else 0.0,
            'average_episode_length': np.mean(self.episode_lengths) if self.episode_lengths else 0.0,
            'total_ue_iterations': total_ue_iterations,
            'convergence_episodes': self.convergence_episodes,
            'final_nash_equilibrium': self.get_nash_equilibrium(),
            'diagnostics': diagnostics
        }

    def _generate_diagnostics_summary(self) -> Dict:
        """
        ç”Ÿæˆå­¦ä¹ è¯Šæ–­æ‘˜è¦

        ä» step_records ä¸­æå–å­¦ä¹ æŒ‡æ ‡ï¼Œè®¡ç®—ç»Ÿè®¡æ‘˜è¦ï¼Œ
        ç”¨äºè¯Šæ–­æ¢¯åº¦æ–­å¼€ã€Qå€¼çˆ†ç‚¸ã€æ¢ç´¢åœæ­¢ç­‰é—®é¢˜ã€‚

        Returns:
            Dict: è¯Šæ–­æ‘˜è¦ï¼ŒåŒ…å«æ¯ä¸ª agent çš„æŒ‡æ ‡ç»Ÿè®¡
        """
        # æ”¶é›†æ‰€æœ‰æœ‰å­¦ä¹ æŒ‡æ ‡çš„ step
        metrics_records = [r['learn_metrics'] for r in self.step_records if r.get('learn_metrics')]

        if not metrics_records:
            return {'status': 'no_learning_data', 'message': 'æ²¡æœ‰å­¦ä¹ æŒ‡æ ‡æ•°æ®'}

        # è·å– agent åˆ—è¡¨
        agents = list(metrics_records[0]['agents'].keys())

        summary = {
            'total_learn_steps': len(metrics_records),
            'agents': {}
        }

        for agent_id in agents:
            agent_metrics = {
                'actor_loss': [],
                'actor_grad_norm': [],
                'critic_loss': [],
                'critic_grad_norm': [],
                'q_value_mean': [],
                'noise_sigma': []
            }

            for record in metrics_records:
                agent_data = record['agents'].get(agent_id, {})
                for key in agent_metrics:
                    if key in agent_data:
                        agent_metrics[key].append(agent_data[key])

            # è®¡ç®—ç»Ÿè®¡æ‘˜è¦
            agent_summary = {}
            for key, values in agent_metrics.items():
                if values:
                    arr = np.array(values)
                    agent_summary[key] = {
                        'mean': float(np.mean(arr)),
                        'std': float(np.std(arr)),
                        'min': float(np.min(arr)),
                        'max': float(np.max(arr)),
                        'first': float(arr[0]),
                        'last': float(arr[-1]),
                    }

            summary['agents'][agent_id] = agent_summary

        # æ·»åŠ è¯Šæ–­ç»“è®º
        summary['diagnosis'] = self._diagnose_learning_issues(summary)

        return summary

    def _diagnose_learning_issues(self, summary: Dict) -> List[str]:
        """
        æ ¹æ®è¯Šæ–­æ‘˜è¦è‡ªåŠ¨æ£€æµ‹æ½œåœ¨é—®é¢˜

        Args:
            summary: è¯Šæ–­æ‘˜è¦æ•°æ®

        Returns:
            List[str]: æ£€æµ‹åˆ°çš„é—®é¢˜åˆ—è¡¨
        """
        issues = []

        for agent_id, agent_data in summary.get('agents', {}).items():
            # æ£€æŸ¥ Actor æ¢¯åº¦æ˜¯å¦æ¥è¿‘ 0
            actor_grad = agent_data.get('actor_grad_norm', {})
            if actor_grad and actor_grad.get('mean', 1) < 1e-6:
                issues.append(f"âš ï¸ {agent_id}: Actor æ¢¯åº¦æ¥è¿‘ 0 (mean={actor_grad['mean']:.2e})ï¼Œå¯èƒ½å­˜åœ¨æ¢¯åº¦æ–­å¼€é—®é¢˜")

            # æ£€æŸ¥æ¢ç´¢æ˜¯å¦è¿‡æ—©åœæ­¢
            noise = agent_data.get('noise_sigma', {})
            if noise and noise.get('last', 1) <= 0.011:  # æ¥è¿‘ min_noise=0.01
                issues.append(f"âš ï¸ {agent_id}: æ¢ç´¢å™ªéŸ³å·²é™è‡³æœ€å°å€¼ (sigma={noise['last']:.4f})ï¼Œå¯èƒ½è¿‡æ—©åœæ­¢æ¢ç´¢")

            # æ£€æŸ¥ Q å€¼æ˜¯å¦çˆ†ç‚¸
            q_mean = agent_data.get('q_value_mean', {})
            if q_mean and (abs(q_mean.get('max', 0)) > 1e6 or abs(q_mean.get('min', 0)) > 1e6):
                issues.append(f"âš ï¸ {agent_id}: Q å€¼å¯èƒ½çˆ†ç‚¸ (max={q_mean['max']:.2e})")

            # æ£€æŸ¥ Actor loss æ˜¯å¦é•¿æœŸä¸å˜
            actor_loss = agent_data.get('actor_loss', {})
            if actor_loss and actor_loss.get('std', 1) < 1e-6:
                issues.append(f"âš ï¸ {agent_id}: Actor loss å‡ ä¹ä¸å˜ (std={actor_loss['std']:.2e})ï¼ŒActor å¯èƒ½æ²¡æœ‰åœ¨å­¦ä¹ ")

        if not issues:
            issues.append("âœ… æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„å­¦ä¹ é—®é¢˜")

        return issues
    
    def save_training_data(self) -> str:
        """
        ä¿å­˜è®­ç»ƒæ•°æ®åˆ°JSONæ–‡ä»¶

        Returns:
            str: ä¿å­˜çš„å®éªŒç›®å½•è·¯å¾„
        """
        # ä½¿ç”¨ ExperimentTask ç”Ÿæˆè§„èŒƒè·¯å¾„
        experiment_dir = self.task.get_output_path()
        os.makedirs(experiment_dir, exist_ok=True)
        
        # ä¿å­˜step_recordsåˆ°JSONæ–‡ä»¶
        step_records_path = os.path.join(experiment_dir, "step_records.json")
        
        # å‡†å¤‡ä¿å­˜æ•°æ®ï¼Œå¤„ç†numpyæ•°ç»„
        save_data = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_episodes": len(self.episode_lengths),
                "total_steps": len(self.step_records),
                "convergence_episodes": self.convergence_episodes,
                "episode_lengths": self.episode_lengths
            },
            "records": []
        }
        
        # è½¬æ¢step_recordsï¼Œå¤„ç†numpyæ•°ç»„
        for record in self.step_records:
            converted_record = {
                "episode": int(record["episode"]),
                "step": int(record["step"]),
                "actions": {k: v.tolist() if hasattr(v, 'tolist') else v 
                           for k, v in record["actions"].items()},
                "actual_prices": {k: v.tolist() if hasattr(v, 'tolist') else v 
                                 for k, v in record["actual_prices"].items()},
                "rewards": {k: float(v) for k, v in record["rewards"].items()},
                "ue_info": record["ue_info"],
                "relative_change_rate": float(record["relative_change_rate"])
            }
            save_data["records"].append(converted_record)
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(step_records_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {experiment_dir}")
        print(f"   ğŸ“ å®éªŒç›®å½•: {experiment_dir}")
        print(f"   ğŸ“„ æ•°æ®æ–‡ä»¶: step_records.json")
        print(f"   ğŸ“Š è®°å½•æ•°é‡: {len(self.step_records)} æ­¥")
        
        return experiment_dir