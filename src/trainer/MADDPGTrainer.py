"""
MADDPGè®­ç»ƒå™¨

ä¸“æ³¨äºæ±‚è§£å……ç”µç«™ä»·æ ¼åšå¼ˆçš„çº³ä»€å‡è¡¡ï¼Œå®ç°ä¸‰å±‚ç»“æ„ï¼š
- Episodeå±‚ï¼šåšå¼ˆæ±‚è§£å°è¯•
- Stepå±‚ï¼šæ™ºèƒ½ä½“ç­–ç•¥è°ƒæ•´
- UE-DTAå±‚ï¼šäº¤é€šä»¿çœŸå“åº”ï¼ˆç”±ç¯å¢ƒæä¾›ï¼‰
"""

import sys
import os
import numpy as np
from typing import Dict, List, Tuple, Any
from tqdm import tqdm
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ¨¡å—
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)

from src.algorithms.maddpg.maddpg import MADDPG
from src.env.EVCSChargingGameEnv import EVCSChargingGameEnv
from src.utils.config import MADDPGConfig, TrainingConfig


class MADDPGTrainer:
    """
    MADDPGå……ç”µç«™ä»·æ ¼åšå¼ˆè®­ç»ƒå™¨
    
    ä¸“æ³¨äºçº³ä»€å‡è¡¡æ±‚è§£ï¼Œé‡‡ç”¨æ¸…æ™°çš„ä¸‰å±‚ç»“æ„ï¼š
    - Episodeå±‚ï¼šåšå¼ˆæ±‚è§£å°è¯•  
    - Stepå±‚ï¼šæ™ºèƒ½ä½“ç­–ç•¥è°ƒæ•´
    - UE-DTAå±‚ï¼šäº¤é€šä»¿çœŸå“åº”
    """
    
    def __init__(self, maddpg_config: MADDPGConfig, training_config: TrainingConfig):
        """
        åˆå§‹åŒ–MADDPGTrainer
        
        Args:
            maddpg_config: MADDPGç®—æ³•é…ç½®
            training_config: è®­ç»ƒæµç¨‹é…ç½®
        """
        self.config = training_config
        self.maddpg_config = maddpg_config
        
        # 1. åˆ›å»ºç¯å¢ƒ
        self.env = EVCSChargingGameEnv(
            network_dir=training_config.network_dir,
            network_name=training_config.network_name,
            random_seed=training_config.seed,
            max_steps=training_config.max_steps_per_episode,
            convergence_threshold=training_config.convergence_threshold
        )
        
        # 2. ä»ç¯å¢ƒè·å–ç»´åº¦ä¿¡æ¯
        obs_space = self.env.observation_space(self.env.agents[0])
        obs_dim = sum(np.prod(space.shape) for space in obs_space.spaces.values())
        action_dim = self.env.action_space(self.env.agents[0]).shape[0]
        global_obs_dim = self.env.global_state_space().shape[0]
        
        # 3. åˆ›å»ºMADDPGç®—æ³•
        self.maddpg = MADDPG(
            agent_ids=self.env.agents,
            obs_dim=obs_dim,
            action_dim=action_dim,
            global_obs_dim=global_obs_dim,
            **maddpg_config.__dict__
        )
        
        # 4. è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.convergence_episodes = []      # æ”¶æ•›çš„episodeåˆ—è¡¨
        self.episode_lengths = []           # æ¯ä¸ªepisodeçš„é•¿åº¦
        self.step_records = []              # æ¯æ­¥è¯¦ç»†è®°å½•
        self.total_ue_iterations = 0        # æ€»UEä»¿çœŸè¿­ä»£æ¬¡æ•°
    
    def train(self) -> Dict:
        """
        ä¸»è®­ç»ƒå¾ªç¯ï¼šå¯»æ‰¾çº³ä»€å‡è¡¡
        
        Episodeå±‚é€»è¾‘ï¼šæ¯ä¸ªepisodeéƒ½æ˜¯å¯¹åŒä¸€ä»·æ ¼åšå¼ˆçš„æ±‚è§£å°è¯•
        æˆåŠŸæ ‡å‡†ï¼šåœ¨å•ä¸ªepisodeä¸­è¾¾åˆ°çº³ä»€å‡è¡¡ï¼ˆä»·æ ¼æ”¶æ•›ï¼‰
        
        Returns:
            Dict: è®­ç»ƒç»“æœç»Ÿè®¡
        """
        convergence_count = 0
        
        with tqdm(total=self.config.max_episodes, desc="å¯»æ‰¾çº³ä»€å‡è¡¡", unit="episode") as episode_pbar:
            
            for episode in range(self.config.max_episodes):
                
                # é‡æ–°åˆå§‹åŒ–åŒä¸€åšå¼ˆï¼ˆä¸æ˜¯æ–°åšå¼ˆï¼‰
                observations, _ = self.env.reset()
                
                # Stepå±‚ï¼šç­–ç•¥è°ƒæ•´å¾ªç¯
                converged_in_episode, episode_length = self._run_episode(episode, observations)
                
                # è®°å½•episodeç»Ÿè®¡
                self.episode_lengths.append(episode_length)
                
                if converged_in_episode:
                    convergence_count += 1
                    self.convergence_episodes.append(episode)
                    episode_pbar.set_postfix({
                        "æ”¶æ•›æ¬¡æ•°": convergence_count,
                        "æ”¶æ•›ç‡": f"{convergence_count/(episode+1):.1%}"
                    })
                    
                    # å¯é€‰ï¼šå¦‚æœè¿ç»­å¤šæ¬¡æ”¶æ•›ï¼Œå¯æå‰ç»“æŸ
                    if self._check_stable_convergence():
                        print(f"è¿ç»­æ”¶æ•›ï¼Œè®­ç»ƒæå‰ç»“æŸäºepisode {episode}")
                        break
                
                episode_pbar.update(1)
        
        return self._generate_training_results()
    
    def _run_episode(self, episode: int, observations: Dict) -> Tuple[bool, int]:
        """
        Stepå±‚é€»è¾‘ï¼šåœ¨å•ä¸ªepisodeå†…è°ƒæ•´æ™ºèƒ½ä½“ç­–ç•¥ç›´åˆ°æ”¶æ•›æˆ–è¶…æ—¶
        
        Args:
            episode: å½“å‰episodeç¼–å·
            observations: åˆå§‹è§‚æµ‹
        
        Returns:
            Tuple[bool, int]: (æ˜¯å¦æ”¶æ•›, episodeé•¿åº¦)
        """
        with tqdm(total=self.config.max_steps_per_episode, 
                  desc=f"Episode {episode}", unit="step", leave=False) as step_pbar:
            
            for step in range(self.config.max_steps_per_episode):
                
                # æ™ºèƒ½ä½“å†³ç­–
                actions = self.maddpg.take_action(observations, add_noise=True)
                
                # ç¯å¢ƒå“åº”
                next_observations, rewards, terminations, truncations, infos = self.env.step(actions)
                
                # å­˜å‚¨ç»éªŒå¹¶å­¦ä¹ 
                self.maddpg.store_experience(observations, actions, rewards, next_observations, terminations)
                self.maddpg.learn()
                
                # è®°å½•è¯¦ç»†ä¿¡æ¯
                self.step_records.append({
                    'episode': episode,
                    'step': step,
                    'actions': actions.copy(),
                    'rewards': rewards.copy(),
                    'ue_info': infos,
                    'relative_change_rate': infos.get('relative_change_rate', float('inf'))
                })
                
                # ç´¯è®¡UEè¿­ä»£æ¬¡æ•°
                if 'ue_iterations' in infos:
                    self.total_ue_iterations += infos['ue_iterations']
                
                # æ£€æŸ¥æ˜¯å¦æ”¶æ•›ï¼ˆçº³ä»€å‡è¡¡ï¼‰
                if terminations.get('__all__', False):
                    step_pbar.set_postfix({"çŠ¶æ€": "æ”¶æ•›"})
                    step_pbar.update(self.config.max_steps_per_episode - step)
                    return True, step + 1
                
                # æ›´æ–°è§‚æµ‹
                observations = next_observations
                
                # æ›´æ–°è¿›åº¦æ¡
                step_pbar.set_postfix({
                    "UEè¿­ä»£": infos.get('ue_iterations', 0),
                    "ç›¸å¯¹å˜åŒ–": f"{infos.get('relative_change_rate', float('inf')):.4f}"
                })
                step_pbar.update(1)
        
        # Episodeè¶…æ—¶æœªæ”¶æ•›
        step_pbar.set_postfix({"çŠ¶æ€": "è¶…æ—¶"})
        return False, self.config.max_steps_per_episode
    
    def get_nash_equilibrium(self) -> Dict:
        """
        è·å–çº³ä»€å‡è¡¡è§£ï¼ˆä»æ”¶æ•›çš„episodeä¸­è·å–ï¼‰
        
        Returns:
            Dict: çº³ä»€å‡è¡¡ç­–ç•¥å’Œç»Ÿè®¡ï¼Œå¦‚æœæœªæ‰¾åˆ°æ”¶æ•›episodeåˆ™è¿”å›None
        """
        if not self.convergence_episodes:
            return {
                'status': 'no_convergence',
                'message': 'æœªæ‰¾åˆ°æ”¶æ•›çš„episode'
            }
        
        # è·å–æœ€è¿‘ä¸€æ¬¡æ”¶æ•›çš„episode
        latest_convergence_episode = self.convergence_episodes[-1]
        
        # ä»step_recordsä¸­æ‰¾åˆ°è¯¥episodeçš„æ”¶æ•›æ­¥éª¤ï¼ˆæœ€åä¸€æ­¥ï¼‰
        convergence_steps = [
            record for record in self.step_records 
            if record['episode'] == latest_convergence_episode
        ]
        
        if not convergence_steps:
            return {
                'status': 'no_data',
                'message': f'æœªæ‰¾åˆ°episode {latest_convergence_episode}çš„è®°å½•æ•°æ®'
            }
        
        # è·å–æ”¶æ•›æ—¶çš„æœ€åä¸€æ­¥æ•°æ®
        final_step = convergence_steps[-1]
        equilibrium_actions = final_step['actions']
        
        # å°†å½’ä¸€åŒ–åŠ¨ä½œè½¬æ¢ä¸ºå®é™…ä»·æ ¼
        actual_prices = {}
        for agent_id, action in equilibrium_actions.items():
            actual_prices[agent_id] = self.env.actions_to_prices({agent_id: action})[int(agent_id)]
        
        return {
            'status': 'converged',
            'episode': latest_convergence_episode,
            'step': final_step['step'],
            'equilibrium_actions': equilibrium_actions,
            'equilibrium_prices': actual_prices,
            'equilibrium_rewards': final_step['rewards'],
            'environment_info': final_step['ue_info']
        }
    
    def evaluate(self, num_episodes: int = 20) -> Dict:
        """
        è¯„ä¼°è®­ç»ƒæ•ˆæœï¼šæµ‹è¯•å‡è¡¡ç¨³å®šæ€§
        
        Args:
            num_episodes: è¯„ä¼°è½®æ•°
        
        Returns:
            Dict: è¯„ä¼°ç»“æœ
            
        TODO: å®ç°å•æ–¹é¢åç¦»æµ‹è¯•ï¼ŒéªŒè¯çº³ä»€å‡è¡¡æ€§è´¨
        """
        # ğŸ”„ æš‚æ—¶ç•™ç©ºï¼Œåç»­å®ç°å•æ–¹é¢åç¦»æµ‹è¯•
        return {
            'status': 'not_implemented',
            'message': 'å°†å®ç°å•æ–¹é¢åç¦»æµ‹è¯•éªŒè¯çº³ä»€å‡è¡¡æ€§è´¨'
        }
    
    def _check_stable_convergence(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç¨³å®šæ”¶æ•›ï¼ˆè¿ç»­å¤šæ¬¡episodeéƒ½æ”¶æ•›ï¼‰
        
        Returns:
            bool: æ˜¯å¦è¿ç»­æ”¶æ•›ï¼Œå¯ç”¨äºæå‰ç»ˆæ­¢è®­ç»ƒ
        """
        if len(self.convergence_episodes) < 3:
            return False
        
        # æ£€æŸ¥æœ€è¿‘3æ¬¡episodeæ˜¯å¦éƒ½æ”¶æ•›
        recent_episodes = list(range(len(self.episode_lengths)))[-3:]
        return all(ep in self.convergence_episodes for ep in recent_episodes)
    
    def _generate_training_results(self) -> Dict:
        """
        ç”Ÿæˆå®Œæ•´çš„è®­ç»ƒç»“æœç»Ÿè®¡
        
        Returns:
            Dict: è®­ç»ƒç»Ÿè®¡ç»“æœ
        """
        total_episodes = len(self.episode_lengths)
        total_convergences = len(self.convergence_episodes)
        
        return {
            'total_episodes': total_episodes,
            'total_convergences': total_convergences,
            'convergence_rate': total_convergences / total_episodes if total_episodes > 0 else 0.0,
            'average_episode_length': np.mean(self.episode_lengths) if self.episode_lengths else 0.0,
            'total_ue_iterations': self.total_ue_iterations,
            'convergence_episodes': self.convergence_episodes,
            'final_nash_equilibrium': self.get_nash_equilibrium()
        }