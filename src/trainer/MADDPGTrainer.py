"""
MADDPGè®­ç»ƒå™¨

ä¸“æ³¨äºæ±‚è§£å……ç”µç«™ä»·æ ¼åšå¼ˆçš„çº³ä»€å‡è¡¡ï¼Œå®ç°ä¸‰å±‚ç»“æ„ï¼š
- Episodeå±‚ï¼šåšå¼ˆæ±‚è§£å°è¯•
- Stepå±‚ï¼šæ™ºèƒ½ä½“ç­–ç•¥è°ƒæ•´
- UE-DTAå±‚ï¼šäº¤é€šä»¿çœŸå“åº”ï¼ˆç”±ç¯å¢ƒæä¾›ï¼‰
"""

import sys
import os
import numpy as np
import torch
from typing import Dict, List, Tuple, Any
from tqdm import tqdm
import logging
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ¨¡å—
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)

from src.algorithms.maddpg.maddpg import MADDPG
from src.env.EVCSChargingGameEnv import EVCSChargingGameEnv
from src.utils.config import MADDPGConfig, TrainingConfig


class MADDPGTrainer:
    """
    MADDPGå……ç”µç«™ä»·æ ¼åšå¼ˆè®­ç»ƒå™¨
    
    ä¸“æ³¨äºçº³ä»€å‡è¡¡æ±‚è§£ï¼Œé‡‡ç”¨æ¸…æ™°çš„ä¸‰å±‚ç»“æ„ï¼š
    - Episodeå±‚ï¼šåšå¼ˆæ±‚è§£å°è¯•  
    - Stepå±‚ï¼šæ™ºèƒ½ä½“ç­–ç•¥è°ƒæ•´
    - UE-DTAå±‚ï¼šäº¤é€šä»¿çœŸå“åº”
    """
    
    def __init__(self, maddpg_config: MADDPGConfig, training_config: TrainingConfig):
        """
        åˆå§‹åŒ–MADDPGTrainer
        
        Args:
            maddpg_config: MADDPGç®—æ³•é…ç½®
            training_config: è®­ç»ƒæµç¨‹é…ç½®
        """
        self.config = training_config
        self.maddpg_config = maddpg_config
        
        # å¤„ç†è®¾å¤‡é…ç½®
        if training_config.device == 'auto':
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = training_config.device
            
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        if self.device == 'cuda':
            print(f"GPU: {torch.cuda.get_device_name(0)}")
        
        # 1. åˆ›å»ºç¯å¢ƒ
        self.env = EVCSChargingGameEnv(
            network_dir=training_config.network_dir,
            network_name=training_config.network_name,
            random_seed=training_config.seed,
            max_steps=training_config.max_steps_per_episode,
            convergence_threshold=training_config.convergence_threshold,
            stable_steps_required=training_config.stable_steps_required
        )
        
        # 2. ä»ç¯å¢ƒè·å–ç»´åº¦ä¿¡æ¯
        obs_space = self.env.observation_space(self.env.agents[0])
        obs_dim = sum(np.prod(space.shape) for space in obs_space.spaces.values())
        action_dim = self.env.action_space(self.env.agents[0]).shape[0]
        global_obs_dim = self.env.global_state_space().shape[0]
        
        # 3. åˆ›å»ºMADDPGç®—æ³•
        self.maddpg = MADDPG(
            agent_ids=self.env.agents,
            obs_dim=obs_dim,
            action_dim=action_dim,
            global_obs_dim=global_obs_dim,
            buffer_capacity=maddpg_config.buffer_capacity,
            max_batch_size=maddpg_config.max_batch_size,
            actor_lr=maddpg_config.actor_lr,
            critic_lr=maddpg_config.critic_lr,
            gamma=maddpg_config.gamma,
            tau=maddpg_config.tau,
            seed=training_config.seed,
            device=self.device,
            actor_hidden_sizes=maddpg_config.actor_hidden_sizes,
            critic_hidden_sizes=maddpg_config.critic_hidden_sizes,
            noise_sigma=maddpg_config.noise_sigma,
            noise_decay=maddpg_config.noise_decay,
            min_noise=maddpg_config.min_noise
        )
        
        # 4. è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.convergence_episodes = []      # æ”¶æ•›çš„episodeåˆ—è¡¨
        self.episode_lengths = []           # æ¯ä¸ªepisodeçš„é•¿åº¦
        self.step_records = []              # æ¯æ­¥è¯¦ç»†è®°å½•ï¼ˆåŒ…å«æ‰€æœ‰è®­ç»ƒæ•°æ®ï¼‰
    
    def train(self) -> Dict:
        """
        ä¸»è®­ç»ƒå¾ªç¯ï¼šå¯»æ‰¾çº³ä»€å‡è¡¡
        
        Episodeå±‚é€»è¾‘ï¼šæ¯ä¸ªepisodeéƒ½æ˜¯å¯¹åŒä¸€ä»·æ ¼åšå¼ˆçš„æ±‚è§£å°è¯•
        æˆåŠŸæ ‡å‡†ï¼šåœ¨å•ä¸ªepisodeä¸­è¾¾åˆ°çº³ä»€å‡è¡¡ï¼ˆä»·æ ¼æ”¶æ•›ï¼‰
        
        Returns:
            Dict: è®­ç»ƒç»“æœç»Ÿè®¡
        """
        convergence_count = 0
        
        with tqdm(total=self.config.max_episodes, desc="å¯»æ‰¾çº³ä»€å‡è¡¡", unit="episode", dynamic_ncols=True) as episode_pbar:
            
            for episode in range(self.config.max_episodes):
                
                # é‡æ–°åˆå§‹åŒ–åŒä¸€åšå¼ˆï¼ˆä¸æ˜¯æ–°åšå¼ˆï¼‰
                observations, _ = self.env.reset()
                
                # Stepå±‚ï¼šç­–ç•¥è°ƒæ•´å¾ªç¯
                converged_in_episode, episode_length = self._run_episode(episode, observations)
                
                # è®°å½•episodeç»Ÿè®¡
                self.episode_lengths.append(episode_length)
                
                if converged_in_episode:
                    convergence_count += 1
                    self.convergence_episodes.append(episode)
                    episode_pbar.set_postfix({
                        "æ”¶æ•›æ¬¡æ•°": convergence_count,
                        "æ”¶æ•›ç‡": f"{convergence_count/(episode+1):.1%}"
                    })
                    
                    # å¯é€‰ï¼šå¦‚æœè¿ç»­å¤šæ¬¡æ”¶æ•›ï¼Œå¯æå‰ç»“æŸ
                    if self._check_stable_convergence():
                        print(f"è¿ç»­æ”¶æ•›ï¼Œè®­ç»ƒæå‰ç»“æŸäºepisode {episode}")
                        break
                
                episode_pbar.update(1)
        
        # ç”Ÿæˆè®­ç»ƒç»“æœ
        results = self._generate_training_results()
        
        # è‡ªåŠ¨ä¿å­˜è®­ç»ƒæ•°æ®ï¼ˆä½¿ç”¨é…ç½®çš„è¾“å‡ºç›®å½•ï¼‰
        experiment_dir = self.save_training_data(self.config.output_dir)
        results['experiment_dir'] = experiment_dir
        
        return results
    
    def _run_episode(self, episode: int, observations: Dict) -> Tuple[bool, int]:
        """
        Stepå±‚é€»è¾‘ï¼šåœ¨å•ä¸ªepisodeå†…è°ƒæ•´æ™ºèƒ½ä½“ç­–ç•¥ç›´åˆ°æ”¶æ•›æˆ–è¶…æ—¶
        
        Args:
            episode: å½“å‰episodeç¼–å·
            observations: åˆå§‹è§‚æµ‹
        
        Returns:
            Tuple[bool, int]: (æ˜¯å¦æ”¶æ•›, episodeé•¿åº¦)
        """
        with tqdm(total=self.config.max_steps_per_episode, 
                  desc=f"Episode {episode}", unit="step", leave=False, dynamic_ncols=True) as step_pbar:
            
            for step in range(self.config.max_steps_per_episode):
                
                # æ™ºèƒ½ä½“å†³ç­–
                actions = self.maddpg.take_action(observations, add_noise=True)
                
                # è·å–å®é™…ä»·æ ¼ï¼ˆåœ¨stepä¹‹å‰ï¼‰
                actual_prices = self.env.actions_to_prices_dict(actions)
                
                # ç¯å¢ƒå“åº”
                next_observations, rewards, terminations, truncations, infos = self.env.step(actions)
                
                # å­˜å‚¨ç»éªŒå¹¶å­¦ä¹ 
                self.maddpg.store_experience(observations, actions, rewards, next_observations, terminations)
                self.maddpg.learn()
                
                # è®°å½•è¯¦ç»†ä¿¡æ¯
                self.step_records.append({
                    'episode': episode,
                    'step': step,
                    'actions': actions.copy(),
                    'actual_prices': actual_prices.copy(),
                    'rewards': rewards.copy(),
                    'ue_info': infos,
                    'relative_change_rate': infos.get('relative_change_rate', float('inf'))
                })
                
                
                # æ£€æŸ¥æ˜¯å¦æ”¶æ•›ï¼ˆçº³ä»€å‡è¡¡ï¼‰
                if all(terminations.values()):
                    step_pbar.set_postfix({"çŠ¶æ€": "æ”¶æ•›"})
                    step_pbar.update(self.config.max_steps_per_episode - step)
                    return True, step + 1
                
                # æ›´æ–°è§‚æµ‹
                observations = next_observations
                
                # æ›´æ–°è¿›åº¦æ¡
                step_pbar.set_postfix({
                    "UEè¿­ä»£": infos.get('ue_iterations', 0),
                    "ç›¸å¯¹å˜åŒ–": f"{infos.get('relative_change_rate', float('inf')):.4f}"
                })
                step_pbar.update(1)
        
        # Episodeè¶…æ—¶æœªæ”¶æ•›
        step_pbar.set_postfix({"çŠ¶æ€": "è¶…æ—¶"})
        return False, self.config.max_steps_per_episode
    
    def get_nash_equilibrium(self) -> Dict:
        """
        è·å–æ‰€æœ‰çº³ä»€å‡è¡¡è§£ï¼ˆä»æ‰€æœ‰æ”¶æ•›çš„episodeçš„ç¨³å®šæ­¥éª¤ä¸­è®¡ç®—å¹³å‡å€¼ï¼‰
        
        Returns:
            Dict: åŒ…å«æ‰€æœ‰çº³ä»€å‡è¡¡è§£çš„ç»Ÿè®¡ï¼Œæ ¼å¼ä¸ºï¼š
                - status: 'converged' | 'no_convergence'
                - total_equilibria: æ‰¾åˆ°çš„å‡è¡¡è§£æ•°é‡
                - equilibria: List[Dict], æ¯ä¸ªå‡è¡¡è§£åŒ…å«ï¼š
                    - episode: episodeç¼–å·
                    - equilibrium_actions: å¹³å‡åŠ¨ä½œ
                    - equilibrium_prices: å¹³å‡ä»·æ ¼  
                    - equilibrium_rewards: å¹³å‡å¥–åŠ±
                    - stable_steps_count: ç¨³å®šæ­¥æ•°
        """
        if not self.convergence_episodes:
            return {
                'status': 'no_convergence',
                'message': 'æœªæ‰¾åˆ°æ”¶æ•›çš„episode',
                'total_equilibria': 0,
                'equilibria': []
            }
        
        equilibria = []
        
        # ä¸ºæ¯ä¸ªæ”¶æ•›çš„episodeè®¡ç®—çº³ä»€å‡è¡¡è§£
        for episode_idx in self.convergence_episodes:
            # ä»step_recordsä¸­æ‰¾åˆ°è¯¥episodeçš„æ”¶æ•›æ­¥éª¤
            convergence_steps = [
                record for record in self.step_records 
                if record['episode'] == episode_idx
            ]
            
            if not convergence_steps:
                continue
            
            # è·å–æ”¶æ•›æ—¶çš„ç¨³å®šæ­¥éª¤æ•°æ®ï¼ˆæœ€å stable_steps_required æ­¥ï¼‰
            stable_steps = convergence_steps[-self.config.stable_steps_required:]
            
            # è®¡ç®—ç¨³å®šæ­¥éª¤çš„å¹³å‡åŠ¨ä½œ
            equilibrium_actions = {}
            for agent in self.env.agents:
                agent_actions = []
                for step in stable_steps:
                    agent_actions.append(step['actions'][agent])
                # è®¡ç®—å¹³å‡åŠ¨ä½œ
                equilibrium_actions[agent] = np.mean(agent_actions, axis=0)
            
            # å°†å¹³å‡åŠ¨ä½œè½¬æ¢ä¸ºå®é™…ä»·æ ¼
            actual_prices = self.env.actions_to_prices_dict(equilibrium_actions)
            
            # è®¡ç®—ç¨³å®šæ­¥éª¤çš„å¹³å‡å¥–åŠ±
            equilibrium_rewards = {}
            for agent in self.env.agents:
                agent_rewards = []
                for step in stable_steps:
                    agent_rewards.append(step['rewards'][agent])
                equilibrium_rewards[agent] = float(np.mean(agent_rewards))
            
            # æ·»åŠ åˆ°å‡è¡¡è§£åˆ—è¡¨
            equilibria.append({
                'episode': episode_idx,
                'final_step': stable_steps[-1]['step'],
                'stable_steps_count': len(stable_steps),
                'equilibrium_actions': equilibrium_actions,
                'equilibrium_prices': actual_prices,
                'equilibrium_rewards': equilibrium_rewards,
                'environment_info': stable_steps[-1]['ue_info']
            })
        
        return {
            'status': 'converged',
            'total_equilibria': len(equilibria),
            'equilibria': equilibria,
            'latest_equilibrium': equilibria[-1] if equilibria else None
        }
    
    def evaluate(self, num_episodes: int = 20) -> Dict:
        """
        è¯„ä¼°è®­ç»ƒæ•ˆæœï¼šæµ‹è¯•å‡è¡¡ç¨³å®šæ€§
        
        Args:
            num_episodes: è¯„ä¼°è½®æ•°
        
        Returns:
            Dict: è¯„ä¼°ç»“æœ
            
        TODO: å®ç°å•æ–¹é¢åç¦»æµ‹è¯•ï¼ŒéªŒè¯çº³ä»€å‡è¡¡æ€§è´¨
        """
        # ğŸ”„ æš‚æ—¶ç•™ç©ºï¼Œåç»­å®ç°å•æ–¹é¢åç¦»æµ‹è¯•
        return {
            'status': 'not_implemented',
            'message': 'å°†å®ç°å•æ–¹é¢åç¦»æµ‹è¯•éªŒè¯çº³ä»€å‡è¡¡æ€§è´¨'
        }
    
    def _check_stable_convergence(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç¨³å®šæ”¶æ•›ï¼ˆè¿ç»­å¤šæ¬¡episodeéƒ½æ”¶æ•›ï¼‰
        
        Returns:
            bool: æ˜¯å¦è¿ç»­æ”¶æ•›ï¼Œå¯ç”¨äºæå‰ç»ˆæ­¢è®­ç»ƒ
        """
        if len(self.convergence_episodes) < self.config.stable_episodes_required:
            return False
        
        # æ£€æŸ¥æœ€è¿‘stable_episodes_requiredæ¬¡episodeæ˜¯å¦éƒ½æ”¶æ•›
        recent_episodes = list(range(len(self.episode_lengths)))[-self.config.stable_episodes_required:]
        return all(ep in self.convergence_episodes for ep in recent_episodes)
    
    def _generate_training_results(self) -> Dict:
        """
        ç”Ÿæˆå®Œæ•´çš„è®­ç»ƒç»“æœç»Ÿè®¡
        
        Returns:
            Dict: è®­ç»ƒç»Ÿè®¡ç»“æœ
        """
        total_episodes = len(self.episode_lengths)
        total_convergences = len(self.convergence_episodes)
        
        # ä»step_recordsè®¡ç®—æ€»UEè¿­ä»£æ¬¡æ•°
        total_ue_iterations = sum(
            record['ue_info'].get('ue_iterations', 0) 
            for record in self.step_records
        )
        
        return {
            'total_episodes': total_episodes,
            'total_convergences': total_convergences,
            'convergence_rate': total_convergences / total_episodes if total_episodes > 0 else 0.0,
            'average_episode_length': np.mean(self.episode_lengths) if self.episode_lengths else 0.0,
            'total_ue_iterations': total_ue_iterations,
            'convergence_episodes': self.convergence_episodes,
            'final_nash_equilibrium': self.get_nash_equilibrium()
        }
    
    def save_training_data(self, output_dir: str = "results") -> str:
        """
        ä¿å­˜è®­ç»ƒæ•°æ®åˆ°JSONæ–‡ä»¶
        
        Args:
            output_dir: è¾“å‡ºæ ¹ç›®å½•
            
        Returns:
            str: ä¿å­˜çš„å®éªŒç›®å½•è·¯å¾„
        """
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å®éªŒç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        experiment_dir = os.path.join(output_dir, f"experiment_{timestamp}")
        os.makedirs(experiment_dir, exist_ok=True)
        
        # ä¿å­˜step_recordsåˆ°JSONæ–‡ä»¶
        step_records_path = os.path.join(experiment_dir, "step_records.json")
        
        # å‡†å¤‡ä¿å­˜æ•°æ®ï¼Œå¤„ç†numpyæ•°ç»„
        save_data = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_episodes": len(self.episode_lengths),
                "total_steps": len(self.step_records),
                "convergence_episodes": self.convergence_episodes,
                "episode_lengths": self.episode_lengths
            },
            "records": []
        }
        
        # è½¬æ¢step_recordsï¼Œå¤„ç†numpyæ•°ç»„
        for record in self.step_records:
            converted_record = {
                "episode": int(record["episode"]),
                "step": int(record["step"]),
                "actions": {k: v.tolist() if hasattr(v, 'tolist') else v 
                           for k, v in record["actions"].items()},
                "actual_prices": {k: v.tolist() if hasattr(v, 'tolist') else v 
                                 for k, v in record["actual_prices"].items()},
                "rewards": {k: float(v) for k, v in record["rewards"].items()},
                "ue_info": record["ue_info"],
                "relative_change_rate": float(record["relative_change_rate"])
            }
            save_data["records"].append(converted_record)
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(step_records_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {experiment_dir}")
        print(f"   ğŸ“ å®éªŒç›®å½•: {experiment_dir}")
        print(f"   ğŸ“„ æ•°æ®æ–‡ä»¶: step_records.json")
        print(f"   ğŸ“Š è®°å½•æ•°é‡: {len(self.step_records)} æ­¥")
        
        return experiment_dir